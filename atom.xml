<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Magical Bean</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://wsx1128.cn/"/>
  <updated>2020-03-15T17:06:13.934Z</updated>
  <id>http://wsx1128.cn/</id>
  
  <author>
    <name>五光君</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Image_Caption 基于pytorch深度学习框架的图像描述</title>
    <link href="http://wsx1128.cn/2020/03/15/Image-Caption-%E5%9F%BA%E4%BA%8Epytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E7%9A%84%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    <id>http://wsx1128.cn/2020/03/15/Image-Caption-基于pytorch深度学习框架的图像描述/</id>
    <published>2020-03-15T15:42:15.000Z</published>
    <updated>2020-03-15T17:06:13.934Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>这次的项目就提升了很大的难度，基本上是将所学的全部进行一个应用。 将卷积网络和循环网络进行一个结合</p><p>卷积网络 提取图片特征</p><p>循环网络 根据提取特征输出对图片的描述</p><p>下面 请看代码 </p><p>还是老规矩，按照步骤一点一点的进行下去</p><h3 id="数据总揽及处理"><a href="#数据总揽及处理" class="headerlink" title="数据总揽及处理"></a>数据总揽及处理</h3><p>这次的数据包含图片 以及 对图片的描述 （5句话，可以随机选择一句）</p><p>这里对数据的处理也是两个方法，处理生数据或者是处理已经预处理数据</p><p>这里两个都贴出来</p><p><strong>数据的预处理</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Config</span>:</span></span><br><span class="line">    annotation_file = <span class="string">'caption_train_annotations_20170902.json'</span></span><br><span class="line">    unknown = <span class="string">'&lt;/UNKNOWN&gt;'</span></span><br><span class="line">    end = <span class="string">'&lt;/EOS&gt;'</span></span><br><span class="line">    padding = <span class="string">'&lt;/PAD&gt;'</span></span><br><span class="line">    max_words = <span class="number">10000</span></span><br><span class="line">    min_appear = <span class="number">2</span></span><br><span class="line">    save_path = <span class="string">'caption.pth'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process</span><span class="params">(**kwargs)</span>:</span></span><br><span class="line">    opt = Config()</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> kwargs.items():</span><br><span class="line">        setattr(opt, k, v)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(opt.annotation_file) <span class="keyword">as</span> f:</span><br><span class="line">        data = json.load(f)</span><br><span class="line"></span><br><span class="line">    id2ix = &#123;item[<span class="string">'image_id'</span>]: ix <span class="keyword">for</span> ix, item <span class="keyword">in</span> enumerate(data)&#125;</span><br><span class="line">    ix2id = &#123;ix: pic <span class="keyword">for</span> pic, ix <span class="keyword">in</span> id2ix.items()&#125;</span><br><span class="line">    <span class="keyword">assert</span> id2ix[ix2id[<span class="number">10</span>]] == <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    captions = [item[<span class="string">'caption'</span>] <span class="keyword">for</span> item <span class="keyword">in</span> data]</span><br><span class="line">    cut_captions = [[list(jieba.cut(ii, cut_all=<span class="keyword">False</span>) <span class="keyword">for</span> ii <span class="keyword">in</span> item)] <span class="keyword">for</span> item <span class="keyword">in</span> tqdm.tqdm(captions)]</span><br><span class="line"></span><br><span class="line">    word_nums = &#123;&#125;  <span class="comment"># 每一个词出现的次数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里的语法结构很神奇，需要后面自己去加深一下理解</span></span><br><span class="line">    <span class="comment"># 下面就是过滤低频词语</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(word_nums)</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">(word)</span>:</span></span><br><span class="line">            word_nums[word] = word_nums.get(word, <span class="number">0</span>) + <span class="number">1</span>  <span class="comment"># 每出现一次就增加一次</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">return</span> fun</span><br><span class="line"></span><br><span class="line">    lambda_ = update(word_nums)</span><br><span class="line">    <span class="comment"># 需要去测试一下，这一句是否与下面一句是相同的效果</span></span><br><span class="line">    <span class="comment">#_ = &#123;[[lambda_(word) for word in sentence] for sentence in sentences]for sentences in cut_captions&#125;</span></span><br><span class="line">    _ = &#123;lambda_(word) <span class="keyword">for</span> sentences <span class="keyword">in</span> cut_captions <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences <span class="keyword">for</span> word <span class="keyword">in</span> sentence&#125;</span><br><span class="line">    word_nums_list = sorted([(num, word) <span class="keyword">for</span> word, num <span class="keyword">in</span> word_nums.items()], reverse=<span class="keyword">True</span>)  <span class="comment"># 倒序排列</span></span><br><span class="line">    <span class="comment"># 排在前1w个高频词汇 直接将其放入到可以操作的字典李</span></span><br><span class="line">    words = [word[<span class="number">1</span>] <span class="keyword">for</span> word <span class="keyword">in</span> word_nums_list[:opt.max_words] <span class="keyword">if</span> word[<span class="number">0</span>] &gt;= opt.min_appear]</span><br><span class="line">    words = [opt.unknown, opt.padding, opt.end] + words</span><br><span class="line">    word2ix = &#123;word: ix <span class="keyword">for</span> ix, word <span class="keyword">in</span> enumerate(words)&#125;</span><br><span class="line">    ix2word = &#123;ix: word <span class="keyword">for</span> word, ix <span class="keyword">in</span> word2ix.items()&#125;</span><br><span class="line">    <span class="keyword">assert</span> word2ix[ix2word[<span class="number">123</span>]] == <span class="number">123</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#这里default的意思就是如果不存在 就返回opt.unknown</span></span><br><span class="line">    ix_captions = [[[word2ix.get(word, default=word2ix.get(opt.unknown)) <span class="keyword">for</span> word <span class="keyword">in</span> sentence] <span class="keyword">for</span> sentence <span class="keyword">in</span> item] <span class="keyword">for</span> item <span class="keyword">in</span> cut_captions]</span><br><span class="line"></span><br><span class="line">    readme = <span class="string">u"""</span></span><br><span class="line"><span class="string">    word：词</span></span><br><span class="line"><span class="string">    ix:index</span></span><br><span class="line"><span class="string">    id:图片名</span></span><br><span class="line"><span class="string">    caption: 分词之后的描述，通过ix2word可以获得原始中文词</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    results = &#123;</span><br><span class="line">        <span class="string">'caption'</span>: ix_captions,</span><br><span class="line">        <span class="string">'word2ix'</span>: word2ix,</span><br><span class="line">        <span class="string">'ix2word'</span>: ix2word,</span><br><span class="line">        <span class="string">'ix2id'</span>: ix2id,</span><br><span class="line">        <span class="string">'id2ix'</span>: id2ix,</span><br><span class="line">        <span class="string">'padding'</span>: <span class="string">'&lt;/PAD&gt;'</span>,</span><br><span class="line">        <span class="string">'end'</span>: <span class="string">'&lt;/EOS&gt;'</span>,</span><br><span class="line">        <span class="string">'readme'</span>: readme</span><br><span class="line">    &#125;</span><br><span class="line">    t.save(results, opt.save_path)</span><br><span class="line">    print(<span class="string">'save file in %s'</span> % opt.save_path)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(ix, ix2= <span class="number">4</span>)</span>:</span></span><br><span class="line">        results = t.load(opt.save_path)</span><br><span class="line">        ix2word = results[<span class="string">'ix2word'</span>]</span><br><span class="line">        examples = results[<span class="string">'caption'</span>][ix][<span class="number">4</span>] <span class="comment"># 第ix张图的第四句话</span></span><br><span class="line">        sentences_p = (<span class="string">''</span>.join([ix2word[ii] <span class="keyword">for</span> ii <span class="keyword">in</span> examples]))</span><br><span class="line">        sentences_r = data[ix][<span class="string">'caption'</span>][ix2]</span><br><span class="line">        <span class="keyword">assert</span> sentences_p == sentences_r, <span class="string">'test failed'</span></span><br><span class="line">    test(<span class="number">1000</span>)</span><br><span class="line">    print(<span class="string">'test success'</span>)</span><br></pre></td></tr></table></figure><p>以后自己要学会怎么去处理数据，因为可能数据不再睡提供好了的。</p><p>注意这里的语料库，结巴分词，直接将词语分好，然后你自己再去选择相应出现频率最高的词语，作为词语的词语嵌入的训练集，加快进度</p><p><strong>Caption数据处理</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> torchvision <span class="keyword">as</span> tv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">IMAGENET_MEAN = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">IMAGENET_STD = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_collate_fn</span><span class="params">(padding, eos, max_length=<span class="number">50</span>)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        将多个样本拼接在一起成一个batch</span></span><br><span class="line"><span class="string">        输入： list of data，形如</span></span><br><span class="line"><span class="string">        [(img1, cap1, index1), (img2, cap2, index2) ....]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        拼接策略如下：</span></span><br><span class="line"><span class="string">        - batch中每个样本的描述长度都是在变化的，不丢弃任何一个词\</span></span><br><span class="line"><span class="string">          选取长度最长的句子，将所有句子pad成一样长</span></span><br><span class="line"><span class="string">        - 长度不够的用&lt;/PAD&gt;在结尾PAD</span></span><br><span class="line"><span class="string">        - 没有START标识符</span></span><br><span class="line"><span class="string">        - 如果长度刚好和词一样，那么就没有&lt;/EOS&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        返回：</span></span><br><span class="line"><span class="string">        - imgs(Tensor): batch_size*2048(2048这里就是卷积层最后的结果)</span></span><br><span class="line"><span class="string">        - cap_tensor(Tensor): batch_size*max_length</span></span><br><span class="line"><span class="string">        - lengths(list of int): 长度为batch_size</span></span><br><span class="line"><span class="string">        - index(list of int): 长度为batch_size</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span><span class="params">(img_cap)</span>:</span></span><br><span class="line">        img_cap.sort(key=<span class="keyword">lambda</span> p: len(p[<span class="number">1</span>]), reverse=<span class="keyword">True</span>)</span><br><span class="line">        imgs, caps, indexs = zip(*img_cap)</span><br><span class="line">        imgs = t.cat([img.unsqueeze(<span class="number">0</span>) <span class="keyword">for</span> img <span class="keyword">in</span> imgs], <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 限制最大不能超过50个词 一句话，如果超过就修建成50个词</span></span><br><span class="line">        lengths = [min(len(c) + <span class="number">1</span>, max_length) <span class="keyword">for</span> c <span class="keyword">in</span> caps]</span><br><span class="line">        batch_length = max(lengths)</span><br><span class="line">        <span class="comment"># 直接先创建一个空向量</span></span><br><span class="line">        cap_tensor = t.LongTensor(batch_length, len(caps)).fill_(padding)</span><br><span class="line">        <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(caps):</span><br><span class="line">            end_cap = lengths[i] - <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> end_cap &lt; batch_length:</span><br><span class="line">                cap_tensor[end_cap, i] = eos</span><br><span class="line">            cap_tensor[:end_cap, i].copy_(c[:end_cap])</span><br><span class="line">        <span class="keyword">return</span> (imgs, (cap_tensor, lengths), indexs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> collate_fn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CaptionDataset</span><span class="params">(data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, opt)</span>:</span></span><br><span class="line">        self.opt = opt</span><br><span class="line">        data = t.load(opt.caption_data_path)</span><br><span class="line">        word2ix = data[<span class="string">'word2ix'</span>]</span><br><span class="line">        self.captions = data[<span class="string">'caption'</span>]</span><br><span class="line">        self.padding = word2ix.get(data.get(<span class="string">'padding'</span>))</span><br><span class="line">        self.end = word2ix.get(data.get(<span class="string">'end'</span>))</span><br><span class="line">        self._data = data</span><br><span class="line">        self.ix2id = data[<span class="string">'ix2id'</span>]</span><br><span class="line">        self.all_imgs = t.load(opt.img_feature_path)</span><br><span class="line">        self.train(<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        返回：</span></span><br><span class="line"><span class="string">        - img: 图像features 2048的向量</span></span><br><span class="line"><span class="string">        - caption: 描述，形如LongTensor([1,3,5,2]),长度取决于描述长度</span></span><br><span class="line"><span class="string">        - index: 下标，图像的序号，可以通过ix2id[index]获取对应图片文件名</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        index = index + self._start</span><br><span class="line">        img = self.all_imgs[index]</span><br><span class="line"></span><br><span class="line">        caption = self.captions[index]</span><br><span class="line">        <span class="comment"># 5句描述随机选一句</span></span><br><span class="line">        rdn_index = np.random.choice(len(caption), <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">        caption = caption[rdn_index]</span><br><span class="line">        <span class="keyword">return</span> img, t.LongTensor(caption), index</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.len_</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, training=True)</span>:</span></span><br><span class="line">        self.training = training</span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            self._start = <span class="number">0</span></span><br><span class="line">            self.len_ = len(self._data) - <span class="number">10000</span></span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            self._start = len(self._data) - <span class="number">10000</span></span><br><span class="line">            self.len_ = <span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataloader</span><span class="params">(opt)</span>:</span></span><br><span class="line">    dataset = CaptionDataset(opt)</span><br><span class="line">    dataloader = data.DataLoader(dataset,</span><br><span class="line">                                 batch_size=opt.batch_size,</span><br><span class="line">                                 shuffle=opt.shuffle,</span><br><span class="line">                                 num_workers=opt.num_workers,</span><br><span class="line">                                 collate_fn=create_collate_fn(dataset.padding, dataset.end))</span><br><span class="line">    <span class="keyword">return</span> dataloader</span><br></pre></td></tr></table></figure><ul><li>collate_fn在加载图片的时候，可以自定义加载方式, <strong>我们目前的理解就是与处理图片的transforms是一样的，只不过一个对图片，一个是对其他数据</strong></li></ul><p>下面就是对图片进行一个 预处理咯</p><p><strong>ImageData</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torchvision <span class="keyword">as</span> tv</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">IMAGENET_MEAN = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">IMAGENET_STD = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line">normalize = tv.transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CaptionDataset</span><span class="params">(data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, opt)</span>:</span></span><br><span class="line">        self.transforms = tv.transforms.Compose([</span><br><span class="line">            tv.transforms.Resize(<span class="number">256</span>),</span><br><span class="line">            tv.transforms.CenterCrop(<span class="number">256</span>),</span><br><span class="line">            tv.transforms.ToTensor(),</span><br><span class="line">            normalize</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">        data = t.load(opt.caption_data_path)</span><br><span class="line">        self.ix2id = data[<span class="string">'ix2id'</span>]</span><br><span class="line"></span><br><span class="line">        self.imgs = [os.path.join(opt.img_path, self.ix2id[ix]) <span class="keyword">for</span> ix <span class="keyword">in</span> range(len(self.ix2id))]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        img = Image.open(self.imgs[index]).convert(<span class="string">'RGB'</span>)</span><br><span class="line">        img = self.transforms(img)</span><br><span class="line">        <span class="keyword">return</span> img, index</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_loader</span><span class="params">(opt)</span>:</span></span><br><span class="line">    dataset = CaptionDataset(opt.caption_data_path)</span><br><span class="line">    dataloader = data.DataLoader(dataset,</span><br><span class="line">                                 batch_size=opt.batch_size,</span><br><span class="line">                                 shuffle=<span class="keyword">False</span>, <span class="comment"># 这里是需要与后面描述那一块的图像进行比对校准，所以不能打乱</span></span><br><span class="line">                                 num_workers=opt.num_workers</span><br><span class="line">                                 )</span><br><span class="line">    <span class="keyword">return</span> dataloader</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract</span><span class="params">()</span>:</span></span><br><span class="line">    opt = Config()</span><br><span class="line">    t.set_grad_enabled(<span class="keyword">False</span>)</span><br><span class="line">    dataloader = get_loader(opt)</span><br><span class="line">    results = t.Tensor(len(dataloader.dataset), <span class="number">2048</span>).fill_(<span class="number">0</span>)</span><br><span class="line">    batch_size = opt.batch_size</span><br><span class="line"></span><br><span class="line">    resnet50 = tv.models.resnet50(pretrained=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">del</span> resnet50.fc</span><br><span class="line">    resnet50.fc = <span class="keyword">lambda</span> x:x</span><br><span class="line">    <span class="keyword">if</span> opt.use_gpu:</span><br><span class="line">        resnet50.cuda()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ii, (imgs, indexs) <span class="keyword">in</span> tqdm.tqdm(enumerate(dataloader)):</span><br><span class="line">        <span class="keyword">assert</span> indexs[<span class="number">0</span>] == batch_size * ii</span><br><span class="line">        <span class="keyword">if</span> opt.use_gpu:</span><br><span class="line">            imgs = imgs.cuda()</span><br><span class="line">        features = resnet50(imgs)</span><br><span class="line">        results[batch_size*ii : batch_size*(ii+<span class="number">1</span>),:] = features.data.cpu()</span><br><span class="line"></span><br><span class="line">    t.save(results, <span class="string">'results.pth'</span>)</span><br></pre></td></tr></table></figure><ul><li><p>注意这个地方是 将预训练模型中的模型拿来改一改就可以直接用来提取特征了</p></li><li><p>这里由于图片后面要与caption 一一对应，所以就不能够打乱顺序</p></li><li><p>保险起见，这里将梯度下降的按钮关掉</p></li></ul><p>下面就是处理模型的时候咯</p><h3 id="模型处理"><a href="#模型处理" class="headerlink" title="模型处理"></a>模型处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">import</span> torchvision <span class="keyword">as</span> tv</span><br><span class="line"><span class="keyword">from</span> utils.beam_search <span class="keyword">import</span> CaptionGenerator</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pack_padded_sequence</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CaptionModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, opt, word2ix, ix2word)</span>:</span></span><br><span class="line">        super(CaptionModel, self).__init__()</span><br><span class="line">        self.ix2word = ix2word</span><br><span class="line">        self.word2ix = word2ix</span><br><span class="line">        self.opt = opt</span><br><span class="line">        self.fc = nn.Linear(<span class="number">2048</span>, opt.rnn_hidden)</span><br><span class="line"></span><br><span class="line">        self.rnn = nn.LSTM(opt.embedding_dim, opt.rnn_hidden, num_layers=opt.num_layers)</span><br><span class="line">        self.classifier = nn.Linear(opt.rnn_hidden, len(word2ix))</span><br><span class="line">        <span class="comment">#  其的参数为 所有词语以及词语的词向量</span></span><br><span class="line">        self.embedding = nn.Embedding(len(word2ix), opt.embedding_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, img_feats, captions, lengths)</span>:</span></span><br><span class="line">        <span class="comment"># 将相应的图片解释加入到256维的词向量</span></span><br><span class="line">        <span class="comment"># seq * batch_size * word_vec</span></span><br><span class="line">        embeddings = self.embedding(captions)</span><br><span class="line">        <span class="comment"># 将图片根据全链接层得到256维的向量</span></span><br><span class="line">        <span class="comment"># 1 * batch_size * 256</span></span><br><span class="line">        img_feats = self.fc(img_feats).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 这里直接将图片的信息直接看作成 第一个词的词向量</span></span><br><span class="line">        <span class="comment"># 记住cat的作用是将tuple或者list中的tensor链接在一起</span></span><br><span class="line">        embeddings = t.cat([img_feats, embeddings], <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        pack_embeddings = pack_padded_sequence(embeddings, lengths)</span><br><span class="line">        output, state = self.rnn(pack_embeddings)</span><br><span class="line">        pred = self.classifier(output[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">return</span> pred, state</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate</span><span class="params">(self, img, eos_token=<span class="string">'&lt;/EOS&gt;'</span>, beam_size=<span class="number">3</span>, max_caption_length=<span class="number">30</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 length_normalization_factor=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        cap_gen = CaptionGenerator(embedder=self.embedding,</span><br><span class="line">                                   rnn=self.rnn,</span><br><span class="line">                                   classifier=self.classifier,</span><br><span class="line">                                   eos_id=self.word2ix[eos_token],</span><br><span class="line">                                   beam_size=beam_size,</span><br><span class="line">                                   max_caption_length=max_caption_length,</span><br><span class="line">                                   length_normalization_factor=length_normalization_factor)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> next(self.parameters()).is_cuda:</span><br><span class="line">            img = img.cuda()</span><br><span class="line">        <span class="comment"># img.size = 1 * 2048</span></span><br><span class="line">        img = img.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># img.size = 1 * 1 * 2048</span></span><br><span class="line">        img = self.fc(img).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># img.size = 1 * 1 * 256</span></span><br><span class="line">        sentences, score = cap_gen.beam_search(img)</span><br><span class="line">        sentences = [sent <span class="keyword">for</span> sent <span class="keyword">in</span> sentences]</span><br><span class="line">        res = sentences[score.index(max(score))]</span><br><span class="line">        <span class="keyword">del</span> res[<span class="number">-1</span>]</span><br><span class="line">        res = <span class="string">''</span>.join([self.ix2word[ii.item()] <span class="keyword">for</span> ii <span class="keyword">in</span> res]) + <span class="string">u'。'</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self, path=None, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> path <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            path = <span class="string">'&#123;prefix&#125;_&#123;time&#125;'</span>.format(prefix=self.opt.prefix,</span><br><span class="line">                                            time=time.strftime(<span class="string">'%m%d_%H%M'</span>))</span><br><span class="line">        states = self.states()</span><br><span class="line">        states.update(kwargs)</span><br><span class="line">        t.save(states, path)</span><br><span class="line">        <span class="keyword">return</span> path</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(self, path, load_opt=False)</span>:</span></span><br><span class="line">        data = t.load(path, map_location=<span class="keyword">lambda</span> s, l: s)</span><br><span class="line">        state_dict = data[<span class="string">'state_dict'</span>]</span><br><span class="line">        self.load_state_dict(state_dict)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> load_opt:</span><br><span class="line">            <span class="keyword">for</span> k, v <span class="keyword">in</span> data[<span class="string">'opt'</span>].items():</span><br><span class="line">                setattr(self.opt, k, v)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_optimizer</span><span class="params">(self, lr)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> t.optim.Adam(self.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将模型的一切进行一个封装</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">state</span><span class="params">(self)</span>:</span></span><br><span class="line">        opt_state_dict = &#123;</span><br><span class="line">            attr: getattr(self.opt, attr)</span><br><span class="line">            <span class="keyword">for</span> attr <span class="keyword">in</span> dir(self.opt)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> attr.startswith(<span class="string">'__'</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">'state_dict'</span>:self.state_dict(),</span><br><span class="line">            <span class="string">'opt'</span>: opt_state_dict</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><ul><li><p>直接将生成函数封装到模型类内部，后面好方便调试</p></li><li><p>这里全链接层是想将图片直接作为第一个词语来进入到RNN的模型中，但是RNN的模型是256，所以需要加上一个全链接层去转换以下，就相当于第一个词的词向量</p></li><li><p>这里用到了Beam Seach的 生成的时候帮助选择，“全局最优点 ”</p></li></ul><p>utils 封装了一个visdom的封装类，另一个就是beam search 算法类了。</p><h3 id="Beam-search-算法模块"><a href="#Beam-search-算法模块" class="headerlink" title="Beam search 算法模块"></a>Beam search 算法模块</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torch.nn.functional <span class="keyword">import</span> log_softmax</span><br><span class="line"><span class="keyword">import</span> heapq</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Caption</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, sentence, state, logprob, score, metadata=None)</span>:</span></span><br><span class="line">        self.sentence = sentence</span><br><span class="line">        self.state = state</span><br><span class="line">        self.logprob = logprob</span><br><span class="line">        self.score = score</span><br><span class="line">        self.metadata = metadata</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在 python3 上已经被移除了</span></span><br><span class="line">    <span class="comment"># 下面是新的写法，只需要写两个 小于或等于</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__cmp__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> isinstance(other, Caption)</span><br><span class="line">        <span class="keyword">if</span> self.score == other.score:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">elif</span> self.score &lt; other.score:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__lt__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> isinstance(other, Caption)</span><br><span class="line">        <span class="keyword">return</span> self.score &lt; other.score</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__eq__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> isinstance(other, Caption)</span><br><span class="line">        <span class="keyword">return</span> self.score == other.score</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TopN</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        self._n = n</span><br><span class="line">        self._data = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">size</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> self._data <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">return</span> len(self._data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> self._data <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> len(self._data) &lt; self._n:</span><br><span class="line">            heapq.heappush(self._data, x)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            heapq.heappushpop(self._data, x)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extract</span><span class="params">(self, sort=False)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> self._data <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span></span><br><span class="line">        data = self._data</span><br><span class="line">        self._data = <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">if</span> sort:</span><br><span class="line">            data.sort(reverse=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span><span class="params">(self)</span>:</span></span><br><span class="line">        self._data = []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CaptionGenerator</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, embedder, rnn, classifier, eos_id, beam_size=<span class="number">3</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 max_caption_length=<span class="number">20</span>, length_normalization_factor=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        self.embedder = embedder</span><br><span class="line">        self.rnn = rnn</span><br><span class="line">        self.classifier = classifier</span><br><span class="line">        self.eos_id = eos_id</span><br><span class="line">        self.beam_size = beam_size</span><br><span class="line">        self.max_caption_length = max_caption_length</span><br><span class="line">        self.length_normalization_factor = length_normalization_factor</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">beam_search</span><span class="params">(self, rnn_input, initial_state=None)</span>:</span></span><br><span class="line">        <span class="string">"""Runs beam search caption generation on a single image.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">          initial_state: An initial state for the recurrent model</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">          A list of Caption sorted by descending score.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">get_topk_words</span><span class="params">(embeddings, state)</span>:</span></span><br><span class="line">            output, new_states = self.rnn(embeddings, state)</span><br><span class="line">            output = self.classifier(output.squeeze(<span class="number">0</span>))</span><br><span class="line">            logprobs = log_softmax(output, dim=<span class="number">1</span>)</span><br><span class="line">            logprobs, words = logprobs.topk(self.beam_size, <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> words.data, logprobs.data, new_states</span><br><span class="line"></span><br><span class="line">        partial_captions = TopN(self.beam_size)</span><br><span class="line">        complete_captions = TopN(self.beam_size)</span><br><span class="line"></span><br><span class="line">        words, logprobs, new_state = get_topk_words(rnn_input, initial_state)</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(self.beam_size):</span><br><span class="line">            cap = Caption(</span><br><span class="line">                sentence=[words[<span class="number">0</span>, k]],</span><br><span class="line">                state=new_state,</span><br><span class="line">                logprob=logprobs[<span class="number">0</span>, k],</span><br><span class="line">                score=logprobs[<span class="number">0</span>, k])</span><br><span class="line">            partial_captions.push(cap)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.max_caption_length - <span class="number">1</span>):</span><br><span class="line">            partial_captions_list = partial_captions.extract()</span><br><span class="line">            partial_captions.reset()</span><br><span class="line"></span><br><span class="line">            input_feed = t.LongTensor([c.sentence[<span class="number">-1</span>] <span class="keyword">for</span> c <span class="keyword">in</span> partial_captions_list])</span><br><span class="line">            <span class="keyword">if</span> rnn_input.is_cuda:</span><br><span class="line">                input_feed = input_feed.cuda()</span><br><span class="line"></span><br><span class="line">            input_feed.detach_()</span><br><span class="line">            state_feed = [c.state <span class="keyword">for</span> c <span class="keyword">in</span> partial_captions_list]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 这个地方可以重点关注一下</span></span><br><span class="line">            <span class="keyword">if</span> isinstance(state_feed[<span class="number">0</span>], tuple):</span><br><span class="line">                <span class="comment"># 这个地方为什么会用到 t.cat</span></span><br><span class="line">                state_feed_h, state_feed_c = zip(*state_feed)</span><br><span class="line">                state_feed = (t.cat(state_feed_h, <span class="number">1</span>),</span><br><span class="line">                              t.cat(state_feed_c, <span class="number">1</span>))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                state_feed = t.cat(state_feed, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            embeddings = self.embedder(input_feed).view(<span class="number">1</span>, len(input_feed), <span class="number">-1</span>)</span><br><span class="line">            words, logprobs, new_states = get_topk_words(embeddings, state_feed)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> i, partial_caption <span class="keyword">in</span> enumerate(partial_captions_list):</span><br><span class="line">                <span class="keyword">if</span> isinstance(new_states, tuple):</span><br><span class="line">                    state = (new_states[<span class="number">0</span>].narrow(<span class="number">1</span>, i, <span class="number">1</span>),</span><br><span class="line">                             new_states[<span class="number">1</span>].narrow(<span class="number">1</span>, i, <span class="number">1</span>))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    state = new_states[i]</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> range(self.beam_size):</span><br><span class="line">                    w = words[i, k]</span><br><span class="line">                    sentence = partial_caption.sentence + [w]</span><br><span class="line">                    logprob = partial_caption.logprob + logprobs[i, k]</span><br><span class="line">                    score = logprob</span><br><span class="line">                    <span class="keyword">if</span> w == self.eos_id:</span><br><span class="line">                        <span class="keyword">if</span> self.length_normalization_factor &gt; <span class="number">0</span>:</span><br><span class="line">                            score /= len(sentence)**self.length_normalization_factor</span><br><span class="line">                        beam = Caption(sentence, state, logprob, score)</span><br><span class="line">                        complete_captions.push(beam)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        beam = Caption(sentence, state, logprob, score)</span><br><span class="line">                        partial_captions.push(beam)</span><br><span class="line">            <span class="keyword">if</span> partial_captions.size() == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> complete_captions.size():</span><br><span class="line">            complete_captions = partial_captions</span><br><span class="line"></span><br><span class="line">        caps = complete_captions.extract(sort=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> [c.sentence <span class="keyword">for</span> c <span class="keyword">in</span> caps], [c.score <span class="keyword">for</span> c <span class="keyword">in</span> caps]</span><br></pre></td></tr></table></figure><ul><li><p>基于优先队列的堆实现的，注意一下这里py3之后的版本不通用的比较器</p></li><li><p><strong>上面说重点关注的地方有时间去看看，在选择数据之后的操作，弄不清楚。</strong></p></li></ul><h3 id="visdom封装模块"><a href="#visdom封装模块" class="headerlink" title="visdom封装模块"></a>visdom封装模块</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> visdom</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> torchvision <span class="keyword">as</span> tv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Visualizer</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, env=<span class="string">'default'</span>, **kwargs)</span>:</span></span><br><span class="line">        self.vis = visdom.Visdom(env=env, **kwargs)</span><br><span class="line">        self.index = &#123;&#125;</span><br><span class="line">        self.log_text = <span class="string">''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reinit</span><span class="params">(self, env=<span class="string">'default'</span>, **kwargs)</span>:</span></span><br><span class="line">        self.vis = visdom.Visdom(env=env, **kwargs)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot_many</span><span class="params">(self, d)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> d.items():</span><br><span class="line">            self.plot(k, v)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">img_many</span><span class="params">(self, d)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> d.items():</span><br><span class="line">            self.img(k, v)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">(self, name, y)</span>:</span></span><br><span class="line">        x = self.index.get(name, <span class="number">0</span>)</span><br><span class="line">        self.vis.line(Y=np.array([y]), X=np.array([x]),</span><br><span class="line">                      win=name,</span><br><span class="line">                      opts=dict(title=name),</span><br><span class="line">                      update=<span class="keyword">None</span> <span class="keyword">if</span> x == <span class="number">0</span> <span class="keyword">else</span> <span class="string">'append'</span></span><br><span class="line">                      )</span><br><span class="line">        self.index[name] = x + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">img</span><span class="params">(self, name, img_, caption=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> len(img_.size()) &lt; <span class="number">3</span>:</span><br><span class="line">            img_ = img_.cpu().unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.vis.image(img_.cpu(),</span><br><span class="line">                       win=name,</span><br><span class="line">                       opts=dict(title=name, caption=caption)</span><br><span class="line">                       )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">img_grid_many</span><span class="params">(self, d)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> d.items():</span><br><span class="line">            self.img_grid(k, v)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">img_grid</span><span class="params">(self, name, input_3d)</span>:</span></span><br><span class="line">        self.img(name, tv.utils.make_grid(</span><br><span class="line">            input_3d.cpu()[<span class="number">0</span>].unsqueeze(<span class="number">1</span>).clamp(max=<span class="number">1</span>, min=<span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">log</span><span class="params">(self, info, win=<span class="string">'log_text'</span>)</span>:</span></span><br><span class="line">        self.log_text += (<span class="string">'[&#123;time&#125;] &#123;info&#125; &lt;br&gt;'</span>.format(</span><br><span class="line">            time=time.strftime(<span class="string">'%m%d_%H%M%S'</span>),</span><br><span class="line">            info=info))</span><br><span class="line">        self.vis.text(self.log_text, win=win)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getattr__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> getattr(self.vis, name)</span><br></pre></td></tr></table></figure><p>这个地方没什么好讲的，直接用就行了，工具而已</p><h3 id="主模块和配置类"><a href="#主模块和配置类" class="headerlink" title="主模块和配置类"></a>主模块和配置类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Config</span>:</span></span><br><span class="line">    caption_data_path = <span class="string">'caption.pth'</span></span><br><span class="line">    img_path = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    img_feature_path = <span class="string">'result.pth'</span></span><br><span class="line">    scale_size = <span class="number">300</span></span><br><span class="line">    img_size = <span class="number">224</span></span><br><span class="line">    batch_size = <span class="number">8</span></span><br><span class="line">    shuffle = <span class="keyword">True</span></span><br><span class="line">    num_workers = <span class="number">4</span></span><br><span class="line">    rnn_hidden = <span class="number">256</span></span><br><span class="line">    embedding_dim = <span class="number">256</span></span><br><span class="line">    num_layers = <span class="number">2</span></span><br><span class="line">    share_embedding_weights = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">    prefix = <span class="string">'checkpoints/caption'</span></span><br><span class="line">    env=<span class="string">'caption'</span></span><br><span class="line">    plot_every=<span class="number">10</span></span><br><span class="line">    debug_file=<span class="string">'/tmp/debug'</span></span><br><span class="line"></span><br><span class="line">    model_ckpt = <span class="string">"ImageCaption"</span></span><br><span class="line">    lr = <span class="number">1e-3</span></span><br><span class="line">    use_gpu = <span class="keyword">False</span></span><br><span class="line">    epoch = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    test_img = <span class="string">'example.jpeg'</span></span><br><span class="line">    test_prefix = <span class="string">'img/'</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torchvision <span class="keyword">as</span> tv</span><br><span class="line"><span class="keyword">from</span> torchnet <span class="keyword">import</span> meter</span><br><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> ipdb</span><br><span class="line"><span class="keyword">import</span> fire</span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pack_padded_sequence</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> CaptionModel</span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> utils.visualize <span class="keyword">import</span> Visualizer</span><br><span class="line"><span class="keyword">from</span> data <span class="keyword">import</span> get_dataloader</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">IMAGENET_MEAN = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">IMAGENET_STD = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate</span><span class="params">(**kwargs)</span>:</span></span><br><span class="line">    opt = Config()</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> kwargs.items():</span><br><span class="line">        setattr(opt, k, v)</span><br><span class="line">    device=t.device(<span class="string">'cuda'</span>) <span class="keyword">if</span> opt.use_gpu <span class="keyword">else</span> t.device(<span class="string">'cpu'</span>)</span><br><span class="line">    data = t.load(opt.caption_data_path, map_location=<span class="keyword">lambda</span> s, l: s)</span><br><span class="line">    word2ix, ix2word = data[<span class="string">'word2ix'</span>], data[<span class="string">'ix2word'</span>]</span><br><span class="line"></span><br><span class="line">    normalize = tv.transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)</span><br><span class="line">    transforms = tv.transforms.Compose([</span><br><span class="line">        tv.transforms.Resize(opt.scale_size),</span><br><span class="line">        tv.transforms.CenterCrop(opt.img_size),</span><br><span class="line">        tv.transforms.ToTensor(),</span><br><span class="line">        normalize</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    img = Image.open(opt.test_prefix + opt.test_img)</span><br><span class="line">    img = transforms(img).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用resnet50来提取图片特征</span></span><br><span class="line">    resnet50 = tv.models.resnet50(<span class="keyword">False</span>).eval()</span><br><span class="line">    resnet50.load_state_dict(t.load(<span class="string">"checkpoints/resnet50-19c8e357.pth"</span>))</span><br><span class="line">    <span class="keyword">del</span> resnet50.fc</span><br><span class="line">    resnet50.fc = <span class="keyword">lambda</span> x: x</span><br><span class="line">    resnet50.to(device)</span><br><span class="line">    img = img.to(device)</span><br><span class="line">    img_feats = resnet50(img).detach()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Caption模型</span></span><br><span class="line">    model = CaptionModel(opt, word2ix, ix2word)</span><br><span class="line">    model = model.load(opt.model_ckpt).eval()</span><br><span class="line">    model.to(device)</span><br><span class="line"></span><br><span class="line">    results = model.generate(img_feats.data[<span class="number">0</span>])</span><br><span class="line">    print(results)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(**kwargs)</span>:</span></span><br><span class="line">    opt = Config()</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> kwargs.items():</span><br><span class="line">        setattr(opt, k, v)</span><br><span class="line">    device=t.device(<span class="string">'cuda'</span>) <span class="keyword">if</span> opt.use_gpu <span class="keyword">else</span> t.device(<span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line">    opt.caption_data_path = <span class="string">'caption.pth'</span>  <span class="comment"># 原始数据</span></span><br><span class="line">    opt.test_img = <span class="string">''</span>  <span class="comment"># 输入图片</span></span><br><span class="line">    <span class="comment"># 数据</span></span><br><span class="line">    vis = Visualizer(env=opt.env)</span><br><span class="line">    dataloader = get_dataloader(opt)</span><br><span class="line">    _data = dataloader.dataset._data</span><br><span class="line">    word2ix, ix2word = _data[<span class="string">'word2ix'</span>], _data[<span class="string">'ix2word'</span>]</span><br><span class="line">    <span class="comment"># 模型</span></span><br><span class="line">    model = CaptionModel(opt, word2ix, ix2word)</span><br><span class="line">    <span class="keyword">if</span> opt.model_ckpt:</span><br><span class="line">        model.load(opt.model_ckpt)</span><br><span class="line">    optimizer = model.get_optimizer(opt.lr)</span><br><span class="line">    criterion = t.nn.CrossEntropyLoss()</span><br><span class="line">    model.to(device)</span><br><span class="line">    <span class="comment"># 统计</span></span><br><span class="line">    loss_meter = meter.AverageValueMeter()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(opt.epoch):</span><br><span class="line">        loss_meter.reset()</span><br><span class="line">        <span class="keyword">for</span> ii, (imgs, (captions, lengths), indexes) <span class="keyword">in</span> tqdm.tqdm(enumerate(dataloader)):</span><br><span class="line">            <span class="comment"># 训练</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            imgs = imgs.to(device)</span><br><span class="line">            captions = captions.to(device)</span><br><span class="line">            input_captions = captions[:<span class="number">-1</span>]</span><br><span class="line">            target_captions = pack_padded_sequence(captions, lengths)[<span class="number">0</span>]</span><br><span class="line">            score, _ = model(imgs, input_captions, lengths)</span><br><span class="line">            loss = criterion(score, target_captions)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            loss_meter.add(loss.item())</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 可视化</span></span><br><span class="line">            <span class="keyword">if</span> (ii + <span class="number">1</span>) % opt.plot_every == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">if</span> os.path.exists(opt.debug_file):</span><br><span class="line">                    ipdb.set_trace()</span><br><span class="line"></span><br><span class="line">                vis.plot(<span class="string">'loss'</span>, loss_meter.value()[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 可视化原始图片 + 可视化人工的描述语句</span></span><br><span class="line">                raw_img = _data[<span class="string">'ix2id'</span>][indexes[<span class="number">0</span>]]</span><br><span class="line">                img_path = opt.img_path + raw_img</span><br><span class="line">                raw_img = Image.open(img_path).convert(<span class="string">'RGB'</span>)</span><br><span class="line">                raw_img = tv.transforms.ToTensor()(raw_img)</span><br><span class="line"></span><br><span class="line">                raw_caption = captions.data[:, <span class="number">0</span>]</span><br><span class="line">                raw_caption = <span class="string">''</span>.join([_data[<span class="string">'ix2word'</span>][ii] <span class="keyword">for</span> ii <span class="keyword">in</span> raw_caption])</span><br><span class="line">                vis.text(raw_caption, <span class="string">u'raw_caption'</span>)</span><br><span class="line">                vis.img(<span class="string">'raw'</span>, raw_img, caption=raw_caption)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 可视化网络生成的描述语句</span></span><br><span class="line">                results = model.generate(imgs.data[<span class="number">0</span>])</span><br><span class="line">                vis.text(<span class="string">'&lt;/br&gt;'</span>.join(results), <span class="string">u'caption'</span>)</span><br><span class="line">        model.save()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    fire.Fire()</span><br></pre></td></tr></table></figure><ul><li>pack_padded_sequence 这里有一个这样的包装，是自动为你封装好数据的长短，它不仅保证长度相同，并且在内部有优化加速。直接防止padding也加入到了模型中去计算。</li></ul><p><strong>花时间深入了解以下 beam search</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
      <category term="机器学习" scheme="http://wsx1128.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://wsx1128.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Pytorch" scheme="http://wsx1128.cn/tags/Pytorch/"/>
    
      <category term="图像描述" scheme="http://wsx1128.cn/tags/%E5%9B%BE%E5%83%8F%E6%8F%8F%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>RNN生成唐诗宋词-基于深度学习框架pytorch</title>
    <link href="http://wsx1128.cn/2020/03/15/RNN%E7%94%9F%E6%88%90%E5%94%90%E8%AF%97%E5%AE%8B%E8%AF%8D-%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6pytorch/"/>
    <id>http://wsx1128.cn/2020/03/15/RNN生成唐诗宋词-基于深度学习框架pytorch/</id>
    <published>2020-03-15T10:16:58.000Z</published>
    <updated>2020-03-15T14:54:01.320Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>这次是基于RNN深度学习模型应用的一个小模型，通过训练大量的唐诗宋词的例子，将学习其中内部逻辑，并且根据给出的开头诗词和风格诗句，来生成自己的诗歌，并且还有是否生成藏头诗的限定哦</p><p>还是按照相应的步骤一次次的来</p><h3 id="数据总揽以及处理"><a href="#数据总揽以及处理" class="headerlink" title="数据总揽以及处理"></a>数据总揽以及处理</h3><p>这里有两种数据，一种是没有经过处理的原始数据，也就是 全部都是诗歌文字，没有半点处理。<br>第二种就是经过处理之后的数据。</p><p>这里不做要求了解处理数据的内容，因为不属于模型构建的内容，所以可以跳过哦。</p><p>处理的过程分为以下几种</p><ul><li><p>给每句诗词将开头结尾都加上特定的标识符号。</p></li><li><p>将诗句的作者，题目，作为该诗词的属性，不作为诗词的内容展览出。</p></li><li><p>注意诗词内容中，不仅仅是文字，还有标点符号的编码都需要修改以下</p></li></ul><p>下面贴出处理数据的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_parseRawData</span><span class="params">(author=None, constrain=None, src=<span class="string">'./chinese-poetry/json/simplified'</span>, category=<span class="string">"poet.tang"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    code from https://github.com/justdark/pytorch-poetry-gen/blob/master/dataHandler.py</span></span><br><span class="line"><span class="string">    处理json文件，返回诗歌内容</span></span><br><span class="line"><span class="string">    @param: author： 作者名字</span></span><br><span class="line"><span class="string">    @param: constrain: 长度限制</span></span><br><span class="line"><span class="string">    @param: src: json 文件存放路径</span></span><br><span class="line"><span class="string">    @param: category: 类别，有poet.song 和 poet.tang</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回 data：list</span></span><br><span class="line"><span class="string">        ['床前明月光，疑是地上霜，举头望明月，低头思故乡。',</span></span><br><span class="line"><span class="string">         '一去二三里，烟村四五家，亭台六七座，八九十支花。',</span></span><br><span class="line"><span class="string">        .........</span></span><br><span class="line"><span class="string">        ]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sentenceParse</span><span class="params">(para)</span>:</span></span><br><span class="line">        <span class="comment"># para 形如 "-181-村橋路不端，數里就迴湍。積壤連涇脉，高林上笋竿。早嘗甘蔗淡，</span></span><br><span class="line">        <span class="comment"># 生摘琵琶酸。（「琵琶」，嚴壽澄校《張祜詩集》云：疑「枇杷」之誤。）</span></span><br><span class="line">        <span class="comment"># 好是去塵俗，煙花長一欄。"</span></span><br><span class="line">        result, number = re.subn(<span class="string">u"（.*）"</span>, <span class="string">""</span>, para)</span><br><span class="line">        result, number = re.subn(<span class="string">u"&#123;.*&#125;"</span>, <span class="string">""</span>, result)</span><br><span class="line">        result, number = re.subn(<span class="string">u"《.*》"</span>, <span class="string">""</span>, result)</span><br><span class="line">        result, number = re.subn(<span class="string">u"《.*》"</span>, <span class="string">""</span>, result)</span><br><span class="line">        result, number = re.subn(<span class="string">u"[\]\[]"</span>, <span class="string">""</span>, result)</span><br><span class="line">        r = <span class="string">""</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> result:</span><br><span class="line">            <span class="keyword">if</span> s <span class="keyword">not</span> <span class="keyword">in</span> set(<span class="string">'0123456789-'</span>):</span><br><span class="line">                r += s</span><br><span class="line">        r, number = re.subn(<span class="string">u"。。"</span>, <span class="string">u"。"</span>, r)</span><br><span class="line">        <span class="keyword">return</span> r</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">handleJson</span><span class="params">(file)</span>:</span></span><br><span class="line">        <span class="comment"># print file</span></span><br><span class="line">        rst = []</span><br><span class="line">        data = json.loads(open(file).read())</span><br><span class="line">        <span class="keyword">for</span> poetry <span class="keyword">in</span> data:</span><br><span class="line">            pdata = <span class="string">""</span></span><br><span class="line">            <span class="keyword">if</span> (author <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> poetry.get(<span class="string">"author"</span>) != author):</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            p = poetry.get(<span class="string">"paragraphs"</span>)</span><br><span class="line">            flag = <span class="keyword">False</span></span><br><span class="line">            <span class="keyword">for</span> s <span class="keyword">in</span> p:</span><br><span class="line">                sp = re.split(<span class="string">u"[，！。]"</span>, s)</span><br><span class="line">                <span class="keyword">for</span> tr <span class="keyword">in</span> sp:</span><br><span class="line">                    <span class="keyword">if</span> constrain <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">and</span> len(tr) != constrain <span class="keyword">and</span> len(tr) != <span class="number">0</span>:</span><br><span class="line">                        flag = <span class="keyword">True</span></span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                    <span class="keyword">if</span> flag:</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> flag:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">for</span> sentence <span class="keyword">in</span> poetry.get(<span class="string">"paragraphs"</span>):</span><br><span class="line">                pdata += sentence</span><br><span class="line">            pdata = sentenceParse(pdata)</span><br><span class="line">            <span class="keyword">if</span> pdata != <span class="string">""</span>:</span><br><span class="line">                rst.append(pdata)</span><br><span class="line">        <span class="keyword">return</span> rst</span><br><span class="line"></span><br><span class="line">    data = []</span><br><span class="line">    <span class="keyword">for</span> filename <span class="keyword">in</span> os.listdir(src):</span><br><span class="line">        <span class="keyword">if</span> filename.startswith(category):</span><br><span class="line">            data.extend(handleJson(src + filename))</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_sequences</span><span class="params">(sequences,</span></span></span><br><span class="line"><span class="function"><span class="params">                  maxlen=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                  dtype=<span class="string">'int32'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                  padding=<span class="string">'pre'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                  truncating=<span class="string">'pre'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                  value=<span class="number">0.</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    code from keras</span></span><br><span class="line"><span class="string">    Pads each sequence to the same length (length of the longest sequence).</span></span><br><span class="line"><span class="string">    If maxlen is provided, any sequence longer</span></span><br><span class="line"><span class="string">    than maxlen is truncated to maxlen.</span></span><br><span class="line"><span class="string">    Truncation happens off either the beginning (default) or</span></span><br><span class="line"><span class="string">    the end of the sequence.</span></span><br><span class="line"><span class="string">    Supports post-padding and pre-padding (default).</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        sequences: list of lists where each element is a sequence</span></span><br><span class="line"><span class="string">        maxlen: int, maximum length</span></span><br><span class="line"><span class="string">        dtype: type to cast the resulting sequence.</span></span><br><span class="line"><span class="string">        padding: 'pre' or 'post', pad either before or after each sequence.</span></span><br><span class="line"><span class="string">        truncating: 'pre' or 'post', remove values from sequences larger than</span></span><br><span class="line"><span class="string">            maxlen either in the beginning or in the end of the sequence</span></span><br><span class="line"><span class="string">        value: float, value to pad the sequences to the desired value.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        x: numpy array with dimensions (number_of_sequences, maxlen)</span></span><br><span class="line"><span class="string">    Raises:</span></span><br><span class="line"><span class="string">        ValueError: in case of invalid values for `truncating` or `padding`,</span></span><br><span class="line"><span class="string">            or in case of invalid shape for a `sequences` entry.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> hasattr(sequences, <span class="string">'__len__'</span>):</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'`sequences` must be iterable.'</span>)</span><br><span class="line">    lengths = []</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> sequences:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> hasattr(x, <span class="string">'__len__'</span>):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'`sequences` must be a list of iterables. '</span></span><br><span class="line">                             <span class="string">'Found non-iterable: '</span> + str(x))</span><br><span class="line">        lengths.append(len(x))</span><br><span class="line"></span><br><span class="line">    num_samples = len(sequences)</span><br><span class="line">    <span class="keyword">if</span> maxlen <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        maxlen = np.max(lengths)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># take the sample shape from the first non empty sequence</span></span><br><span class="line">    <span class="comment"># checking for consistency in the main loop below.</span></span><br><span class="line">    sample_shape = tuple()</span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> sequences:</span><br><span class="line">        <span class="keyword">if</span> len(s) &gt; <span class="number">0</span>:  <span class="comment"># pylint: disable=g-explicit-length-test</span></span><br><span class="line">            sample_shape = np.asarray(s).shape[<span class="number">1</span>:]</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    x = (np.ones((num_samples, maxlen) + sample_shape) * value).astype(dtype)</span><br><span class="line">    <span class="keyword">for</span> idx, s <span class="keyword">in</span> enumerate(sequences):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> len(s):  <span class="comment"># pylint: disable=g-explicit-length-test</span></span><br><span class="line">            <span class="keyword">continue</span>  <span class="comment"># empty list/array was found</span></span><br><span class="line">        <span class="keyword">if</span> truncating == <span class="string">'pre'</span>:</span><br><span class="line">            trunc = s[-maxlen:]  <span class="comment"># pylint: disable=invalid-unary-operand-type</span></span><br><span class="line">        <span class="keyword">elif</span> truncating == <span class="string">'post'</span>:</span><br><span class="line">            trunc = s[:maxlen]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'Truncating type "%s" not understood'</span> % truncating)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># check `trunc` has expected shape</span></span><br><span class="line">        trunc = np.asarray(trunc, dtype=dtype)</span><br><span class="line">        <span class="keyword">if</span> trunc.shape[<span class="number">1</span>:] != sample_shape:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">'Shape of sample %s of sequence at position %s is different from '</span></span><br><span class="line">                <span class="string">'expected shape %s'</span></span><br><span class="line">                % (trunc.shape[<span class="number">1</span>:], idx, sample_shape))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> padding == <span class="string">'post'</span>:</span><br><span class="line">            x[idx, :len(trunc)] = trunc</span><br><span class="line">        <span class="keyword">elif</span> padding == <span class="string">'pre'</span>:</span><br><span class="line">            x[idx, -len(trunc):] = trunc</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'Padding type "%s" not understood'</span> % padding)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">(opt)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    @param opt 配置选项 Config对象</span></span><br><span class="line"><span class="string">    @return word2ix: dict,每个字对应的序号，形如u'月'-&gt;100</span></span><br><span class="line"><span class="string">    @return ix2word: dict,每个序号对应的字，形如'100'-&gt;u'月'</span></span><br><span class="line"><span class="string">    @return data: numpy数组，每一行是一首诗对应的字的下标</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(opt.pickle_path):</span><br><span class="line">        data = np.load(opt.pickle_path, allow_pickle=<span class="keyword">True</span>)</span><br><span class="line">        data, word2ix, ix2word = data[<span class="string">'data'</span>], data[<span class="string">'word2ix'</span>].item(), data[<span class="string">'ix2word'</span>].item()</span><br><span class="line">        <span class="keyword">return</span> data, word2ix, ix2word</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果没有处理好的二进制文件，则处理原始的json文件</span></span><br><span class="line">    data = _parseRawData(opt.author, opt.constrain, opt.data_path, opt.category)</span><br><span class="line">    words = &#123;_word <span class="keyword">for</span> _sentence <span class="keyword">in</span> data <span class="keyword">for</span> _word <span class="keyword">in</span> _sentence&#125;</span><br><span class="line">    word2ix = &#123;_word: _ix <span class="keyword">for</span> _ix, _word <span class="keyword">in</span> enumerate(words)&#125;</span><br><span class="line">    word2ix[<span class="string">'&lt;EOP&gt;'</span>] = len(word2ix)  <span class="comment"># 终止标识符</span></span><br><span class="line">    word2ix[<span class="string">'&lt;START&gt;'</span>] = len(word2ix)  <span class="comment"># 起始标识符</span></span><br><span class="line">    word2ix[<span class="string">'&lt;/s&gt;'</span>] = len(word2ix)  <span class="comment"># 空格</span></span><br><span class="line">    ix2word = &#123;_ix: _word <span class="keyword">for</span> _word, _ix <span class="keyword">in</span> list(word2ix.items())&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 为每首诗歌加上起始符和终止符</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data)):</span><br><span class="line">        data[i] = [<span class="string">"&lt;START&gt;"</span>] + list(data[i]) + [<span class="string">"&lt;EOP&gt;"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将每首诗歌保存的内容由‘字’变成‘数’</span></span><br><span class="line">    <span class="comment"># 形如[春,江,花,月,夜]变成[1,2,3,4,5]</span></span><br><span class="line">    new_data = [[word2ix[_word] <span class="keyword">for</span> _word <span class="keyword">in</span> _sentence]</span><br><span class="line">                <span class="keyword">for</span> _sentence <span class="keyword">in</span> data]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 诗歌长度不够opt.maxlen的在前面补空格，超过的，删除末尾的</span></span><br><span class="line">    pad_data = pad_sequences(new_data,</span><br><span class="line">                             maxlen=opt.maxlen,</span><br><span class="line">                             padding=<span class="string">'pre'</span>,</span><br><span class="line">                             truncating=<span class="string">'post'</span>,</span><br><span class="line">                             value=len(word2ix) - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存成二进制文件</span></span><br><span class="line">    np.savez_compressed(opt.pickle_path,</span><br><span class="line">                        data=pad_data,</span><br><span class="line">                        word2ix=word2ix,</span><br><span class="line">                        ix2word=ix2word)</span><br><span class="line">    <span class="keyword">return</span> pad_data, word2ix, ix2word</span><br></pre></td></tr></table></figure><p>这里按照步骤详细的说明一下 处理生数据的过程：<br>因为生图片格式为json，相当于一个大型字典，所以首先是要把这个大型数据进行一个处理。</p><ul><li><p>首先分好相应的类别，比如，特定的作者，特定的朝代类别，并且提取出相应的内容，并且在遍历的过程中一步一步的检测，有没有字数限制，作者限制之类的。</p></li><li><p>然后对内容进行限制，这个时候就要用到特定的包 <strong>re</strong> 正则表达式的包，把所有的注释，所有一切稀奇古怪的字符全部删除替换掉。</p></li><li><p>将每一个字进行一个编号，放入不可以重复的set中，并且建立起相应的映射，并且在每一句诗词的开始与结尾都加上标识符号</p></li><li><p>给每一首诗的长度进行限制，多的截断，少的加上padding，并且直接变成一个numpy里的数据</p></li><li><p>不过这里我多了一个疑问，最后输出来的数据的形状是 batch_size <em> max_len </em> words_in_len ??? 我就没有懂，最后不就是最长max_len吗？ </p></li></ul><p>这个问题留到后面去解决，经过我过多的研究，我认为应该是代码错了，这个地方不应该去管每一句有多少词语的事情，明明一个二维数组，却变成了一个三维数组，真的有点搞笑</p><p>好了好长时间去看上面那个问题，结果确定了应该是错了的，编写这个代码的人 真的不用心呀</p><p>接下来 就到模型的构建了，唯一需要注意的是各个参数的设置</p><h3 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PoetModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, word_vec, hidden_dim)</span>:</span></span><br><span class="line">        super(PoetModel, self).__init__()</span><br><span class="line">        <span class="comment"># 之前一直不太理解这里的隐藏纬度是什么意思，后面就知道了用于描述状态的节点数</span></span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.embeddings = nn.Embedding(vocab_size, word_vec)</span><br><span class="line">        self.lstm = nn.LSTM(word_vec, self.hidden_dim, num_layers=<span class="number">2</span>)</span><br><span class="line">        self.linear1 = nn.Linear(self.hidden_dim, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden=None)</span>:</span></span><br><span class="line">        seq_len, batch_size = input.size()</span><br><span class="line">        <span class="keyword">if</span> hidden <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            h_0 = input.data.new(<span class="number">2</span>, batch_size, self.hidden_dim).fill_(<span class="number">0</span>).float()</span><br><span class="line">            c_0 = input.data.new(<span class="number">2</span>, batch_size, self.hidden_dim).fill_(<span class="number">0</span>).float()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            h_0,c_0 = hidden</span><br><span class="line">        embeds = self.embeddings(input)</span><br><span class="line">        output, hidden = self.lstm(embeds, (h_0, c_0))</span><br><span class="line">        output = self.linear1(output.view(seq_len * batch_size, <span class="number">-1</span>))</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br></pre></td></tr></table></figure><p>三点注意：</p><ul><li><p>Embedding 过程介绍的是所有词语的个数和词向量的个数，这里的词向量是需要自己去学习的词向量</p></li><li><p>lstm中的 hidden_dim 指的就是在这个模型中隐藏元的计算个数，后面num_layers就指的是模型层数咯</p></li><li><p>当时在写这个线性层数的时候，我也最终考虑了好久，线性层 x_feature 和 y_feature 可以直接这样写，最后输出的一定是m_batch * y_feature，这里就是所有的词语数，还有 这里的h0，c0需要保留，应该包含了记忆系数，或者是忘记系数。</p></li><li><p>ho,co的形状必须是（num_layer， batch_size， hidden_dim（隐藏计算机元））</p></li></ul><p>接下来便是主模块和处理类</p><h3 id="主模块和处理类："><a href="#主模块和处理类：" class="headerlink" title="主模块和处理类："></a>主模块和处理类：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Config</span><span class="params">(object)</span>:</span></span><br><span class="line">    pickle_path = <span class="string">'data/tang.npz'</span></span><br><span class="line">    author = <span class="keyword">None</span></span><br><span class="line">    constrain = <span class="keyword">None</span></span><br><span class="line">    category = <span class="string">'poet.tang'</span></span><br><span class="line">    lr = <span class="number">1e-3</span></span><br><span class="line">    weight_dacay = <span class="number">1e-4</span></span><br><span class="line">    use_gpu = <span class="keyword">False</span></span><br><span class="line">    max_epoch = <span class="number">251</span></span><br><span class="line">    batch_size = <span class="number">128</span></span><br><span class="line">    maxlen = <span class="number">125</span></span><br><span class="line">    plot_erevy = <span class="number">20</span></span><br><span class="line">    env = <span class="string">'poetry'</span></span><br><span class="line">    max_gen_len = <span class="number">200</span>       <span class="comment"># 生成诗歌的长度</span></span><br><span class="line">    debug_file = <span class="string">'/tmp/debug'</span></span><br><span class="line">    model_prefix = <span class="string">'checkpoints/tang'</span></span><br><span class="line"></span><br><span class="line">    acrostic = <span class="keyword">False</span></span><br><span class="line">    model_path = <span class="string">'checkpoints/tang_model'</span></span><br><span class="line">    prefix_words = <span class="keyword">None</span></span><br><span class="line">    start_words = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">opt = Config()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ipdb</span><br><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> fire</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">import</span> torch.utils.data</span><br><span class="line"><span class="keyword">from</span> data <span class="keyword">import</span> get_data</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> PoetModel</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> Visualizer</span><br><span class="line"><span class="keyword">from</span> torchnet <span class="keyword">import</span> meter</span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> opt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(**kwargs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> kwargs.items():</span><br><span class="line">        setattr(opt, k, v)</span><br><span class="line">    opt.device = t.device(<span class="string">'cuda'</span>) <span class="keyword">if</span> opt.use_gpu <span class="keyword">else</span> t.device(<span class="string">'cpu'</span>)</span><br><span class="line">    vis = Visualizer(env=opt.env)</span><br><span class="line"></span><br><span class="line">    data, word2ix, ix2word = get_data(opt)</span><br><span class="line">    data = t.from_numpy(data)</span><br><span class="line">    dataloader = t.utils.data.DataLoader(data, batch_size=opt.batch_size, shuffle=<span class="keyword">True</span>, num_workers=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    model = PoetModel(len(word2ix), <span class="number">128</span>, <span class="number">256</span>)</span><br><span class="line">    optim = t.optim.Adam(model.parameters(), lr= opt.lr)</span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> opt.model_path:</span><br><span class="line">        model.load_state_dict(t.load(opt.model_path))</span><br><span class="line">    model.to(opt.device)</span><br><span class="line"></span><br><span class="line">    loss_meter = meter.AverageValueMeter()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(opt.max_epoch):</span><br><span class="line">        loss_meter.reset()</span><br><span class="line">        <span class="keyword">for</span> ii, data_ <span class="keyword">in</span> tqdm.tqdm(enumerate(dataloader)):</span><br><span class="line">            data_ = data_.long().transpose(<span class="number">1</span>, <span class="number">0</span>).contiguous()</span><br><span class="line">            data_ = data_.to(opt.device)</span><br><span class="line">            optim.zero_grad()</span><br><span class="line">            input_, target_ = data_[:<span class="number">-1</span>, :], data_[<span class="number">1</span>:,:]</span><br><span class="line">            output_, _ = model(input_)</span><br><span class="line">            loss = criterion(output_, target_.view(<span class="number">-1</span>))</span><br><span class="line">            loss.backward()</span><br><span class="line">            optim.step()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 这里必须写上item() 因为生成的是Tensor类型，防止动态图的显存会炸</span></span><br><span class="line">            loss_meter.add(loss.item())</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (ii + <span class="number">1</span>) % opt.plot_erevy == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">if</span> os.path.exists(opt.debug_file):</span><br><span class="line">                    ipdb.set_trace()</span><br><span class="line"></span><br><span class="line">                vis.plot(<span class="string">'loss'</span>, loss_meter.value()[<span class="number">0</span>])</span><br><span class="line">                poetrys = [[ix2word[_word] <span class="keyword">for</span> _word <span class="keyword">in</span> data_[:, _iii].tolist()]</span><br><span class="line">                          <span class="keyword">for</span> _iii <span class="keyword">in</span> range(data_.shape[<span class="number">1</span>])][:<span class="number">16</span>]</span><br><span class="line">                vis.text(<span class="string">'&lt;/br&gt;'</span>.join([<span class="string">''</span>.join(poetry) <span class="keyword">for</span> poetry <span class="keyword">in</span> poetrys]), win=<span class="string">u'origin_poem'</span>)</span><br><span class="line"></span><br><span class="line">                gen_poetries = []</span><br><span class="line">                <span class="comment">#接下来就是验证 模型在训练过程中生成的诗词</span></span><br><span class="line">                <span class="keyword">for</span> word <span class="keyword">in</span> list(<span class="string">u'春江花月夜凉如水'</span>):</span><br><span class="line">                    gen_poetry = <span class="string">''</span>.join(generate(model, word, ix2word, word2ix))</span><br><span class="line">                    gen_poetries.append(gen_poetry)</span><br><span class="line"></span><br><span class="line">                vis.text(<span class="string">'&lt;/br&gt;'</span>.join([<span class="string">''</span>.join(poetry) <span class="keyword">for</span> poetry <span class="keyword">in</span> gen_poetries]), win=<span class="string">u'gen_poem'</span>)</span><br><span class="line"></span><br><span class="line">        t.save(model.state_dict(), <span class="string">'%s_%s.pth'</span> %(opt.model_prefix, epoch))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen</span><span class="params">(**kwargs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> kwargs.items():</span><br><span class="line">        setattr(opt, k, v)</span><br><span class="line">    data, word2ix, ix2word = get_data(opt)</span><br><span class="line">    device = t.device(<span class="string">'cuda'</span>) <span class="keyword">if</span> opt.use_gpu <span class="keyword">else</span> t.device(<span class="string">'cpu'</span>)</span><br><span class="line">    model = PoetModel(len(word2ix), <span class="number">128</span>, <span class="number">256</span>)</span><br><span class="line">    model.load_state_dict(t.load(opt.model_path, map_location= <span class="keyword">lambda</span> s, _: s))</span><br><span class="line">    model.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 处理字符串</span></span><br><span class="line">    <span class="comment"># 并且考虑到两个版本之间的差异，将其兼容了一下</span></span><br><span class="line">    <span class="keyword">if</span> sys.version_info.major  == <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">if</span> opt.start_words.isprintable():</span><br><span class="line">            start_words = opt.start_words</span><br><span class="line">            prefix_words = opt.prefix_words</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            start_words = opt.start_words.encode(<span class="string">'ascii'</span>, <span class="string">'surrogateescape'</span>).decode(<span class="string">'utf8'</span>)</span><br><span class="line">            prefix_words = opt.prefix_words.encode(<span class="string">'ascii'</span>, <span class="string">'surrogateescape'</span>).decode(<span class="string">'utf8'</span>) <span class="keyword">if</span> opt.prefix_words <span class="keyword">else</span> <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        start_words = opt.start_words.decode(<span class="string">'utf8'</span>)</span><br><span class="line">        prefix_words = opt.prefix_words.decode(<span class="string">'utf8'</span>) <span class="keyword">if</span> opt.prefix_words <span class="keyword">else</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    start_words = opt.start_words.replace(<span class="string">','</span>, <span class="string">u'，'</span>).replace(<span class="string">'?'</span>, <span class="string">u'？'</span>).replace(<span class="string">'.'</span>, <span class="string">u'。'</span>)</span><br><span class="line">    gen_poetry = gen_acrostic <span class="keyword">if</span> opt.acrostic <span class="keyword">else</span> generate</span><br><span class="line">    result = gen_poetry(model, start_words, ix2word, word2ix, prefix_words)</span><br><span class="line">    print(<span class="string">''</span>.join(result))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate</span><span class="params">(model, start_words, ix2word, word2ix, prefix_words= None)</span>:</span></span><br><span class="line">    result = list(start_words)</span><br><span class="line">    start_words_len = len(start_words)</span><br><span class="line">    input = t.Tensor([word2ix[<span class="string">'&lt;START&gt;'</span>]]).view(<span class="number">1</span>,<span class="number">1</span>).long()</span><br><span class="line">    <span class="keyword">if</span> opt.use_gpu : input = input.cuda()</span><br><span class="line">    <span class="keyword">else</span>: input = input.cpu()</span><br><span class="line">    hidden = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> prefix_words <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> prefix_words:</span><br><span class="line">            output, hidden = model(input, hidden)</span><br><span class="line">            input = input.data.new([word2ix[word]]).view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(opt.max_gen_len):</span><br><span class="line">        output, hidden = model(input, hidden)</span><br><span class="line">        <span class="keyword">if</span> i &lt; start_words_len:</span><br><span class="line">            w = result[i]</span><br><span class="line">            input = input.data.new([word2ix[w]]).view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            top_index = output.data[<span class="number">0</span>].topk(<span class="number">1</span>)[<span class="number">1</span>][<span class="number">0</span>].item()</span><br><span class="line">            w = ix2word[top_index]</span><br><span class="line">            result.append(w)</span><br><span class="line">            input = input.data.new([top_index]).view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> w == <span class="string">'&lt;EOP&gt;'</span>:</span><br><span class="line">            <span class="keyword">del</span> result[<span class="number">-1</span>]</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_acrostic</span><span class="params">(model, start_words, ix2word, word2ix, prefix_words= None)</span>:</span></span><br><span class="line">    result = []</span><br><span class="line">    start_word_len = len(start_words)</span><br><span class="line">    input = (t.Tensor([word2ix[<span class="string">'&lt;START&gt;'</span>]]).view(<span class="number">1</span>, <span class="number">1</span>).long())</span><br><span class="line">    <span class="keyword">if</span> opt.use_gpu: input = input.cuda()</span><br><span class="line">    hidden = <span class="keyword">None</span></span><br><span class="line">    index = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 这里与前面相比 主要是多了一个 向前的词汇</span></span><br><span class="line">    pre_word = <span class="string">'&lt;START&gt;'</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> prefix_words:</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> prefix_words:</span><br><span class="line">            output, hidden = model(input, hidden)</span><br><span class="line">            input = input.data.new([word2ix[word]]).view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(opt.max_gen_len):</span><br><span class="line">        output, hidden = model(input, hidden)</span><br><span class="line">        top_index = output.data[<span class="number">0</span>].topk(<span class="number">1</span>)[<span class="number">1</span>][<span class="number">0</span>].item()</span><br><span class="line">        w = ix2word[top_index]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> pre_word <span class="keyword">in</span> &#123;<span class="string">u'。'</span>, <span class="string">u'！'</span>, <span class="string">'&lt;START&gt;'</span>&#125; :</span><br><span class="line">            <span class="keyword">if</span> index == start_word_len :</span><br><span class="line">                <span class="comment"># 这里就已经将藏头诗全部给用完了</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                w = start_words[index]</span><br><span class="line">                index += <span class="number">1</span></span><br><span class="line">                input = (input.data.new([word2ix[w]])).view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            input = input.data.new([word2ix[w]]).view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        result.append(w)</span><br><span class="line">        pre_word = w</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    fire.Fire()</span><br></pre></td></tr></table></figure><p><img src="/2020/03/15/RNN生成唐诗宋词-基于深度学习框架pytorch/1.jpg" alt=""></p><p>三点需要总结</p><ul><li><p>模型中的veb_size 指的集合中的词语，不是词向量，也不是所有的词语</p></li><li><p>上面的这些输入输出的参数形状一定要弄清楚</p></li><li><p>python的encode 和 decode有点奇怪start_words.encode(‘ascii’, ‘surrogateescape’).decode(‘utf8’)</p></li><li><p>后面生成过程中input的形状一定要改成view，并且要实时注意更新，更新的前一个文字的值</p></li><li><p>后面使用的topk（n）返回n个元祖 值和index取出来要item</p></li><li><p>关于这里训练的原理，就是为了使预测的下一个值贴近真实的下一个值</p></li><li><p>CrossEntropyLoss target只需要一个标量就行了，其的维度一定是和score的第一维度相同</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
      <category term="机器学习" scheme="http://wsx1128.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://wsx1128.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Pytorch" scheme="http://wsx1128.cn/tags/Pytorch/"/>
    
      <category term="RNN" scheme="http://wsx1128.cn/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>基于RNN的序列模型总结</title>
    <link href="http://wsx1128.cn/2020/03/15/%E5%9F%BA%E4%BA%8ERNN%E7%9A%84%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/"/>
    <id>http://wsx1128.cn/2020/03/15/基于RNN的序列模型总结/</id>
    <published>2020-03-15T05:29:03.000Z</published>
    <updated>2020-03-15T09:30:41.374Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h2><h3 id="序列模型的应用"><a href="#序列模型的应用" class="headerlink" title="序列模型的应用"></a>序列模型的应用</h3><p>一般对于序列模型 有如下应用：</p><ul><li><p>语音识别： 将输入的语言信号直接输出相应的语音文本信息。无论是语音信号还是文本信息均是序列数据。</p></li><li><p>音乐生成：生成音乐乐谱。只有输出的音乐乐谱是序列数据，输入可以是空或者一个整数。</p></li><li><p>情感分类：将输入的评论句子转换为相应的等级或评分。输入是一个序列，输出则是一个单独的类别。</p></li><li><p>DNA序列分析：找到输入的DNA序列的蛋白质表达的子序列。</p></li><li><p>机器翻译：两种不同语言之间的想换转换。输入和输出均为序列数据。</p></li><li><p>视频行为识别：识别输入的视频帧序列中的人物行为。</p></li><li><p>命名实体识别：从输入的句子中识别实体的名字。</p></li></ul><h3 id="数字符号"><a href="#数字符号" class="headerlink" title="数字符号"></a>数字符号</h3><ul><li><p>输入 $ x $：如“Harry Potter and Herminone Granger invented a new spell.”(以序列作为一个输入)， $ x^{<t>} $ 表示输入 x 中的第 t 个符号。</t></p></li><li><p>输出 $ y $ ：如“1 1 0 1 1 0 0 0 0”（人名定位），同样，用 $ y^{<t>} $ 表示输出 y 中的第 t 个符号。</t></p></li><li><p>$ T_x $ 用来表示输入x的长度</p></li><li><p>$ T_y $ 用来表示输出y的长度</p></li><li><p>利用单词字典编码来表示每一个输入的符号：如one-hot编码等，实现输入 x 和输出 y 之间的映射关系。</p></li></ul><h3 id="循环神经网络模型"><a href="#循环神经网络模型" class="headerlink" title="循环神经网络模型"></a>循环神经网络模型</h3><h4 id="传统标准的神经网络："><a href="#传统标准的神经网络：" class="headerlink" title="传统标准的神经网络："></a>传统标准的神经网络：</h4><p>对于学习 X 和 Y 的映射，我们可以很直接的想到一种方法就是使用传统的标准神经网络。也许我们可以将输入的序列X以某种方式进行字典编码以后，如one-hot编码，输入到一个多层的深度神经网络中，最后得到对应的输出 Y 。</p><p>但是，结果表明这种方法并不好，主要是存在下面两个问题：</p><ul><li><p>输入和输出数据在不同的例子中可以有不同的长度；</p></li><li><p>这种朴素的神经网络结果并不能共享从文本不同位置所学习到的特征。（如卷积神经网络中学到的特征的快速地推广到图片其他位置）</p></li></ul><h4 id="循环神经网络："><a href="#循环神经网络：" class="headerlink" title="循环神经网络："></a>循环神经网络：</h4><p>循环神经网络作为一种新型的网络结构，在处理序列数据问题上则不存在上面的两个缺点。在每一个时间步中，循环神经网络会传递一个激活值到下一个时间步中，用于下一时间步的计算。如下图所示：</p><p><img src="/2020/03/15/基于RNN的序列模型总结/1.jpg" alt=""></p><p>这里需要注意在零时刻，我们需要编造一个激活值，通常输入一个零向量，有的研究人员会使用随机的方法对该初始激活向量进行初始化。同时，上图中右边的循环神经网络的绘制结构与左边是等价的。</p><p>循环神经网络是从左到右扫描数据的，同时共享每个时间步的参数。</p><ul><li><p>$ W_{ax} $管理从输入$ x^{<t>} $到隐藏层的连接，每个时间步都使用相同的$ W_{ax} $ ，同下；</t></p></li><li><p>$ W_{aa} $ 管理激活值 $ a^{<t>} $ 到隐藏层的连接；</t></p></li><li><p>$ W_{ya} $ 管理隐藏层到激活值 $ y^{<t>} $ 的连接。</t></p></li></ul><p>上述循环神经网络结构的缺点：每个预测输出 $ y^{<t>} $ 仅使用了前面的输入信息，而没有使用后面的信息。Bidirectional RNN（双向循环神经网络）可以解决这种存在的缺点。</t></p><h4 id="循环神经网络的前向传播："><a href="#循环神经网络的前向传播：" class="headerlink" title="循环神经网络的前向传播："></a>循环神经网络的前向传播：</h4><p>下图是循环神经网络结构图：</p><p><img src="/2020/03/15/基于RNN的序列模型总结/2.jpg" alt=""></p><p><img src="/2020/03/15/基于RNN的序列模型总结/3.png" alt=""></p><p><img src="/2020/03/15/基于RNN的序列模型总结/4.jpg" alt=""></p><h3 id="穿越时间的反向传播"><a href="#穿越时间的反向传播" class="headerlink" title="穿越时间的反向传播"></a>穿越时间的反向传播</h3><p>为了进行反向传播计算，使用梯度下降等方法来更新RNN的参数，我们需要定义一个损失函数，如下：</p><p>$ L^{<t>}(\hat a) = -y^{<t>}\log{\hat y^{<t>}} - (1-y^{<t>})\log(1-\hat y^{<t>})\\L(\hat y,y) = \sum_{t=1}^{T^y}L^{<t>}(\hat y^{<t>},y^{<t>}) $</t></t></t></t></t></t></t></t></p><p>上式表示将每个输出的损失进行求和即为整体的损失函数。反向传播算法按照前向传播相反的方向进行导数计算，来对参数进行更新。其中比较特别的是在RNN中，从右向左的反向传播计算是通过时间来进行，像穿越时间的反向计算。</p><h3 id="不同类型的RNN"><a href="#不同类型的RNN" class="headerlink" title="不同类型的RNN"></a>不同类型的RNN</h3><p>对于RNN，不同的问题需要不同的输入输出结构。</p><ul><li>多对多（输入与输出相同）</li></ul><p>这种情况下的输入和输出的长度相同，是上面例子的结构，如下图所示：</p><p><img src="/2020/03/15/基于RNN的序列模型总结/5.jpg" alt=""></p><ul><li>多对一<br>如在情感分类问题中，我们要对某个序列进行正负判别或者打星操作。在这种情况下，就是输入是一个序列，但输出只有一个值：</li></ul><p><img src="/2020/03/15/基于RNN的序列模型总结/6.jpg" alt=""></p><ul><li>一对多</li></ul><p>如在音乐生成的例子中，输入一个音乐的类型或者空值，直接生成一段音乐序列或者音符序列。在这种情况下，就是输入是一个值，但输出是一个序列：</p><p><img src="/2020/03/15/基于RNN的序列模型总结/7.jpg" alt=""></p><ul><li>多对多 输入与输出不同</li></ul><p>我们上面介绍的一种RNN的结构是输入和输出序列的长度是相同的，但是像机器翻译这种类似的应用来说，输入和输出都是序列，但长度却不相同，这是另外一种多对多的结构：</p><p><img src="/2020/03/15/基于RNN的序列模型总结/8.jpg" alt=""></p><h3 id="语言模型和序列生成"><a href="#语言模型和序列生成" class="headerlink" title="语言模型和序列生成"></a>语言模型和序列生成</h3><p>对于下面的例子，两句话有相似的发音，但是想表达的意义和正确性却不相同，如何让我们的构建的语音识别系统能够输出正确地给出想要的输出。也就是对于语言模型来说，从输入的句子中，评估各个句子中各个单词出现的可能性，进而给出整个句子出现的可能性。</p><p><img src="/2020/03/15/基于RNN的序列模型总结/9.jpg" alt=""></p><p><strong>使用RNN构建语言模型：</strong></p><ul><li><p>训练集：一个很大的语言文本语料库</p></li><li><p>Tokenize：将句子使用字典库标记化</p></li><li><p>其中，未出现在字典库中的词使用“UNK”来表示</p></li><li><p>第一步：使用零向量对输出进行预测，即预测第一个单词是某个单词的可能性</p></li><li><p>第二步：通过前面的输入，逐步预测后面一个单词出现的概率</p></li><li><p>训练网络：使用softmax损失函数计算损失，对网络进行参数更新，提升语言模型的准确率。</p></li></ul><p><img src="/2020/03/15/基于RNN的序列模型总结/10.jpg" alt=""></p><h3 id="新序列采样"><a href="#新序列采样" class="headerlink" title="新序列采样"></a>新序列采样</h3><p>在完成一个序列模型的训练之后，如果我们想要了解这个模型学到了什么，其中一种非正式的方法就是进行一次新序列采样（sample novel sequences）。对于一个序列模型，其模拟了任意特定单词序列的概率，如 $ P(y^{<1>},…,y^{T_y}) $,而我们要做的就是对这个概率分布进行采样，来生成一个新的单词序列</1></p><p>如下面的一个已经训练好的RNN结构，我们为了进行采样需要做的：</p><p><img src="/2020/03/15/基于RNN的序列模型总结/11.png" alt=""></p><p>同样处理文本数据，在这个地方同样是一大难题</p><p>上面的模型是基于词汇的语言模型，我们还可以构建基于字符的语言模型，其中每个单词和符号则表示一个相应的输入或者输出：</p><p><img src="/2020/03/15/基于RNN的序列模型总结/12.jpg" alt=""></p><p>但是基于字符的语言模型，一个主要的缺点就是我们最后会得到太多太长的输出序列，其对于捕捉句子前后依赖关系，也就是句子前部分如何影响后面部分，不如基于词汇的语言模型那样效果好；同时基于字符的语言模型训练代价比较高。所以目前的趋势和常见的均是基于词汇的语言模型。但随着计算机运算能力的增强，在一些特定的情况下，也会开始使用基于字符的语言模型。</p><h3 id="RNN的梯度消失"><a href="#RNN的梯度消失" class="headerlink" title="RNN的梯度消失"></a>RNN的梯度消失</h3><p>RNN在NLP中具有很大的应用价值，但是其存在一个很大的缺陷，那就是梯度消失的问题。例如下面的例句中：</p><ul><li>The cat, which already ate ………..，was full；</li><li>The cats, which already ate ………..，were full.</li></ul><p>在这两个句子中，cat对应着was，cats对应着were，（中间存在很多很长省略的单词），句子中存在长期依赖（long-term dependencies），前面的单词对后面的单词有很重要的影响。但是我们目前所见到的基本的RNN模型，是不擅长捕获这种长期依赖关系的。</p><p>如下图所示，和基本的深度神经网络结构类似，输出y得到的梯度很难通过反向传播再传播回去，也就是很难对前面几层的权重产生影响，所以RNN也有同样的问题，也就是很难让网络记住前面的单词是单数或者复数，进而对后面的输出产生影响。</p><p><img src="/2020/03/15/基于RNN的序列模型总结/13.jpg" alt=""></p><p>对于梯度消失问题，在RNN的结构中是我们首要关心的问题，也更难解决；</p><p>虽然梯度爆炸在RNN中也会出现，但对于<strong>梯度爆炸问题</strong>，因为参数会指数级的梯度，会让我们的网络参数变得很大，得到很多的Nan或者数值溢出，所以梯度爆炸是很容易发现的，我们的解决方法就是用<strong>梯度修剪</strong>，也就是观察梯度向量，如果其大于某个阈值，则对其进行缩放，保证它不会太大。</p><h3 id="GRU单元"><a href="#GRU单元" class="headerlink" title="GRU单元"></a>GRU单元</h3><p>门控循环单元（Gated Recurrent Unit, GRU）改变了RNN的隐藏层，使其能够更好地捕捉深层次连接，并改善了梯度消失的问题。</p><h4 id="RNN单元"><a href="#RNN单元" class="headerlink" title="RNN单元"></a>RNN单元</h4><p>对于RNN的一个时间步的计算单元，在计算 $ a^{<t>} $ 也就是下图右边的公式，能以左图的形式可视化呈现：</t></p><p><img src="/2020/03/15/基于RNN的序列模型总结/14.jpg" alt=""></p><h4 id="简化的GRU单元"><a href="#简化的GRU单元" class="headerlink" title="简化的GRU单元"></a>简化的GRU单元</h4><p><img src="/2020/03/15/基于RNN的序列模型总结/15.png" alt=""></p><p>GRU的可视化实现如下图右边所示：</p><p><img src="/2020/03/15/基于RNN的序列模型总结/16.jpg" alt=""></p><h4 id="完整的GRU单元："><a href="#完整的GRU单元：" class="headerlink" title="完整的GRU单元："></a>完整的GRU单元：</h4><p>完整的GRU单元还存在另外一个门，以决定每个时间步的候选值，公式如下：</p><p><img src="/2020/03/15/基于RNN的序列模型总结/17.png" alt=""></p><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>GRU能够让我们在序列中学习到更深的联系，长短期记忆，对捕捉序列中更深层次的联系要比GRU更加有效。</p><p>LSTM中，使用了单独的更新门 $ \Gamma_u $ 和遗忘门 $ \Gamma_f $ ，以及一个输出门 $ \Gamma_o $ ，其主要的公式如下：</p><p><img src="/2020/03/15/基于RNN的序列模型总结/18.png" alt=""></p><p>LSTM单元的可视化图如下所示：</p><p><img src="/2020/03/15/基于RNN的序列模型总结/19.jpg" alt=""></p><p>其中，在实际使用时，几个门值不仅仅取决于 $ a^{<t-1>} $ 和 $ x^{<t>} $ ，还可能会取决于上一个记忆细胞的值 $ c^{<t-1>} $ ，这也叫做偷窥孔连接。</t-1></t></t-1></p><h3 id="双向RNN"><a href="#双向RNN" class="headerlink" title="双向RNN"></a>双向RNN</h3><p>双向RNN（bidirectional RNNs）模型能够让我们在序列的某处，不仅可以获取之间的信息，还可以获取未来的信息。</p><p>对于下图的单向RNN的例子中，无论我们的RNN单元是基本的RNN单元，还是GRU，或者LSTM单元，对于例子中第三个单词”Teddy”很难判断是否是人名，仅仅使用前面的两个单词是不够的，需要后面的信息来进行判断，但是单向RNN就无法实现获取未来的信息。</p><p><img src="/2020/03/15/基于RNN的序列模型总结/20.jpg" alt=""></p><p>而双向RNN则可以解决单向RNN存在的弊端。在BRNN中，不仅有从左向右的前向连接层，还存在一个从右向左的反向连接层。</p><p><img src="/2020/03/15/基于RNN的序列模型总结/21.jpg" alt=""></p><p>预测结果即有前向的信息，又有反向的信息。在NLP问题中，常用的就是使用双向RNN的LSTM。</p><p>不过后面就可能替换成更高级的模型了。</p><h3 id="深层RNNs"><a href="#深层RNNs" class="headerlink" title="深层RNNs"></a>深层RNNs</h3><p>与深层的基本神经网络结构相似，深层RNNs模型具有多层的循环结构，但不同的是，在传统的神经网络中，我们可能会拥有很多层，几十层上百层，但是对与RNN来说，三层的网络结构就已经很多了，因为RNN存在时间的维度，所以其结构已经足够的庞大。</p><p>在LSTM中的dim_layer可能指的就是这个</p><h2 id="自然语言处理和词语嵌入"><a href="#自然语言处理和词语嵌入" class="headerlink" title="自然语言处理和词语嵌入"></a>自然语言处理和词语嵌入</h2><h3 id="词汇表征"><a href="#词汇表征" class="headerlink" title="词汇表征"></a>词汇表征</h3><p>在前面学习的内容中，我们表征词汇是直接使用英文单词来进行表征的，但是对于计算机来说，是无法直接认识单词的。为了让计算机能够能更好地理解我们的语言，建立更好的语言模型，我们需要将词汇进行表征。下面是几种不同的词汇表征方式：</p><p><strong>one-hot表征</strong></p><p>在前面的一节课程中，已经使用过了one-hot表征的方式对模型字典中的单词进行表征，对应单词的位置用1表示，其余位置用0表示，如下图所示：</p><p><img src="/2020/03/15/基于RNN的序列模型总结/22.jpg" alt=""></p><p>one-hot表征的缺点：这种方法将每个词孤立起来，使得模型对相关词的泛化能力不强。每个词向量之间的距离都一样，乘积均为0，所以无法获取词与词之间的相似性和关联性。</p><p>特征表征：词嵌入</p><p>用不同的特征来对各个词汇进行表征，相对与不同的特征，不同的单词均有不同的值。如下例所示：</p><p><img src="/2020/03/15/基于RNN的序列模型总结/23.jpg" alt=""></p><p>这种表征方式使得词与词之间的相似性很容易地表征出来，这样对于不同的单词，模型的泛化性能会好很多。下面是使用t-SNE算法将高维的词向量映射到2维空间，进而对词向量进行可视化，很明显我们可以看出对于相似的词总是聚集在一块儿：</p><p><img src="/2020/03/15/基于RNN的序列模型总结/24.jpg" alt=""></p><h3 id="使用-Word-Embeddings"><a href="#使用-Word-Embeddings" class="headerlink" title="使用 Word Embeddings"></a>使用 Word Embeddings</h3><p>Word Embeddings对不同单词进行了实现了特征化的表示，那么如何将这种表示方法应用到自然语言处理的应用中呢？</p><h4 id="名字实体识别的例子："><a href="#名字实体识别的例子：" class="headerlink" title="名字实体识别的例子："></a>名字实体识别的例子：</h4><p>如下面的一个句子中名字实体的定位识别问题，假如我们有一个比较小的数据集，可能不包含durain（榴莲）和cultivator（培育家）这样的词汇，那么我们就很难从包含这两个词汇的句子中识别名字实体。但是如果我们从网上的其他地方获取了一个学习好的word Embedding，它将告诉我们榴莲是一种水果，并且培育家和农民相似，那么我们就有可能从我们少量的训练集中，归纳出没有见过的词汇中的名字实体。</p><p><img src="/2020/03/15/基于RNN的序列模型总结/25.jpg" alt=""></p><h4 id="词嵌入的迁移学习："><a href="#词嵌入的迁移学习：" class="headerlink" title="词嵌入的迁移学习："></a>词嵌入的迁移学习：</h4><p>有了词嵌入，就可以让我们能够使用迁移学习，通过网上大量的无标签的文本中学习到的知识，应用到我们少量文本训练集的任务中。下面是做词嵌入迁移学习的步骤：</p><ul><li><p>第一步：从大量的文本集合中学习word Embeddings（1-100B words），或者从网上下载预训练好的词嵌入模型；</p></li><li><p>第二步：将词嵌入模型迁移到我们小训练集的新任务上；</p></li><li><p>第三步：可选，使用我们新的标记数据对词嵌入模型继续进行微调。</p></li></ul><p><strong>词嵌入和人脸编码：</strong></p><p>词嵌入和人脸编码之间有很奇妙的联系。在人脸识别领域，我们会将人脸图片预编码成不同的编码向量，以表示不同的人脸，进而在识别的过程中使用编码来进行比对识别。词嵌入则和人脸编码有一定的相似性。</p><p><img src="/2020/03/15/基于RNN的序列模型总结/26.jpg" alt=""></p><p>但是不同的是，对于人脸识别，我们可以将任意一个没有见过的人脸照片输入到我们构建的网络中，则可输出一个对应的人脸编码。而在词嵌入模型中，所有词汇的编码是在一个固定的词汇表中进行学习单词的编码以及其之间的关系的。</p><h3 id="词嵌入的特性"><a href="#词嵌入的特性" class="headerlink" title="词嵌入的特性"></a>词嵌入的特性</h3><h4 id="类比推理特性："><a href="#类比推理特性：" class="headerlink" title="类比推理特性："></a>类比推理特性：</h4><p>词嵌入还有一个重要的特性，它还能够帮助实现类比推理。如下面的例子中，通过不同词向量之间的相减计算，可以发现不同词之间的类比关系，man——woman、king——queen，如下图所示：</p><p><img src="/2020/03/15/基于RNN的序列模型总结/27.jpg" alt=""></p><p>这种思想帮助研究者们对词嵌入建立了更加深刻的理解和认识。</p><p>计算词与词之间的相似度，实际上是在多维空间中，寻找词向量之间各个维度的距离相似度。</p><p>所以就可以根据所谓的相似度函数来看看两个词汇之间的关系</p><p><strong>相似度函数：</strong></p><p><img src="/2020/03/15/基于RNN的序列模型总结/28.png" alt=""></p><h3 id="嵌入矩阵"><a href="#嵌入矩阵" class="headerlink" title="嵌入矩阵"></a>嵌入矩阵</h3><p>在我们要对一个词汇表学习词嵌入模型时，实质上就是要学习这个词汇表对应的一个嵌入矩阵 E 。当我们学习好了这样一个嵌入矩阵后，通过嵌入矩阵与对应词的one-hot向量相乘，则可得到该词汇的embedding，如下图所示：</p><p><img src="/2020/03/15/基于RNN的序列模型总结/29.jpg" alt=""></p><h3 id="学习词嵌入"><a href="#学习词嵌入" class="headerlink" title="学习词嵌入"></a>学习词嵌入</h3><p>词嵌入的学习算法随着时间的进行逐渐变得越来越简单。</p><h4 id="早期的学习算法"><a href="#早期的学习算法" class="headerlink" title="早期的学习算法"></a>早期的学习算法</h4><p>如下面的例子中，我们要通过前面几个单词，预测最后一个单词：</p><ul><li><p>通过将每个单词的one-hot向量与嵌入矩阵相乘，得到相应的Embedding；</p></li><li><p>利用窗口控制影响预测结果的单词数量，并将窗口内单词的Embedding堆叠起来输入到神经网络中；</p></li><li><p>最后通过softmax层输出整个词汇表各个单词可能的概率；</p></li><li><p>其中，隐藏层和softmax层都有自己的参数，假设词汇表的大小为 100000 ，每个单词的Embedding大小是 300 ，历史窗口大小为 4 ，那么输入的大小即为 1200 ，softmax输出大小为词汇表大小 100000 ；</p></li><li><p>整个模型的参数就是嵌入矩阵 E ，以及隐藏层和softmax层的参数 w1,b1,w2,b2 ；</p></li><li><p>可以利用反向传播算法进行梯度下降，来最大化训练集似然函数，不断地从语料库中预测最后一个词的输出。</p></li></ul><p>在不断地训练过程中，算法会发现要想最好地拟合训练集，就要使得一些特性相似的词汇具有相似的特征向量，从而就得到了最后的词嵌入矩阵 $  $ 。</p><h4 id="其他的上下文和目标词对："><a href="#其他的上下文和目标词对：" class="headerlink" title="其他的上下文和目标词对："></a>其他的上下文和目标词对：</h4><p>我们将要预测的单词称为目标词，其是通过一些上下文推导预测出来的。对于不同的问题，上下文的大小和长度以及选择的方法有所不同。</p><ul><li>选取目标词之前的几个词；</li><li>选取目标词前后的几个词；</li><li>选取目标词前的一个词；</li><li>选取目标词附近的一个词，（一种Skip-Gram模型的思想）。</li></ul><h3 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h3><p>Word2Vec算法是一种简单的计算更加高效的方式来实现对词嵌入的学习。</p><h4 id="Skip-grams："><a href="#Skip-grams：" class="headerlink" title="Skip-grams："></a>Skip-grams：</h4><p>在Skip-grams模型中，我们需要抽取上下文（Content）和目标词（Target）配对，来构造一个监督学习问题。</p><p><strong>上下文不一定是要目标词前面或者后面离得最近的几个单词，而是随机选择一个词作为上下文，同时在上下文的一定距离范围内随机选择另外一个词作为目标词</strong></p><p>构造这样一个监督学习问题的目的，并不是想要解决监督学习问题本身，而是想要使用这个问题来学习一个好的词嵌入模型。</p><h4 id="模型流程"><a href="#模型流程" class="headerlink" title="模型流程"></a>模型流程</h4><ul><li><p>使用一个具有大量词汇的词汇表，如Vocab size = 10000k；</p></li><li><p>构建基本的监督学习问题，也就是构建上下文（C）和目标词（T）的映射关系：C——T；</p></li><li>$ o_c $（one-hot）——$ E $（词嵌入矩阵）——$ e_c = E *o_c $ （词嵌入）—— $ Softmax $ 层—— $ \hat y $ ；</li><li>$ Softmax $ ： $ p(t|c)=\frac{e^{\Theta^T_te_c}}{\sum_{j=1}^{10000}e^{\Theta_j^Te_c}} $，其中$ \Theta_t $ 是与输出 $ t $ 有关的参数；</li><li>损失函数： $ L(\hat y,y)= - \sum_{i=1}^{10000}y_i\log {\hat y_i} $ ，这是在目标词 $ y $ 表示为one-hot向量时，常用的softmax损失函数。</li><li>通过反向传播梯度下降的训练过程，可以得到模型的参数 $E$ 和softmax的参数。</li></ul><p>不过这个存在一些需要解决的问题</p><ul><li><p>计算速度慢，因为计算量特别庞大</p></li><li><p>需要简化方案，就像之前并查集那样，将不太需要使用的词语放在数的深处</p></li></ul><p><strong>那么该如何采样上下文呢</strong></p><ul><li><p>对语料库均匀且随机地采样：使得如the、of、a等这样的一些词会出现的相当频繁，导致上下文和目标词对经常出现这类词汇，但我们想要的目标词却很少出现。</p></li><li><p>采用不同的启发来平衡常见和不常见的词进行采样。这种方法是实际使用的方法。</p></li></ul><h3 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h3><p>Skip-grams模型可以帮助我们构造一个监督学习任务，将上下文映射到目标词上，从而让我们能够学习到一个实用的词嵌入模型。但是其缺点就是softmax计算的时间复杂度较高。下面介绍一种改善的学习问题：负采样。其能够做到的和Skip-grams模型相似，但其学习算法更加有效。</p><p>这样以来才产生新的学习问题：</p><ul><li><p>定义一个新的学习问题：预测两个词之间时候是上下文，目标词对，如果是词对，则学习的目标是1，否则就为0</p></li><li><p>使用k次相同的上下文，随机选择不同的目标词，并且对相应的词对进行正负样本的标记，生成新的训练集</p></li><li>建议：小数据集，k=5~20；大数据集，k=2~5。</li><li>最后学习x-y的映射关系</li></ul><p><img src="/2020/03/15/基于RNN的序列模型总结/30.jpg" alt=""></p><h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><p>负采样上采用逻辑回归的模型：</p><p>每个正样本均有k个对应的负样本。在训练的过程中，对于每个上下文词，我们就有对应的 k+1 个分类器。如下图所示：</p><p><img src="/2020/03/15/基于RNN的序列模型总结/31.jpg" alt=""></p><p>相比与Skip-grams模型，负采样不再使用一个具有词汇表大小时间复杂度高的庞大维度的Softmax，而是将其转换为词汇表大小个二分类问题。每个二分类任务都很容易解决，因为每个的训练样本均是1个正样本，外加k个负样本。</p><p>但是现在又产生一个新的问题，怎么并且分成这样的样本呢</p><ul><li><p>通过单词出现的频率进行采样：导致一些类似a、the、of等词的频率较高；</p></li><li><p>均匀随机地抽取负样本：没有很好的代表性</p></li></ul><p>其实有一种方法是着重推荐的，处于上面两种极端之间，公式为</p><p>$ P(w_i)=\frac{f(w_i)^{\frac{3}{4}}}{\sum_{j=1}^{10000}f(w_j)^{\frac{3}{4}}} $</p><p>即采用的是对词频的3/4除以词频3/4的整体的和进行采样，其中$ f(w_j) $是语料库中观察到的某个词的词频</p><h3 id="GloVe-词向量（这一部分内容没有搞懂，去补一下数学知识再回来探索）"><a href="#GloVe-词向量（这一部分内容没有搞懂，去补一下数学知识再回来探索）" class="headerlink" title="GloVe 词向量（这一部分内容没有搞懂，去补一下数学知识再回来探索）"></a>GloVe 词向量（这一部分内容没有搞懂，去补一下数学知识再回来探索）</h3><p>GloVe（global vectors for word representation）词向量模型是另外一种计算词嵌入的方法，虽然相比下没有Skip-grams模型用的多，但是相比这种模型却更加简单。</p><p><img src="/2020/03/15/基于RNN的序列模型总结/32.png" alt=""></p><p>通过上面的很多算法得到的词嵌入向量，我们无法保证词嵌入向量的每个独立分量是能够让我们理解的。我们能够确定是每个分量是和我们所想的一些特征是有关联的，其可能是一些我们能够理解的特征的组合而构成的一个组合分量。使用上面的GloVe模型，从线性代数的角度解释如下：</p><p>$ \Theta_i^Te_j=\Theta_i^TA^TA^{-T}e_j=(A\Theta_i)^T(A^{-T}e_j) $</p><p><img src="/2020/03/15/基于RNN的序列模型总结/36.jpg" alt=""></p><h3 id="情感分类"><a href="#情感分类" class="headerlink" title="情感分类"></a>情感分类</h3><p>情感分类就是通过一段文本来判断这个文本中的内容是否喜欢其所讨论的内容，这是NLP中最重要的模块之一。</p><p><img src="/2020/03/15/基于RNN的序列模型总结/33.jpg" alt=""></p><p>情感分类任务存在的一个问题就是只有很小的数据集，缺乏训练样本。但是在使用了词嵌入后，则能够带来很好的效果，足以训练一个良好的情感分类模型。</p><p>用RNN模型</p><p><img src="/2020/03/15/基于RNN的序列模型总结/34.jpg" alt=""></p><ul><li>获取一个训练好的词嵌入矩阵 E ；</li><li>得到每个词的词嵌入向量，输入到many-to-one的RNN模型中；</li><li>通过最后的softmax分类器，得到最后的输出 $ \hat y $ 。</li><li>优点：考虑了词序，效果好很多。</li></ul><h3 id="词嵌入消除偏"><a href="#词嵌入消除偏" class="headerlink" title="词嵌入消除偏"></a>词嵌入消除偏</h3><p>当下机器学习或者人工智能算法已经被应用到做一些非常重要的决策中，因此我们需要尽可能地保证其不受非预期形式的偏见的影响，如性别、种族歧视等等。下面介绍一些在词嵌入中消除偏见的办法。</p><p>个人觉得这对于现在的我来说不是问题，所以以后有机会再去研究这一方面。</p><h2 id="序列模型-—-序列模型和注意力机制"><a href="#序列模型-—-序列模型和注意力机制" class="headerlink" title="序列模型 —- 序列模型和注意力机制"></a>序列模型 —- 序列模型和注意力机制</h2><h3 id="基础模型"><a href="#基础模型" class="headerlink" title="基础模型"></a>基础模型</h3><h4 id="sequence-to-sequence-模型："><a href="#sequence-to-sequence-模型：" class="headerlink" title="sequence to sequence 模型："></a>sequence to sequence 模型：</h4><p>sequence to sequence 模型最为常见的就是机器翻译，假如这里我们要将法语翻译成英文：</p><p>这里可以直接去应用多对多的RNN网络模型 输入Tx个 输出Ty个</p><p>对于机器翻译的序列对序列模型，如果我们拥有大量的句子语料，则可以得到一个很有效的机器翻译模型。模型的前部分使用一个编码网络来对输入的法语句子进行编码，后半部分则使用一个解码网络来生成对应的英文翻译。</p><p><img src="/2020/03/15/基于RNN的序列模型总结/37.jpg" alt=""></p><p><strong>这里还是今后去看相关论文吧</strong></p><ul><li>Sutskever et al., Sequence to sequence learning with neural networks, 2014;</li><li>Cho et al., Learning phrase representation using RNN encoder-decoder for statistical machine translation, 2014;</li></ul><h3 id="image-to-sequence-模型："><a href="#image-to-sequence-模型：" class="headerlink" title="image to sequence 模型："></a>image to sequence 模型：</h3><p>与上面的这种编解码类似的还有就是图像描述的应用。</p><p>输入：图像<br>输出：描述图像的句子<br>网络结构：CNN结构学习图像的编码，RNN结构解码输出对应的句子。</p><p>对于图像描述的网络结构如下图所示：</p><p><img src="/2020/03/15/基于RNN的序列模型总结/38.jpg" alt=""></p><p>后面的有一个项目会涉及到这个，还是贴出论文，有时间去看看咯</p><ul><li>Mao et. al., 2014. Deep captioning with multimodal recurrent neural networks；</li><li>Vinyals et. al., 2014. Show and tell: Neural image caption generator；</li><li>Karpathy and Li, 2015. Deep visual-semantic alignments for generating image descriptions；</li></ul><h3 id="挑选最可能的句子"><a href="#挑选最可能的句子" class="headerlink" title="挑选最可能的句子"></a>挑选最可能的句子</h3><p>一般用在搜索词条弹出，或者是机器翻译中的条件语言模型</p><p>对于机器翻译来说和之前几节介绍的语言模型有很大的相似性但也有不同之处。</p><ul><li><p>在语言模型中，我们通过估计句子的可能性，来生成新的句子。语言模型总是以零向量开始，也就是其第一个时间步的输入可以直接为零向量；</p></li><li><p>在机器翻译中，包含了编码网络和解码网络，其中解码网络的结构与语言模型的结构是相似的。机器翻译以句子中每个单词的一系列向量作为输入，所以相比语言模型来说，机器翻译可以称作条件语言模型，其输出的句子概率是相对于输入的条件概率。</p></li></ul><p><img src="/2020/03/15/基于RNN的序列模型总结/39.jpg" alt=""></p><p>对于各种可能的翻译结果，我们并不是要从得到的分布中进行随机取样，而是要找到一个使得条件概率最大化的英文句子作为输出。所以在设计机器翻译模型的时候，一个重要的步骤就是设计一个合适的算法，找到使得条件概率最大化的的结果。目前最通用的算法就是：束搜索（Beam Search）。</p><p>个人觉得 Beam Search 贼神奇，为什么会又这样的结果呢，找时间去研究</p><p>对了，这里还要提一句为什么不用贪心搜索，打过acm的人应该都知道，<strong>局部最优解，并不等于全局最优解</strong></p><h3 id="Beam-search"><a href="#Beam-search" class="headerlink" title="Beam search"></a>Beam search</h3><p>个人的理解 就是类似于动态规划，依次保留多种可能性，然后用dfs往下面深搜，同时在多种可能性中去寻找可能性最大的那一种</p><p>官方给出的步骤如下：</p><p>Step 1：对于我们的词汇表，我们将法语句子输入到编码网络中得到句子的编码，通过一个softmax层计算各个单词（词汇表中的所有单词）输出的概率值，通过设置集束宽度（beam width）的大小如3，我们则取前3个最大输出概率的单词，并保存起来。</p><p><img src="/2020/03/15/基于RNN的序列模型总结/40.jpg" alt=""></p><p>Step 2：在第一步中得到的集束宽度的单词数，我们分别对第一步得到的每一个单词计算其与单词表中的所有单词组成词对的概率。并与第一步的概率相乘，得到第一和第二两个词对的概率。有 3 * 10000 个选择，（这里假设词汇表有10000个单词），最后再通过beam width大小选择前3个概率最大的输出对；</p><p><img src="/2020/03/15/基于RNN的序列模型总结/41.jpg" alt=""></p><p>Step 3~Step T：与Step2的过程是相似的，直到遇到句尾符号结束。</p><p><img src="/2020/03/15/基于RNN的序列模型总结/42.jpg" alt=""></p><h3 id="集束搜索的改进"><a href="#集束搜索的改进" class="headerlink" title="集束搜索的改进"></a>集束搜索的改进</h3><p><img src="/2020/03/15/基于RNN的序列模型总结/43.png" alt=""></p><p>Beam width：B的选择，B越大考虑的情况越多，但是所需要进行的计算量也就相应的越大。在常见的产品系统中，一般设置B = 10，而更大的值（如100，1000，…）则需要对应用的领域和场景进行选择。</p><p>相比于算法范畴中的搜索算法像BFS或者DFS这些精确的搜索算法，Beam Search 算法运行的速度很快，但是不能保证找到目标准确的最大值。</p><h3 id="集束搜索的误差分析"><a href="#集束搜索的误差分析" class="headerlink" title="集束搜索的误差分析"></a>集束搜索的误差分析</h3><p>集束搜索算法是一种近似搜索算法，也被称为启发式搜索算法。它的输出不能保证总是可能性最大的句子，因为其每一步中仅记录着Beam width为3或者10或者100种的可能的句子。</p><p>所以，如果我们的集束搜素算法出现错误了要怎么办呢？我们如何确定是算法出现了错误还是模型出现了错误呢？此时集束搜索算法的误差分析就显示出了作用。</p><h3 id="Bleu-Score-用来评估翻译结果（不懂就问？？？）"><a href="#Bleu-Score-用来评估翻译结果（不懂就问？？？）" class="headerlink" title="Bleu Score 用来评估翻译结果（不懂就问？？？）"></a>Bleu Score 用来评估翻译结果（不懂就问？？？）</h3><p>对于机器翻译系统来说，一种语言对于另外一种语言的翻译常常有多种正确且合适的翻译，我们无法做到像图像识别一样有固定准确度答案，所以针对不同的翻译结果，往往很难评估那一个结果是更好的，哪一个翻译系统是更加有效的。这里引入Bleu score 用来评估翻译系统的准确性。（Bleu, bilingual evaluation understudy）</p><p>如下面的法语翻译的例子，我们有两种不同的翻译，但是两种翻译都是正确且较好的翻译结果。Bleu score 的评估理念是观察机器生成的翻译结果中的每一个词是否出现在至少一个人工翻译结果的参考之中。（这些参考位于开发集或者测试集中）。</p><p><img src="/2020/03/15/基于RNN的序列模型总结/44.jpg" alt=""></p><ul><li>精确度：观察输出结果的每一个词是否出现在参考中。但是对于图中机器翻译的糟糕的结果，精确度确非常高。</li><li>改良的精确度：将每个单词设置一个得分上限（单个参考句子中出现的最大的次数，如图中的the单词的上限为2）。</li></ul><h4 id="二元词组的Bleu-score："><a href="#二元词组的Bleu-score：" class="headerlink" title="二元词组的Bleu score："></a>二元词组的Bleu score：</h4><p>与单个词的评估相似，这里我们以两个相邻的单词作为一个二元词组来进行Bleu得分评估，得到机器翻译的二元词组的得分和其相应的得分上限，进而得到改进的精确度。</p><p><img src="/2020/03/15/基于RNN的序列模型总结/45.jpg" alt=""></p><h3 id="注意力模型"><a href="#注意力模型" class="headerlink" title="注意力模型"></a>注意力模型</h3><p>这个也是近期比较重要的一种模型了</p><p><strong>长句子存在的问题：</strong></p><p>利用我们前面的编码和解码的RNN模型，我们能够实现较为准确度机器翻译结果。对于短句子来说，其性能是十分良好的，但是如果是很长的句子，翻译的结果就会变差。</p><p>对于我们人类进行人工翻译的时候，我们所做的也不是像编码解码RNN模型一样记忆整个输入句子，再进行相应的输出，因为记忆整个长句子是很难的，所以我们是一部分一部分地进行翻译。编码解码RNN模型的结构，其Bleu score会在句子长度超过一定值的时候就会下降，如图中的蓝色线所示。而引入的注意力机制，和人类的翻译过程非常相似，其也是一部分一部分地进行长句子的翻译，而其得到的翻译结果的Bleu曲线则如图中绿色线所示</p><p><img src="/2020/03/15/基于RNN的序列模型总结/46.jpg" alt=""></p><p>对于法语翻译英语的例子中，针对每个单词的输出，一般来说与某个输出相关的或者有较大影响的单词应该是集中在附近的几个单词或者说是某些部分。所以注意力模型会在输入的每个输入的信息块上计算注意力权重，不同的权重对每一步的输出结果有不同的注意力影响。</p><p><img src="/2020/03/15/基于RNN的序列模型总结/47.jpg" alt=""></p><p><img src="/2020/03/15/基于RNN的序列模型总结/48.png" alt=""></p><p><img src="/2020/03/15/基于RNN的序列模型总结/49.png" alt=""></p><p>注意力的应用</p><ul><li><p>将不标准的时间格式转换为统一的时间格式</p></li><li><p>对注意力权重进行可视化</p></li></ul><p><img src="/2020/03/15/基于RNN的序列模型总结/50.jpg" alt=""></p><p><strong>后面还有语音识别，因为语音识别这一块也算一个大的方向，我想整理出一个专门的章节，所以这个地方不去总结了。</strong></p><p>以上最最最最最重要的就是注意力模型了</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
      <category term="机器学习" scheme="http://wsx1128.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://wsx1128.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="RNN" scheme="http://wsx1128.cn/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>风格迁移---基于pytorch框架的图片生成模型</title>
    <link href="http://wsx1128.cn/2020/03/14/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB-%E5%9F%BA%E4%BA%8Epytorch%E6%A1%86%E6%9E%B6%E7%9A%84%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
    <id>http://wsx1128.cn/2020/03/14/风格迁移-基于pytorch框架的图片生成模型/</id>
    <published>2020-03-14T12:55:08.000Z</published>
    <updated>2020-03-14T17:49:08.383Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>所谓风格迁移，实际上就是利用算法学习著名画作的风格，然后把这种风格应用到另外一张图片上的技术。著名的国像处理应用Prisma是利用风格迁移技术，将普通的照片自动转换成具有艺术家风格的图片。</p><p>首先 我们需要弄清楚的是关于图像的风格迁移，有两种形式（算法）：</p><p>一种是通过从0开始模仿风格图片，类似于GAN的生成方式，生成，这样每一张照片都需要重新去训练生成，比较麻烦，且花费的时间会非常长，优点是生成的效果会更好。</p><p>但是另一种则是将风格图片与特征图片进行一个建模，一个同类型的风格图片便可以快速迁移。这里则主要是讲究第二种方法。</p><h2 id="图像风格迁移的原理："><a href="#图像风格迁移的原理：" class="headerlink" title="图像风格迁移的原理："></a>图像风格迁移的原理：</h2><h3 id="原始风格迁移原理："><a href="#原始风格迁移原理：" class="headerlink" title="原始风格迁移原理："></a>原始风格迁移原理：</h3><p>首先先来将以下VGGNet结构，这个模型的结构：前面是卷积层从图像中提取特征，而后面的全链接层把图片的“特征”转换成类别概率，其中VGGNet中的浅层（如conv1_1，conv1_2），提取的特征往往是比较简单的（如检测点、线、亮度），VGGNet中的深层（如conv5_1、conv5_2），提取的特征往往是比较复杂（如有无人脸或某种特定物体）</p><p>VGGNet的本意是输入图像，提取特征，并输出图像类别。图像风格迁移正好与其相反，输入特征，输出对应这种特征的图片，如下图所示：</p><p><img src="/2020/03/14/风格迁移-基于pytorch框架的图片生成模型/1.png" alt=""></p><p>具体来说，风格迁移使用卷积层的中间特征还原出对应这种特征的原始图像。先选取衣服原始图像，经过VGGNet计算后得到各种卷积层特征。接下来，根据这些卷积层的特征，还原出对应这种特征的原始图像。图像b、c 、d 、e 、f 分别为使用卷积层conv1_2、conv2_2 、conv3 _2 、conv4_2 、conv5_2 的还原图像。可以发现：浅层的还原效果往往比较好，卷积特征基本保留了所高原始图像中形状、位置、颜色、纹理等信息； 深层对应的还原图像丢失了部分颜色和纹理信息，但大体保留原始图像中物体的形状和位置。</p><p>还原图像的方法是梯度下降法。设原始图像为p，期望还原的图像为x，使用卷积是第l层，原始图像p在第l层的特征为$p^l_{ij}$, i表示卷积的第i个通道，j表示卷积的第j个位置。通常卷积的第i个通道，j表示卷积的第j个位置。通常卷积的特征是三维的，三维的坐标对应（通道，高，宽）此处不考虑具体的高和宽，只考虑位置j，相当于把卷积“压扁”了。比如一个10x10x32的卷积特征，对应1⩽i⩽32,1⩽j⩽100。 对于生成图像$\vec x$, 同样定义它在$l$层的卷积特征为$F^l_{ij}$。</p><p>有了上面这些符号后，可以写出“内容损失”（Content Loss）。 内容损失$L_{content}(\vec p, \vec x,l) = \frac{1}{2} \sum_{ij}(F_{ij}^l - P_{ij}^l)^2$ </p><p>$L_{content}(\vec p, \vec x,l)$ 描述了原始图像$\vec p$ 和生成图像$\vec x$在内容上面的差异。内容损失越小，说明他们的内容越接近；内容损失越大，说明他们的内容差距也越大。先使用原始图像$\vec p$计算出它的卷积特征 $P_{ij}^l $同时随机初始化$\vec x$。接着以内容损失$L_{content}(\vec p, \vec x,l)$ 为优化目标，通过梯度下降法逐步改变$\vec x$, 进过一定的步数后，得到的$\vec x$是希望的还原图像了。在这个过程中，内容损失$L_{content}(\vec p, \vec x,l)$应该是越来越小了。</p><p>除了还原图像原本的“内容”之外，另一方面，还希望还原图像的“风格”。那么图像的“风格”应该用什么来表示呢？一种方法是使用图像的卷积层特征的Gram矩阵。</p><p>Gram矩阵是关于一组向量的内积的对称矩阵，例如，向量组$\vec x_1, \vec x_2, …, \vec x_n$的Gram矩阵是：</p><p><img src="/2020/03/14/风格迁移-基于pytorch框架的图片生成模型/2.png" alt=""></p><p>通常取内积为欧几里得空间上的标准内积， 即$(\vec x_i, \vec x_j) = \vec x_i^T \vec x_j$</p><p>设卷积层的输出为$F_{ij}^l $，那么这个卷积特征对应的Gram矩阵的第i行第j个元素定义为 $G_{ij}^l = \sum_k F_{ik}^lF^l_{jk} $ 设在第i层中，卷积特征的通道数为$N_l$，卷积的高、宽成绩数为$M_l$ 那么$F^l_{ij} $ 满足 $1 \leq i \leq N_l , 1 \leq i \leq M_l $。G实际上向量组$F_1^l,F_2^l,…,F_i^l,…F_{N_i}^l的Gran$矩阵，其中$F_i^l = (F^l{i1},F_{i2}^l, …,F^l_{ij},…,F^l_{iM_l})$</p><p>此处数学符号特较多，因此再举一个例子来加深读者对此Gram矩阵的理解。假设某一层输出的卷积特征为10x10x32，即它是一个宽、高均为10，通道数为32的张量。$F_{l}^1$表示第一个通道的特征，它是一个100维的向量。$F_l^2$表示第二个通道的特征，他同样是一个100维的向量，它对应的Gram矩阵G是</p><p><img src="/2020/03/14/风格迁移-基于pytorch框架的图片生成模型/3.png" alt=""></p><p>Gram矩阵可以在一定程度上反映原始图像的“风格”。仿照“内容损失”，还可以定义一个“风格损失”（Style Loss）。设原始图像为 $ \vec a $ ，要还原的的风格图像是$ \vec x $ ,先计算出原始图像某一层的卷积的Gram矩阵为 $ A^l $ ， 要还原的图像经过$ \vec x $ 经过同样的计算的到对应卷积层的Gram矩阵是$ G^l $ ，所以这里的风格损失就定义为</p><p>$ L_{style}(\vec p, \vec x, l)= \frac{1}{4N^2_lM^2_l}\sum_{ij}(A^l_{ij}-G^l_{ij})^2 $</p><p>根据上述的这个公式就得出来了这些东西，另外总结一下，目前为止介绍了两个内容</p><ul><li><p>利用内容损失还原图像内容</p></li><li><p>利用风格损失还原图像风格</p></li></ul><p>那么，可不可以将内容损失和风格损失组合起来，在还原一张图像的同事还原里一张图像的风格呢？答案是肯定的，这是图像风格迁移的基本算法。</p><p>设原始的内容图像为$ \vec p $， 原始的风格图像为$ \vec a $， 待生成的图像为$ \vec x $。 希望$ \vec x $ 可以爆出保持内容图像$ \vec p $的内容，同时具有风格图像的风格，所以定义下来总的损失函数为：</p><p>$ L_{total}(\vec p, \vec a, \vec x) = \alpha L_{content}(\vec p, \vec x) + \beta L_{style}(\vec a, \vec x) \\<br>其中 \alpha 和 \beta 都是平衡损失中的超参数，\\如果 \alpha偏大，那么还原的图像会更接近\vec a 的风格,另外一点其次。\\<br>所以这个使用两个超参数进行平反而是一件好事情。<br>$</p><h3 id="快速图像风格迁移的原理"><a href="#快速图像风格迁移的原理" class="headerlink" title="快速图像风格迁移的原理"></a>快速图像风格迁移的原理</h3><p>本篇的代码讲的就是快速风格迁移的原理</p><p>原始图像会用一个损失函数来衡量其是否成功的组合了风格图像和内容图像，依次梯度下降达到最小，从而在这种优化下速度导致了很慢。</p><p>而快速图像风格迁移的办法是：不使用优化的办法来足部迭代生成$ \vec x $，而是使用一个神经网络直接生成 。对应的网络结构就像下面这张图</p><p><strong>这里忍不住吐槽一句 花书的作者牛逼！！！！！ 我的偶像！！！！</strong></p><p><img src="/2020/03/14/风格迁移-基于pytorch框架的图片生成模型/4.png" alt=""></p><p>整个系统由两个神经网络组成，它们在图中由连个虚线框分别标出。左边的是图像生成网络，右边是损失网络。损失网络实际上是VGGNet，这与原始的风格迁移是一致的。同1.1节一样，利用损失网络来定义内容损失、风格损失。这个损失用来训练图像生成网络。图像生成网络的职责是生成某一种风格的图像，它的输入是一个图像，输出同样是一个图像。由于生成图像只需要在生成网络中计算一遍，所以速度比原始图像风格迁移快很多。</p><p>接下来同样用数据证明相应的合理性，下面这一段全部摘自论文和大佬们对论文的理解</p><p>$同样使用数学符号严格地阐述上面的过程：设输入的图像为\vec x,经过图像生成网络为\vec y \\ 而\vec y 在内容上应该与原始的内容图像 \vec y_c接近。因此可以利用损失网络可以直接用上面内容损失函数来定义，内容损失使用的是VGG-16的relu3_3层输出的特征，对应图中的 l_{feat}^{\Phi,relu3_3}。另一方面，我们还希望\vec y 具有目标风格的，因此在这个基础上又再次定义一个风格损失函数。定义风格损失时用了VGG-16的四个relu1_2，relu2_2，relu3_3，relu4_3，对应图中的l_{feat}^{\Phi,relu1_2}，l_{feat}^{\Phi,relu2_2}，l_{feat}^{\Phi,relu3_3}，l_{feat}^{\Phi,relu4_3}<br>$</p><p>利用总损失可以训练图像生成网络。训练完成后直接使用图像生成网络生成图像。值得一提的是，在整个训练过程中，一般只是固定使用一种风格。<br>所以经常应用快速迁移的多个模版。</p><h2 id="Pytorch-实现快速风格迁移"><a href="#Pytorch-实现快速风格迁移" class="headerlink" title="Pytorch 实现快速风格迁移"></a>Pytorch 实现快速风格迁移</h2><p>由于这次的模型定义的比较复杂，所以还是由工具包的定义开始。</p><h3 id="工具包"><a href="#工具包" class="headerlink" title="工具包"></a>工具包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torchvision <span class="keyword">as</span> tv</span><br><span class="line"><span class="keyword">import</span> visdom</span><br><span class="line"></span><br><span class="line"><span class="comment">## 下面是业界经过数次实践得出的 对于图片最好的归一化参数</span></span><br><span class="line">IMAGENET_MEAN = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">IMAGENET_STD = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个地方仍需要花时间去弄懂，后面有时间去读一下相关论文</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram_matrix</span><span class="params">(y)</span>:</span></span><br><span class="line">    (b, ch, h, w) = y.size()</span><br><span class="line">    features = y.view(b, ch, w * h)</span><br><span class="line">    features_t = features.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    gram = features.bmm(features_t) / (ch * h * w)</span><br><span class="line">    <span class="keyword">return</span> gram</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_style_data</span><span class="params">(path)</span>:</span></span><br><span class="line">    style_transform = tv.transforms.Compose([</span><br><span class="line">        tv.transforms.ToTensor(),</span><br><span class="line">        tv.transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),</span><br><span class="line">    ])</span><br><span class="line">    style_image = tv.datasets.folder.default_loader(path)</span><br><span class="line">    style_tensor = style_transform(style_image)</span><br><span class="line">    <span class="keyword">return</span> style_tensor.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 还没有弄清楚这个函数到底是用来干什么的</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize_batch</span><span class="params">(batch)</span>:</span></span><br><span class="line">    mean = batch.data.new(IMAGENET_MEAN).view(<span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    std = batch.data.new(IMAGENET_STD).view(<span class="number">1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    mean = (mean.expand_as(batch.data))</span><br><span class="line">    std = (std.expand_as(batch.data))</span><br><span class="line">    <span class="keyword">return</span> (batch / <span class="number">255.0</span> - mean) / std</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Visualizer</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, env=<span class="string">'default'</span>, **kwargs)</span>:</span></span><br><span class="line">        self.vis = visdom.Visdom(env=env, use_incoming_socket=<span class="keyword">False</span>, **kwargs)</span><br><span class="line">        self.index = &#123;&#125;</span><br><span class="line">        self.log_text = <span class="string">''</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reinit</span><span class="params">(self, env=<span class="string">'default'</span>, **kwargs)</span>:</span></span><br><span class="line">        self.vis = visdom.Visdom(env=env, use_incoming_socket= <span class="keyword">False</span>, **kwargs)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot_many</span><span class="params">(self, d)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> d.items():</span><br><span class="line">            self.plot(k, v)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">img_many</span><span class="params">(self, d)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> d.items():</span><br><span class="line">            self.img(k, v)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">plot</span><span class="params">(self, name, y)</span>:</span></span><br><span class="line">        x = self.index.get(name, <span class="number">0</span>)</span><br><span class="line">        self.vis.line(Y=np.array([y]), X=np.array([x]),</span><br><span class="line">                      win=name,</span><br><span class="line">                      opts=dict(title=name),</span><br><span class="line">                      update=<span class="keyword">None</span> <span class="keyword">if</span> x == <span class="number">0</span> <span class="keyword">else</span> <span class="string">'append'</span></span><br><span class="line">                      )</span><br><span class="line">        self.index[name] = x + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">img</span><span class="params">(self, name, img_)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> len(img_.size()) &lt; <span class="number">3</span>:</span><br><span class="line">            img_ = img_.cpu().unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.vis.image(img_.cpu(),</span><br><span class="line">                       win=name,</span><br><span class="line">                       opts=dict(title=name)</span><br><span class="line">                       )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">img_grid_many</span><span class="params">(self, d)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> d.items():</span><br><span class="line">            self.img_grid(k, v)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">img_grid</span><span class="params">(self, name, input_3d)</span>:</span></span><br><span class="line">        self.img(name, tv.utils.make_grid(</span><br><span class="line">            input_3d.cpu()[<span class="number">0</span>].unsqueeze(<span class="number">1</span>).clamp(max=<span class="number">1</span>, min=<span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">log</span><span class="params">(self, info, win=<span class="string">'log_text'</span>)</span>:</span></span><br><span class="line">        self.log_text += (<span class="string">'[&#123;time&#125;] &#123;info&#125; &lt;br&gt;'</span>.format(</span><br><span class="line">            time=time.strftime(<span class="string">'%m%d_%H%M%S'</span>),</span><br><span class="line">            info=info))</span><br><span class="line">        self.vis.text(self.log_text, win=win)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getattr__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> getattr(self.vis, name)</span><br></pre></td></tr></table></figure><p>上面的这个工具包里面包含了一个visdom的封装类，以及两个函数<br>需要注意三点</p><ul><li><p>业界统一规范的 使用的归一化参数，固定下来了，没有什么特殊的需求，就不要去轻易的更改</p></li><li><p>计算风格参数的矩阵，中 两个操作 如果是多维度矩阵需要转制，那么就去使用transpose， 如果是多维度矩阵相乘，那么就使用 bmm来保证第一个batch_size不变化</p></li><li><p>normalize_batch函数没有弄的太清楚，计算风格系数的矩阵需要去看一下论文。</p></li></ul><p>接下来就来敲定复杂的模型了，使用了一部分的预训练模型。</p><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><h4 id="VGG-16"><a href="#VGG-16" class="headerlink" title="VGG-16"></a>VGG-16</h4><p>首先 风格迁移的模型有两个，前面是生成图像的模型，后面则是提取特征，并且计算损失的模型，这里先从后面计算损失的模型VGG-16来看</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> vgg16</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vgg16</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Vgg16,self).__init__()</span><br><span class="line">        feature = list(vgg16(pretrained=<span class="keyword">True</span>).features)[:<span class="number">23</span>]</span><br><span class="line">        self.features = nn.ModuleList(feature).eval()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        result = []</span><br><span class="line">        <span class="keyword">for</span> ii, model <span class="keyword">in</span> enumerate(self.features):</span><br><span class="line">            x = model(x)</span><br><span class="line">            <span class="keyword">if</span> ii <span class="keyword">in</span> [<span class="number">3</span>, <span class="number">8</span>, <span class="number">15</span>, <span class="number">22</span>]:</span><br><span class="line">                result.append(x)</span><br><span class="line">        Resultset = namedtuple(<span class="string">"VggOutputs"</span>,[<span class="string">'relu1_2'</span>, <span class="string">'relu2_2'</span>, <span class="string">'relu3_3'</span>, <span class="string">'relu4_3'</span>])</span><br><span class="line">        <span class="keyword">return</span> Resultset(*result)</span><br></pre></td></tr></table></figure><p>这里需要注意的是三个地方，忍不住吐槽一句，python真的是博大精深，总是有一些稀奇古怪的用法让你欲罢不能。</p><ul><li><p>由于这里计算损失仅仅只是用到四个特征量，所以就没必要全部计算，直接提取前23层就可以了，而且基本上只要3，8，15，22层，直接在前向传播的函数中去提取即可</p></li><li><p>另外如果真的要提取前面的层数的花，直接用nn.Seqential()是不行的，所以需要用list拿出相应的层数，并且最后转换成ModuleList放到模型中去。</p></li><li><p>namedtuple 我的理解就是直接封装成一个有名字的元祖，后面之所以要将result变成*加进去的原因就是将result这个list变成可变参数移到里面去。</p></li></ul><p>接下来便是第二个模型</p><h4 id="TransformerNet"><a href="#TransformerNet" class="headerlink" title="TransformerNet"></a>TransformerNet</h4><p>这个模型可是相当的复杂咯，我这里贴一张图，诠释一切：</p><p><img src="/2020/03/14/风格迁移-基于pytorch框架的图片生成模型/5.png" alt=""></p><p>分为三层，具体为什么要著有构造，那就是 大佬花了很多时间一点一点去研究得来的，</p><p><strong>花书大佬牛逼</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConvLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, out_channels, kernel_size, stride)</span>:</span></span><br><span class="line">        super(ConvLayer, self).__init__()</span><br><span class="line">        reflection_padding = int(np.floor(kernel_size / <span class="number">2</span>))</span><br><span class="line">        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)</span><br><span class="line">        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.reflection_pad(x)</span><br><span class="line">        out = self.conv2d(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResidualBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, channels)</span>:</span></span><br><span class="line">        super(ResidualBlock, self).__init__()</span><br><span class="line">        self.conv1 = ConvLayer(channels, channels, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.in1 = nn.InstanceNorm2d(channels, affine=<span class="keyword">True</span>,track_running_stats=<span class="keyword">True</span>)</span><br><span class="line">        self.conv2 = ConvLayer(channels, channels, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.in2 = nn.InstanceNorm2d(channels, affine=<span class="keyword">True</span>,track_running_stats=<span class="keyword">True</span>)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        residual = x</span><br><span class="line">        out = self.relu(self.in1(self.conv1(x)))</span><br><span class="line">        out = self.in2(self.conv2(out))</span><br><span class="line">        out = out + residual</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UpsampleConvLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, out_channels, kernel_size, stride, upsample=None)</span>:</span></span><br><span class="line">        super(UpsampleConvLayer, self).__init__()</span><br><span class="line">        self.upsample = upsample</span><br><span class="line">        reflection_padding = int(np.floor(kernel_size / <span class="number">2</span>))</span><br><span class="line">        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)</span><br><span class="line">        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x_in = x</span><br><span class="line">        <span class="keyword">if</span> self.upsample:</span><br><span class="line">            x_in = t.nn.functional.interpolate(x_in, scale_factor=self.upsample)</span><br><span class="line">        out = self.reflection_pad(x_in)</span><br><span class="line">        out = self.conv2d(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(TransformerNet, self).__init__()</span><br><span class="line">        self.initial_layers = nn.Sequential(</span><br><span class="line">            ConvLayer(<span class="number">3</span>, <span class="number">32</span>, kernel_size=<span class="number">9</span>, stride=<span class="number">1</span>),</span><br><span class="line">            nn.InstanceNorm2d(<span class="number">32</span>, affine=<span class="keyword">True</span>,track_running_stats=<span class="keyword">True</span>),</span><br><span class="line">            nn.ReLU(<span class="keyword">True</span>),</span><br><span class="line"></span><br><span class="line">            ConvLayer(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.InstanceNorm2d(<span class="number">64</span>, affine=<span class="keyword">True</span>,track_running_stats=<span class="keyword">True</span>),</span><br><span class="line">            nn.ReLU(<span class="keyword">True</span>),</span><br><span class="line"></span><br><span class="line">            ConvLayer(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.InstanceNorm2d(<span class="number">128</span>, affine=<span class="keyword">True</span>,track_running_stats=<span class="keyword">True</span>),</span><br><span class="line">            nn.ReLU(<span class="keyword">True</span>)</span><br><span class="line">        )</span><br><span class="line">        self.res_layers = nn.Sequential(</span><br><span class="line">            ResidualBlock(<span class="number">128</span>),</span><br><span class="line">            ResidualBlock(<span class="number">128</span>),</span><br><span class="line">            ResidualBlock(<span class="number">128</span>),</span><br><span class="line">            ResidualBlock(<span class="number">128</span>),</span><br><span class="line">            ResidualBlock(<span class="number">128</span>)</span><br><span class="line">        )</span><br><span class="line">        self.upsample_layers = nn.Sequential(</span><br><span class="line">            UpsampleConvLayer(<span class="number">128</span>, <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.InstanceNorm2d(<span class="number">64</span>, affine=<span class="keyword">True</span>,track_running_stats=<span class="keyword">True</span>),</span><br><span class="line">            nn.ReLU(<span class="keyword">True</span>),</span><br><span class="line"></span><br><span class="line">            UpsampleConvLayer(<span class="number">64</span>, <span class="number">32</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.InstanceNorm2d(<span class="number">32</span>, affine=<span class="keyword">True</span>,track_running_stats=<span class="keyword">True</span>),</span><br><span class="line">            nn.ReLU(<span class="keyword">True</span>),</span><br><span class="line">            ConvLayer(<span class="number">32</span>, <span class="number">3</span>, <span class="number">9</span>, <span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.initial_layers(x)</span><br><span class="line">        x = self.res_layers(x)</span><br><span class="line">        x = self.upsample_layers(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>上面模型建立的图片，我还是找个时间去拜读一下原来的论文吧</p><p>这里也有四个点需要去注意的</p><ul><li><p>ReflectionPad2d这是一种更高级的padding方式，不像之前的padding是直接加0，这个会在内部进行优化。</p></li><li><p>这里用到了残切块的方式，防止过深的网络结构梯度消失，</p></li><li><p>这个地方运用到了InstanceNorm2d这种归一化方式，这里特意上网查询了一下 BN，LN，IN，GN从学术化上解释差异：</p><ul><li><p>BatchNorm：batch方向做归一化，算NHW的均值，对小batchsize效果不好；BN主要缺点是对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以如果batchsize太小，则计算的均值、方差不足以代表整个数据分布</p><ul><li><p>num_features： 来自期望输入的特征数，该期望输入的大小为’batch_size x num_features [x width]’</p></li><li><p>eps： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。</p></li><li><p>momentum： 动态均值和动态方差所使用的动量。默认为0.1。</p></li><li><p>affine： 布尔值，当设为true，给该层添加可学习的仿射变换参数。</p></li><li><p>track_running_stats：布尔值，当设为true，记录训练过程中的均值和方差；</p></li></ul></li></ul></li></ul><pre><code>* LayerNorm：channel方向做归一化，算CHW的均值，主要对RNN作用明显；    * normalized_shape： 输入尺寸        [∗×normalized_shape[0]×normalized_shape[1]×…×normalized_shape[−1]]    * eps： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。    * elementwise_affine： 布尔值，当设为true，给该层添加可学习的仿射变换参数。* InstanceNorm：一个channel内做归一化，算H * W的均值，用在风格化迁移；因为在图像风格化中，生成结果主要依赖于某个图像实例，所以对整个batch归一化不适合图像风格化中，因而对HW做归一化。可以加速模型收敛，并且保持每个图像实例之间的独立。    * num_features： 来自期望输入的特征数，该期望输入的大小为’batch_size x num_features [x width]’    * eps： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。    * momentum： 动态均值和动态方差所使用的动量。默认为0.1。    * affine： 布尔值，当设为true，给该层添加可学习的仿射变换参数。    * track_running_stats：布尔值，当设为true，记录训练过程中的均值和方差；* GroupNorm：将channel方向分group，然后每个group内做归一化，算(C//G)HW的均值；这样与batchsize无关，不受其约束。    * num_groups：需要划分为的groups    * num_features： 来自期望输入的特征数，该期望输入的大小为’batch_size x * num_features [x width]’    * eps： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。    * momentum： 动态均值和动态方差所使用的动量。默认为0.1。    * affine： 布尔值，当设为true，给该层添加可学习的仿射变换参数。* SwitchableNorm是将BN、LN、IN结合，赋予权重，让网络自己去学习归一化层应该使用什么方法。个人感觉最后一种归一化的方法好牛逼啊。</code></pre><ul><li>t.nn.functional.interpolate 这个函数就是用来上采样，虽然我还不懂什么是上采样 大意就是这个函数是用来上采样或下采样，可以给定size或者scale_factor来进行上下采样。同时支持3D、4D、5D的张量输入。插值算法可选，最近邻、线性、双线性等等。</li></ul><p>这里模型已经建立完毕了，接下来就是去关注 主模块和配置函数了，看看这个模型怎么去使用</p><h3 id="主模块"><a href="#主模块" class="headerlink" title="主模块"></a>主模块</h3><p>直接贴出代码咯<br><strong>配置类：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Config</span><span class="params">(object)</span>:</span></span><br><span class="line">    use_gpu = <span class="keyword">False</span></span><br><span class="line">    model_path = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    image_size = <span class="number">256</span></span><br><span class="line">    batch_size = <span class="number">8</span></span><br><span class="line">    data_root = <span class="string">'data/'</span></span><br><span class="line">    num_works = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    lr = <span class="number">1e-3</span></span><br><span class="line">    max_epoch = <span class="number">2</span></span><br><span class="line">    content_weight = <span class="number">1e5</span></span><br><span class="line">    style_weight = <span class="number">1e10</span></span><br><span class="line"></span><br><span class="line">    style_path = <span class="string">'style.png'</span></span><br><span class="line">    env = <span class="string">'neural-style'</span></span><br><span class="line">    plot_every = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    debug_file = <span class="string">'/tmp/debug'</span></span><br><span class="line"></span><br><span class="line">    content_path = <span class="string">'input.png'</span></span><br><span class="line">    result_path = <span class="string">'output.png'</span></span><br></pre></td></tr></table></figure><p><strong>主模块：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> fire</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> ipdb</span><br><span class="line"><span class="keyword">import</span> torchnet <span class="keyword">as</span> tnt</span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">import</span> torchvision <span class="keyword">as</span> tv</span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> Config</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data</span><br><span class="line"><span class="keyword">from</span> models.transformer_net <span class="keyword">import</span> TransformerNet</span><br><span class="line"><span class="keyword">from</span> models.vgg16 <span class="keyword">import</span> Vgg16</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(**kwargs)</span>:</span></span><br><span class="line">    opt = Config()</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> kwargs.items():</span><br><span class="line">        setattr(opt, k, v)</span><br><span class="line"></span><br><span class="line">    device = t.device(<span class="string">'cuda'</span>) <span class="keyword">if</span> opt.use_gpu <span class="keyword">else</span> t.device(<span class="string">'cpu'</span>)</span><br><span class="line">    vis = utils.Visualizer(opt.env)</span><br><span class="line"></span><br><span class="line">    transforms = tv.transforms.Compose([</span><br><span class="line">        tv.transforms.Resize(opt.image_size),</span><br><span class="line">        tv.transforms.CenterCrop(opt.image_size),</span><br><span class="line">        tv.transforms.ToTensor(),</span><br><span class="line">        tv.transforms.Lambda(<span class="keyword">lambda</span> x: x * <span class="number">255</span>)</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    dataset = tv.datasets.ImageFolder(opt.data_root, transforms)</span><br><span class="line">    dataloader = data.DataLoader(dataset, opt.batch_size, num_workers=opt.num_works)</span><br><span class="line"></span><br><span class="line">    transformer = TransformerNet()</span><br><span class="line">    <span class="keyword">if</span> opt.model_path:</span><br><span class="line">        transformer.load_state_dict(t.load(opt.model_path, map_location=<span class="keyword">lambda</span> storage, loc: storage))</span><br><span class="line">    transformer.to(device)</span><br><span class="line"></span><br><span class="line">    vgg = Vgg16.eval()</span><br><span class="line">    vgg.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment">## 这里是为了保护Vgg里面的参数不要去变动， 直接将里面的参数改成不能反向传播</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> vgg.parameters():</span><br><span class="line">        param.requires_grad = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">    optimizer = t.optim.Adam(transformer.parameters(), opt.lr)</span><br><span class="line"></span><br><span class="line">    style = utils.get_style_data(opt.style_path)</span><br><span class="line">    vis.img(<span class="string">'style'</span>, (style.data[<span class="number">0</span>] * <span class="number">0.225</span> + <span class="number">0.45</span>).clamp(min=<span class="number">0</span>, max=<span class="number">1</span>))</span><br><span class="line">    style = style.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> t.no_grad():</span><br><span class="line">        feature_style = vgg(style)</span><br><span class="line">        gram_style = list(utils.gram_matrix(y) <span class="keyword">for</span> y <span class="keyword">in</span> feature_style)</span><br><span class="line"></span><br><span class="line">    style_meter = tnt.meter.AverageValueMeter()</span><br><span class="line">    content_meter = tnt.meter.AverageValueMeter()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(opt.max_epoch):</span><br><span class="line">        content_meter.reset()</span><br><span class="line">        style_meter.reset()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> ii, (x, _) <span class="keyword">in</span> tqdm.tqdm(enumerate(dataloader)):</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            x = x.to(device)</span><br><span class="line">            y = transforms(x)</span><br><span class="line">            y = utils.normalize_batch(y)</span><br><span class="line">            x = utils.normalize_batch(x)</span><br><span class="line"></span><br><span class="line">            <span class="comment">## 这个地方得在仔细敲定一下，没有弄清楚这个features_y的结构</span></span><br><span class="line">            features_y = vgg(y)</span><br><span class="line">            features_x = vgg(x)</span><br><span class="line">            content_loss = opt.content_weight * F.mse_loss(features_y.relu2_2, features_x.relu2_2)</span><br><span class="line"></span><br><span class="line">            style_loss = <span class="number">0.</span></span><br><span class="line">            <span class="keyword">for</span> ft_y, gm_s <span class="keyword">in</span> zip(features_y, gram_style):</span><br><span class="line">                gram_y = utils.gram_matrix(ft_y)</span><br><span class="line">                style_loss += F.mse_loss(gram_y, gm_s.expand_as(gram_y))</span><br><span class="line">            style_loss *= opt.style_weight</span><br><span class="line"></span><br><span class="line">            total_loss = content_loss + style_loss</span><br><span class="line">            total_loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            content_meter.add(content_loss.item())</span><br><span class="line">            style_meter.add(style_loss.item())</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (ii + <span class="number">1</span>) % opt.plot_every == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">if</span> os.path.exists(opt.debug_file):</span><br><span class="line">                    ipdb.set_trace()</span><br><span class="line"></span><br><span class="line">                vis.plot(<span class="string">'content_loss'</span>, content_meter.value()[<span class="number">0</span>])</span><br><span class="line">                vis.plot(<span class="string">'style_loss'</span>, style_meter.value()[<span class="number">0</span>])</span><br><span class="line">                vis.img(<span class="string">'output'</span>, (y.data.cpu()[<span class="number">0</span>] * <span class="number">0.225</span> + <span class="number">0.45</span>).clamp(min=<span class="number">0</span>, max=<span class="number">1</span>))</span><br><span class="line">                vis.img(<span class="string">'input'</span>, (x.data.cpu()[<span class="number">0</span>] * <span class="number">0.225</span> + <span class="number">0.45</span>).clamp(min=<span class="number">0</span>, max=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        vis.save([opt.env])</span><br><span class="line">        t.save(transformer.state_dict(), <span class="string">'checkpoints/%s_style.pth'</span> % epoch)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stylize</span><span class="params">(**kwargs)</span>:</span></span><br><span class="line">    opt = Config()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> kwargs.items():</span><br><span class="line">        setattr(opt, k, v)</span><br><span class="line">    device = t.device(<span class="string">'cuda'</span>) <span class="keyword">if</span> opt.use_gpu <span class="keyword">else</span> t.device(<span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line">    content_image = tv.datasets.folder.default_loader(opt.content_path)</span><br><span class="line">    content_transform = tv.transforms.Compose([</span><br><span class="line">        tv.transforms.ToTensor(),</span><br><span class="line">        tv.transforms.Lambda(<span class="keyword">lambda</span> x: x * <span class="number">255</span>)</span><br><span class="line">    ])</span><br><span class="line">    content_image = content_transform(content_image)</span><br><span class="line">    content_image = content_image.unsqueeze(<span class="number">0</span>).to(device).detach()</span><br><span class="line"></span><br><span class="line">    style_model = TransformerNet().eval()</span><br><span class="line">    style_model.load_state_dict(t.load(opt.model_path, map_location=<span class="keyword">lambda</span> _s, _: _s))</span><br><span class="line">    style_model.to(device)</span><br><span class="line"></span><br><span class="line">    output = style_model(content_image)</span><br><span class="line">    output_data = output.cpu().data[<span class="number">0</span>]</span><br><span class="line">    tv.utils.save_image((output_data / <span class="number">255</span>).clamp(max=<span class="number">1</span>, min=<span class="number">0</span>), opt.result_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    fire.Fire()</span><br></pre></td></tr></table></figure><p>注意五点：</p><ul><li><p>保护VGG-16里面的预选参数不去改变，需要将其里面的所有参数的requires_grad设置成False，并且在提取data数据的是时候，直接将里面所有的数据像素乘以255倍</p></li><li><p>另外在展示图片的时候，将风格图片style.data[0] * 0.225 + 0.45).clamp(min=0, max=1)， 这个地方没怎么看懂，有会的大佬可以交流一下，以后找个时间去弄懂</p></li><li><p>在计算风格特征的时候，为什么不能够直接用detach（），而这里是直接将Vgg-16里面参数设置成不可以no_grad（）</p></li><li><p>Vgg-16只是提取特征的网络，首先先用transnet跑出来比较，并且将图片的通道数进行一个有效的归一化？这个需要去好好想想</p></li><li><p>另外 将特征提取出来之后，与风格图片相比的就是 风格loss，与内容相比的就是内容loss，关于风格系数这里 涉及到很多数学知识，我需要后期去弄清楚。</p></li></ul><p>最后总结一下 几个问题要弄清楚，</p><ol><li><p>为什么要对照片*255，最后输出的时候再去255</p></li><li><p>为什么再训练过程中，对图片有一个* 0.225 + 0.45).clamp(min=0, max=1)的操作</p></li><li><p>为什么 要在转换函数之后，对愿图片和转换图片进行一个通道数的归一化？？？ 不知道是不是通道数</p></li><li><p>风格系数的公式以及计算风格特征的损失函数需要去查看相关论文。</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
      <category term="机器学习" scheme="http://wsx1128.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://wsx1128.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Pytorch" scheme="http://wsx1128.cn/tags/Pytorch/"/>
    
      <category term="风格迁移" scheme="http://wsx1128.cn/tags/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"/>
    
  </entry>
  
  <entry>
    <title>GAN对抗网络生成动漫图像---基于pytorch框架的小应用</title>
    <link href="http://wsx1128.cn/2020/03/14/GAN%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E7%94%9F%E6%88%90%E5%8A%A8%E6%BC%AB%E5%9B%BE%E5%83%8F-%E5%9F%BA%E4%BA%8Epytorch%E6%A1%86%E6%9E%B6%E7%9A%84%E5%B0%8F%E5%BA%94%E7%94%A8/"/>
    <id>http://wsx1128.cn/2020/03/14/GAN对抗网络生成动漫图像-基于pytorch框架的小应用/</id>
    <published>2020-03-14T10:16:09.000Z</published>
    <updated>2020-03-14T11:47:35.888Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>这是在pytorch教程上面一个简单的应用，利用对抗网络生成图像生成动漫图像</p><p>该类网络 分成两个结构模型： 捕获数据分布的生成模型G，和估计样本来自训练数据的概率的判别模型D</p><p>G的训练就是将D的错误最大化，GAN主要针对的是生成类的模型</p><p>目前，深度学习的领域的图像生成、风格迁移，图像变化，图像描述，无监督学习，甚至强化学习领域都能看到GAN的影子。</p><p>目前深度学习领域可以分为两大类：</p><p>其一是检测识别，比如图像分类，目标识别，此类模型主要有VGG，GoogLenet，Residual net等，几乎所有的网络都是基于识别的</p><p>其二是图像生成，即解决如何从一些数据里生成图像，生成类模型主要有深度信念网络DBN，变分zibianmaqiVAE。而某种程度上，GAN的生成能力远超过DBN、VAE。经过改进后的GAN足以生成以假乱真的图像。</p><p>机器学习的模型大体可分为两类：生成模型（Generative model）和判别模型（Discriminative model）。</p><p>判别模型需要输入变量，通过某种模型来预测。</p><p>生成模型是给定某种隐含信息，来随机产生观测数据。</p><h3 id="GAN原理"><a href="#GAN原理" class="headerlink" title="GAN原理"></a>GAN原理</h3><p>GAN基本原理：以生成图片为例。假设我们有两个网络：G(Generator)和D(Discriminator)。他们的功能分别为：</p><p>G：一个生成图片的网络，它接收一个随机噪声z，通过这个噪声生成图片，记做G(z)；</p><p>D：一个判别网络，判别一张图片是不是“真实的”。它的输入参数是x，x代表一张图片，输出D(x)代表x为真实图片的概率。</p><p>如果为1，就带包100%是真实的图片，而输出为0，就代表不可能是真实图片。</p><p>在训练过程中，生成网络G的目标就是尽量生成真实的图片去欺骗判别网络D，而D的目标就是尽量把G生成的图片和真实的图片分别开来。这样，G和D构成了一个动态的“博弈”。</p><p>理想状态下，G可以生成足以“以假乱真”的图片G(z)。 对D来说。它难以判定G生成的图片究竟是不是真实的。这样我们的目的就达到了</p><p>这样我们就可以得到一个生成式的模型，可以用它来生成图片</p><p><img src="/2020/03/14/GAN对抗网络生成动漫图像-基于pytorch框架的小应用/1.png" alt=""></p><p>其中G为生成模型，D为判别模型。</p><p>GAN的大概流程是：G以随机噪声作为输入，生成一张图像G(z)，暂且不管生成质量多好，然后D以G(z)和真实图像x作为输入，对G(z)和x做一个二分类，检测谁是真实图像，谁是生成的假图像。</p><p>举一个最简单的例子</p><p>A为卖假钞的团队  B为抓假钞的警察</p><p>A最先开始没有经验，所以一开始做出来的假钞很没有水准，而B一开始也没有能力辨别假钞，所以一开始对A制造出来的假钞没有太大敌意，全盘接受。</p><p>于是B开始慢慢学习，久而久之发现不太对劲，于是开始钻研假钞。最后发现，A团队研制假钞，所以开始追杀A。</p><p>这个时候A发现自己的假钞被识别出来了，于是加大研究力度，研究出了更不容易被发现的假钞<br>然后B也被绕进去了，于是也开始研究反假钞…</p><p>于是就一直这样重复进行博弈，最后我们得到的训练模型是 一个可以完美生成“真的”假钞的技术</p><p><strong>以上只是举个小小的例子，帮助理解GAN，我坚决拥护中国共产党</strong></p><p>下面 回到本次小应用上面，</p><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>这里也是同样存在两个模型，</p><ul><li><p>一个模型用来接受一个随机的一维噪音维度，并且生成一系列动漫头像。</p></li><li><p>一个是用来接受动漫头像，并且用于判断其是否是真正的动漫头像。</p></li></ul><p>接下来，请看模型的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net_G</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, opt)</span>:</span></span><br><span class="line">        super(Net_G,self).__init__()</span><br><span class="line"></span><br><span class="line">        ngf = opt.ngf <span class="comment"># 卷积层中间用来描述给予图像的特征</span></span><br><span class="line"></span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            <span class="comment"># 100 * 1 * 1 -&gt; (ngf * 8) * 4 * 4</span></span><br><span class="line">            nn.ConvTranspose2d(opt.nc, ngf*<span class="number">8</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ngf * <span class="number">8</span>),</span><br><span class="line">            nn.ReLU(<span class="keyword">True</span>),</span><br><span class="line"></span><br><span class="line">            <span class="comment"># (ngf * 8) * 4 * 4 -&gt; (ngf * 4) * 8 * 8</span></span><br><span class="line">            nn.ConvTranspose2d(ngf*<span class="number">8</span>, ngf*<span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ngf * <span class="number">4</span>),</span><br><span class="line">            nn.ReLU(<span class="keyword">True</span>),</span><br><span class="line"></span><br><span class="line">            <span class="comment"># (ngf * 4) * 8 * 8 -&gt; (ngf * 2) * 16 * 16</span></span><br><span class="line">            nn.ConvTranspose2d(ngf*<span class="number">4</span>, ngf*<span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ngf*<span class="number">2</span>),</span><br><span class="line">            nn.ReLU(<span class="keyword">True</span>),</span><br><span class="line"></span><br><span class="line">            <span class="comment"># (ngf * 2) * 16 * 16 -&gt; ngf * 32 * 32</span></span><br><span class="line">            nn.ConvTranspose2d(ngf*<span class="number">2</span>, ngf, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ngf),</span><br><span class="line">            nn.ReLU(<span class="keyword">True</span>),</span><br><span class="line"></span><br><span class="line">            <span class="comment"># ngf * 32 * 32 -&gt; 3 * 96 * 96</span></span><br><span class="line">            nn.ConvTranspose2d(ngf, <span class="number">3</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.Tanh()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.net(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net_D</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, opt)</span>:</span></span><br><span class="line">        super(Net_D,self).__init__()</span><br><span class="line">        ndf = opt.ndf</span><br><span class="line"></span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            <span class="comment"># 3 * 96 * 96 -&gt; ndf * 32 * 32</span></span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, ndf, <span class="number">5</span>, <span class="number">3</span>, <span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ndf),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>,<span class="keyword">True</span>),</span><br><span class="line"></span><br><span class="line">            <span class="comment"># ndf * 32 * 32 -&gt; (ndf * 2) * 16 * 16</span></span><br><span class="line">            nn.Conv2d(ndf, ndf*<span class="number">2</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ndf*<span class="number">2</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>,<span class="keyword">True</span>),</span><br><span class="line"></span><br><span class="line">            <span class="comment"># (ndf * 2) * 16 * 16 -&gt; (ndf * 4) * 8 * 8</span></span><br><span class="line">            nn.Conv2d(ndf*<span class="number">2</span>, ndf*<span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ndf*<span class="number">4</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, <span class="keyword">True</span>),</span><br><span class="line"></span><br><span class="line">            <span class="comment"># (ndf * 4) * 8 * 8 -&gt; (ndf*8) * 4 * 4</span></span><br><span class="line">            nn.Conv2d(ndf*<span class="number">4</span>, ndf*<span class="number">8</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.BatchNorm2d(ndf*<span class="number">8</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>, <span class="keyword">True</span>),</span><br><span class="line"></span><br><span class="line">            <span class="comment"># (ndf*8) * 4 * 4 -&gt; 100 * 1 * 1,</span></span><br><span class="line">            nn.Conv2d(ndf*<span class="number">8</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="keyword">False</span>),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.net(x).view(<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><p>上面这段代码需要注意四点</p><ul><li><p>仔细看网络其实是对称的，并且图片的维度应该是 m <em> nc </em> n * n, nc表示通道数，n表示图片尺寸</p></li><li><p>Conv2d 卷积层是一个将图片变成特征的过程，而ConvTranspose2d则是将特征变成图片的过程，参数都是类似的过程，另外计算公式正好就是恰恰相反 (fs - kerner_size + 2*padding) / stride 后者的公式就是将其倒过来就可以</p></li><li><p>不需要bias的原因是因为在BatchNorm2d中已经进行了参数归一化的过程</p></li></ul><p>模型有了，接下来就是处理数据图片</p><h3 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h3><p>这里就不像之前一样专门封装一个数据集，上次封装数据集，是因为在取出数据的时候，需要对标签值，进行一个处理，（也就是对Y值进行一个处理），而这次全部都是图片数据，且不需要做一些额外的处理，所以直接使用即可</p><p>这里直接使用的是ImageFolder类，帮助处理相关图片文件夹群</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">transforms = tv.transforms.Compose([</span><br><span class="line">        tv.transforms.Resize(opt.image_size),</span><br><span class="line">        tv.transforms.CenterCrop(opt.image_size),</span><br><span class="line">        tv.transforms.ToTensor(),</span><br><span class="line">        tv.transforms.Normalize((<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>),(<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>))</span><br><span class="line">    ])</span><br><span class="line">    dataset = tv.datasets.ImageFolder(opt.data_path,transform=transforms)</span><br><span class="line">    dataloader = DataLoader(dataset,</span><br><span class="line">                            opt.batch_size,</span><br><span class="line">                            shuffle=<span class="keyword">True</span>,</span><br><span class="line">                            num_workers=opt.num_works,</span><br><span class="line">                            drop_last=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><h3 id="主模块"><a href="#主模块" class="headerlink" title="主模块"></a>主模块</h3><p>前面都算是简单的环节，其实主要困难在配置函数和主类中</p><p>接下里请看代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Config</span><span class="params">(object)</span>:</span></span><br><span class="line">    data_path = <span class="string">'data/'</span></span><br><span class="line">    num_works = <span class="number">4</span></span><br><span class="line">    image_size = <span class="number">96</span></span><br><span class="line">    batch_size = <span class="number">256</span></span><br><span class="line">    max_epoch = <span class="number">200</span></span><br><span class="line">    lr1 = <span class="number">2e-4</span></span><br><span class="line">    lr2 = <span class="number">2e-4</span></span><br><span class="line">    beta1 = <span class="number">0.5</span></span><br><span class="line">    gpu = <span class="keyword">False</span></span><br><span class="line">    nc = <span class="number">100</span></span><br><span class="line">    ngf = <span class="number">64</span></span><br><span class="line">    ndf = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">    save_path = <span class="string">'imgs/'</span></span><br><span class="line">    vis = <span class="keyword">True</span></span><br><span class="line">    env=<span class="string">'GAN'</span></span><br><span class="line">    plot_every = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">    debug_file = <span class="string">'/tmp/debug'</span></span><br><span class="line">    d_every = <span class="number">1</span></span><br><span class="line">    g_every = <span class="number">5</span></span><br><span class="line">    save_every = <span class="number">10</span></span><br><span class="line">    netd_path = <span class="keyword">None</span></span><br><span class="line">    netg_path = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    gen_img = <span class="string">'result.png'</span></span><br><span class="line">    gen_num = <span class="number">64</span></span><br><span class="line">    gen_search_num = <span class="number">512</span></span><br><span class="line">    gen_mean = <span class="number">0</span></span><br><span class="line">    gen_std = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">opt = Config()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> fire</span><br><span class="line"><span class="keyword">import</span> ipdb</span><br><span class="line"><span class="keyword">import</span> visdom</span><br><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">import</span> torchvision <span class="keyword">as</span> tv</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable <span class="keyword">as</span> V</span><br><span class="line"><span class="keyword">from</span> models <span class="keyword">import</span> Net_G,Net_D</span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> opt</span><br><span class="line"><span class="keyword">from</span> visualize <span class="keyword">import</span> Visualize</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchnet.meter <span class="keyword">import</span> AverageValueMeter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(**kwargs)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> vis, fix_fake_img</span><br><span class="line">    <span class="keyword">for</span> k_,v_ <span class="keyword">in</span> kwargs.items():</span><br><span class="line">        setattr(opt, k_, v_)</span><br><span class="line">    device = t.device(<span class="string">'cuda'</span>) <span class="keyword">if</span> opt.gpu <span class="keyword">else</span> t.device(<span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line">    vv = visdom.Visdom(env=<span class="string">'image'</span>)</span><br><span class="line">    print(opt.netd_path)</span><br><span class="line">    print(opt.netg_path)</span><br><span class="line">    <span class="keyword">if</span> opt.vis:</span><br><span class="line">        <span class="keyword">from</span> visualize <span class="keyword">import</span> Visualize</span><br><span class="line">        vis = Visualize(opt.env)</span><br><span class="line"></span><br><span class="line">    transforms = tv.transforms.Compose([</span><br><span class="line">        tv.transforms.Resize(opt.image_size),</span><br><span class="line">        tv.transforms.CenterCrop(opt.image_size),</span><br><span class="line">        tv.transforms.ToTensor(),</span><br><span class="line">        tv.transforms.Normalize((<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>),(<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>))</span><br><span class="line">    ])</span><br><span class="line">    dataset = tv.datasets.ImageFolder(opt.data_path,transform=transforms)</span><br><span class="line">    dataloader = DataLoader(dataset,</span><br><span class="line">                            opt.batch_size,</span><br><span class="line">                            shuffle=<span class="keyword">True</span>,</span><br><span class="line">                            num_workers=opt.num_works,</span><br><span class="line">                            drop_last=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    net_g,net_d = Net_G(opt),Net_D(opt)</span><br><span class="line">    <span class="keyword">if</span> opt.netd_path:</span><br><span class="line">        net_d.load_state_dict(t.load(opt.netd_path, map_location=<span class="keyword">lambda</span> storage, loc: storage))</span><br><span class="line">    <span class="keyword">if</span> opt.netg_path:</span><br><span class="line">        net_g.load_state_dict(t.load(opt.netg_path, map_location=<span class="keyword">lambda</span> storage, loc:storage))</span><br><span class="line"></span><br><span class="line">    net_g.to(device)</span><br><span class="line">    net_d.to(device)</span><br><span class="line"></span><br><span class="line">    optimizer_g = t.optim.Adam(net_g.parameters(), opt.lr1, betas=(opt.beta1, <span class="number">0.999</span>))</span><br><span class="line">    optimizer_d = t.optim.Adam(net_d.parameters(), opt.lr2, betas=(opt.beta1, <span class="number">0.999</span>))</span><br><span class="line"></span><br><span class="line">    criterion = t.nn.BCELoss().to(device)</span><br><span class="line"></span><br><span class="line">    true_labels = V(t.ones(opt.batch_size).to(device))</span><br><span class="line">    fake_labels = V(t.zeros(opt.batch_size).to(device))</span><br><span class="line">    noises = V(t.randn(opt.batch_size, opt.nc, <span class="number">1</span>, <span class="number">1</span>).to(device))</span><br><span class="line">    fix_noises = V(t.randn(opt.batch_size, opt.nc, <span class="number">1</span>, <span class="number">1</span>).to(device))</span><br><span class="line"></span><br><span class="line">    error_d_meter = AverageValueMeter()</span><br><span class="line">    error_g_meter = AverageValueMeter()</span><br><span class="line"></span><br><span class="line">    epochs = range(opt.max_epoch)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> epochs:</span><br><span class="line">        <span class="keyword">for</span> ii, (img,_) <span class="keyword">in</span> tqdm.tqdm(enumerate(dataloader)):</span><br><span class="line">            real_img = img.to(device)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> ii % opt.d_every == <span class="number">0</span>:</span><br><span class="line">                optimizer_d.zero_grad()</span><br><span class="line">                output = net_d(real_img)</span><br><span class="line">                error_d_real = criterion(output,true_labels)</span><br><span class="line">                error_d_real.backward()</span><br><span class="line"></span><br><span class="line">                noises.data.copy_(t.randn(opt.batch_size, opt.nc, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">                fake_img = net_g(noises).detach()</span><br><span class="line">                output = net_d(fake_img)</span><br><span class="line">                error_d_fake = criterion(output, fake_labels)</span><br><span class="line">                error_d_fake.backward()</span><br><span class="line">                optimizer_d.step()</span><br><span class="line">                error_d = error_d_fake + error_d_real</span><br><span class="line">                error_d_meter.add(error_d.item())</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> ii % opt.g_every == <span class="number">0</span>:</span><br><span class="line">                optimizer_g.zero_grad()</span><br><span class="line">                noises.data.copy_(t.randn(opt.batch_size, opt.nc, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">                fake_img = net_g(noises)</span><br><span class="line">                output = net_d(fake_img)</span><br><span class="line">                error_g = criterion(output, true_labels)</span><br><span class="line">                error_g.backward()</span><br><span class="line">                optimizer_g.step()</span><br><span class="line">                error_g_meter.add(error_g.item())</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> opt.vis <span class="keyword">and</span> ii % opt.plot_every == opt.plot_every - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">if</span> os.path.exists(opt.debug_file):</span><br><span class="line">                    ipdb.set_trace()</span><br><span class="line">                fix_fake_img = net_g(fix_noises)</span><br><span class="line">                <span class="comment"># vis.img('fixfake',fix_fake_img.detach().data.cpu().numpy()[:64] * 0.5 + 0.5)</span></span><br><span class="line">                <span class="comment"># vis.img('real', (real_img.cpu().numpy()[:64] * 0.5 + 0.5))</span></span><br><span class="line">                vv.images(fix_fake_img.detach().cpu().numpy()[:<span class="number">64</span>] * <span class="number">0.5</span> + <span class="number">0.5</span>, win=<span class="string">'fixfake'</span>)</span><br><span class="line">                vv.images(real_img.data.cpu().numpy()[:<span class="number">64</span>] * <span class="number">0.5</span> + <span class="number">0.5</span>, win=<span class="string">'real'</span>)</span><br><span class="line"></span><br><span class="line">                vis.plot(<span class="string">'errord'</span>, error_d_meter.value()[<span class="number">0</span>])</span><br><span class="line">                vis.plot(<span class="string">'errorg'</span>, error_g_meter.value()[<span class="number">0</span>])</span><br><span class="line">            print(<span class="string">"current epoch: %d"</span> % epoch)</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % opt.save_every == <span class="number">0</span>:</span><br><span class="line">            tv.utils.save_image(fix_fake_img.data[:<span class="number">64</span>], <span class="string">'%s/%s.png'</span> %(opt.save_path, epoch)</span><br><span class="line">                                , normalize=<span class="keyword">True</span>,range=(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">            t.save(net_g.state_dict(),<span class="string">'checkpoints/netg_%s.pth'</span> % epoch)</span><br><span class="line">            t.save(net_d.state_dict(),<span class="string">'checkpoints/netd_%s.pth'</span> % epoch)</span><br><span class="line">            error_g_meter.reset()</span><br><span class="line">            error_d_meter.reset()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate</span><span class="params">(**kwargs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> k,v <span class="keyword">in</span> kwargs.items():</span><br><span class="line">        setattr(opt, k, v)</span><br><span class="line"></span><br><span class="line">    device = t.device(<span class="string">'cuda'</span>) <span class="keyword">if</span> opt.gpu <span class="keyword">else</span> t.device(<span class="string">'cpu'</span>)</span><br><span class="line">    net_g,net_d = Net_G(opt).eval(),Net_D(opt).eval()</span><br><span class="line">    noises = t.randn(opt.gen_search_num, opt.nc, <span class="number">1</span>, <span class="number">1</span>).normal_(opt.gen_mean, opt.gen_std)</span><br><span class="line">    noises = noises.to(device)</span><br><span class="line"></span><br><span class="line">    net_g.load_state_dict(t.load(opt.netg_path, map_location=<span class="keyword">lambda</span> storage,loc:storage))</span><br><span class="line">    net_d.load_state_dict(t.load(opt.netd_path, map_location=<span class="keyword">lambda</span> storage,loc:storage))</span><br><span class="line">    net_g.to(device)</span><br><span class="line">    net_d.to(device)</span><br><span class="line"></span><br><span class="line">    fake_img = net_g(noises)</span><br><span class="line">    scores = net_d(fake_img).detach()</span><br><span class="line"></span><br><span class="line">    indexs = scores.topk(opt.gen_num)[<span class="number">1</span>]</span><br><span class="line">    result = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ii <span class="keyword">in</span> indexs:</span><br><span class="line">        result.append(fake_img.data[ii])</span><br><span class="line"></span><br><span class="line">    tv.utils.save_image(t.stack(result), opt.gen_num, normalize=<span class="keyword">True</span>, range=(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    fire.Fire()</span><br></pre></td></tr></table></figure><p>关于这个主模块中，同样也有五点需要注意</p><ul><li><p>关于Adam优化器中的beta1 最好为0.8到0.9 而beta2最好为0.999， 而在这里前者用的是0.5，可能是研究出来的结果</p></li><li><p>这里需要几个标签来辅助训练，真标签与假标签，真标签用来与判定模型训练绑定，而假标签需要与生成模型训练绑定，然后在需要两个随机生成的噪声维度，一个是每次都需要去变化，另一个是不用变化，并且时时可视化生成器生成的最终效果的。</p></li><li><p>训练过程就是分为两个流程：训练判别模型和训练生成模型</p><ul><li><p>将真照片放到判别模型中，并且让结果与真标签对比的得到loss，而在此过程中同时还需要将生成器生成的假图片同样放到判别模型中去判假（与假标签进行联动），在词过程中得到两种loss，相加为判别器的loss</p></li><li><p>将随机噪音输入到生成模型中，并且让其对比训练，以得到假意乱真的图片</p></li></ul></li><li><p>在训练的过程中，使用了detach函数，因为A模型的输出做为了B模型的输入，因为不想反向传播甚至梯度下降的时候影响到A模型，所以这里就直接将A模型直接分离。</p></li><li><p>在生成函数中 用生成模型生成的图片，交给判定模型去判别，并且选出近似度最高的几个图形，并且输出。 其实说白了，判别模型在生成图片的时候不会给予直接或者间接的帮助，只能够在最后判定的过程中选出最相像的模型。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
      <category term="机器学习" scheme="http://wsx1128.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="对抗网络GAN" scheme="http://wsx1128.cn/tags/%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9CGAN/"/>
    
      <category term="深度学习" scheme="http://wsx1128.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Pytorch" scheme="http://wsx1128.cn/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Anomaly detection and Recommender Systems</title>
    <link href="http://wsx1128.cn/2020/03/14/Anomaly-detection-and-Recommender-Systems/"/>
    <id>http://wsx1128.cn/2020/03/14/Anomaly-detection-and-Recommender-Systems/</id>
    <published>2020-03-14T07:54:42.000Z</published>
    <updated>2020-03-14T09:46:48.119Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>首先声明，这篇博客下面所有的图片均来自与互联网上面其他多个人的博客</p><p>原因是一些公式只能截图发下来，要不然没有原汁原味的感觉。如果有侵权，本人务必会删除</p><h3 id="异常检测"><a href="#异常检测" class="headerlink" title="异常检测"></a>异常检测</h3><p>机器学习这门课即将接近尾声，这是倒数第二个章节的内容，意思就是异常检测，当机器学习的系统出现了相应的故障，这个时候该怎么去检测出，其到底出错在什么地方。</p><p><strong>那么，该如何检测呢</strong><br>举例：比如生产汽车引擎，需要进行质量测试，而作为测试的一部分，需要测量汽车引擎的一些特征变量：</p><ul><li><p>引擎运转时产生的热量</p></li><li><p>引擎的振动</p></li></ul><p><img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/1.png" alt=""></p><p>差不多更正式的定义： 给定一个数据集，对它进行数据建模 p(x)，当有新的特征变量Xtest时</p><p><img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/2.png" alt=""></p><p><strong>而其的相关应用就体现在</strong></p><ul><li><p>欺诈检测，例如有一些网站会记录用户的一些信息，比如说打字速度，单位时间浏览网页的次数等等，然后用这些建立模型，然后用这些数据去建立模型，看看哪些用户的指标不符合这些指令</p></li><li><p>工作生产领域： 就像之前提到的汽车引擎的问题，可以找到异常的汽车引擎的问题，然后进一步工业化快速的检测这些引擎的质量</p></li><li><p>数据中心的计算机监控： 如果管理一个计算机集群，可以为每一台计算机计算特征向量，内存损耗，以及硬盘访问量，cpu负载类似的情况，根据这些特征向量联合起来，然后在一起去判断此刻计算机是否处于正常的状态</p></li></ul><p><strong>接下来就来探讨有关异常检测的算法</strong></p><ul><li><p>高斯分布（概率论中的分布问题）<br>  这一节中将讨论高斯分布，也加正态分布，下一节中将由高斯分布推导异常检测算法。</p><p>  <img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/3.png" alt=""></p><p>  <img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/4.png" alt=""></p></li></ul><ul><li><p>高斯分布推导异常检测算法</p><p>  这一节中将应用高斯分布来推导异常检测算法。</p><p>  <img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/5.png" alt=""></p></li></ul><p><strong>下面是异常检测算法的步骤和实现：</strong></p><p><img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/6.png" alt=""></p><p>这一节中讲述了如何估计p(x)的值，即x的概率值，使得构造出了一种异常检测算法。</p><ul><li><p>开发和评估异常检测系</p><ul><li><p>这一节中将介绍如何开发一个异常检测的应用来解决实际问题，同时也将重点介绍如何评估一个异常检测算法。</p></li><li><p>到目前为止，都把异常检测看作是一个无监督学习问题，因为用的是无标签的数据，但如果有一个带标签的数据来指定哪些是异常样本和哪些是正常样本，这是能评估异常检测的标准方法。</p></li><li><p>为了评估一个异常检测系统，假设有一些带标签的数据，代表异常样本，同时还有一些无异常样本。（用y=0表示无异常样本，y=1表示异常样本）</p></li></ul></li></ul><ul><li><p>异常检测算法的开发过程以及评估方法</p><ol><li><p>假设有一个训练集，就将他看作是没有标签的，所以它是一个很大无异常的样本数据结合</p></li><li><p>定义一个交叉验证集和测试集，用来评估这个异常检测算法</p></li><li><p>得到上面所说的这些集合之后，使用训练集去拟合模型p(x),即把m个无标签的样本都用高斯函数来拟合</p></li><li><p>在交叉集上用异常检测的算法来预测出y的值</p></li><li><p>用一些好的评价指标来评价算法</p><ul><li><p>计算True positive、false positive、false negative、true negative的比例</p></li><li><p>计算出算法的精确率和召回率</p></li><li><p>计算出F1-score，来总结和反映精确率和召回率</p></li></ul></li><li><p>尝试使用不同的ε值，然后从中选出一个，该值能够最大化F1-score。</p></li></ol></li></ul><p>这一节中讨论了评估一个异常检测算法的步骤，开始用一些带标签的数据来评估异常检测算法，很像在监督学习中所做的。</p><p><strong>异常检测与监督学习比较</strong></p><p>在这一节中将介绍一些思想和方法可以在使用异常检测算法时用到，并且在使用监督学习算法时，这些方法可能会更有效。</p><p>异常检测算法更有效的地方：</p><ul><li><p>它的正常例子数量更少，记住当y=1时这些例子为异常样例（0-20个正样本）</p></li><li><p>通常会有一个相比于正样本数量更大的负样本，可以用这数量庞大的负样本来拟合出的值。</p></li></ul><p>所以如果只有负样本，仍然可以很好的拟合的值。</p><p>监督学习的算法更有效的地方：</p><ul><li>在合理范围内有大量的正样本和负样本。</li></ul><p>所以这是一种思考方式决定应该使用异常检测算法还是监督学习算法。</p><p>另外还有一种使用异常检测算法的方法：</p><pre><code>对于异常检测应用来说，经常有许多不同类型的异常。因此，如果是这种情况，有很少数量的正样本，那么对一个算法就很难去从小数量的正样本中去学习异常是什么；尤其是，未来可能出现的异常看起来会与已有的截然不同，如果是这种情况，那么更有可能的是对负样本用高斯分布模型来建模。</code></pre><p>另一种经常考虑使用监督学习算法的方法：</p><pre><code>在一些其他问题中，有足够数量的正样本或是一个已经能识别正样本的算法；尤其是，假如认为未来可能出现的正样本与当前训练集中的正样本类似，那么这种情况下，使用监督学习算法更合理，它能够查看大量正样本和大量负样本来学到相应特征。所以决定应该使用异常检测算法还是监督学习算法，它们之间关键不同在于在异常检测中，通常只有很少量的正样本，对学习算法而言是不可能从这些正样本中学习到足够的内容。</code></pre><p>所以从本节的学习中，就可以知道分别在什么时候使用异常检测算法还是时候监督学习算法。</p><p><strong>选择要使用的功能</strong></p><p>这一节中将会给出一些建议：如何设计或选择异常检测算法的特征。</p><p>如果数据不是符合高斯分布，但是数据画出来图像是像下图所示的这样：</p><p><img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/7.png" alt=""></p><p>该图看上去近似于高斯分布，所以如果特征是这样的话，那么也可以将数据输入算法中，但是如果画出来的数据像这样：</p><p><img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/8.png" alt=""></p><p>该图看上去不像高斯分布的图像，是一个不均匀的分布，遇到这样的数据，通常会对数据进行一些不同地转换，使得它看上去更接近高斯分布。比如会进行一次对数转换，进行一次指数转换。</p><p>如何得到异常检测算法的特征？通过一个误差分析步骤：</p><ul><li><p>异常检测中，希望P(x)的值在正常样本的情况下比较大，在异常的情况下比较小</p></li><li><p>很常见的问题：P(x)是可比较的，当样本正常和异常时，P(x)的值都比较大。</p></li></ul><p><strong>多变量高斯分布以及它在异常检测中的应用</strong></p><pre><code>这一节中将介绍目前为止学习的异常检测算法的一种延伸，这个延伸将会用到多元高斯分布。</code></pre><p><img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/9.png" alt=""></p><p>多元高斯分布的几个例子如下：</p><p><img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/10.png" alt=""></p><p>通过这些例子可以让我们对多元高斯分布所能描述的概率分布有更多的了解，它最大的优点就是能描述两个特征变量之间可能存在正相关或者负相关的情况。</p><p>那么，如何在这两个模型之间进行选择呢？</p><p>原始模型要用的跟多一些；多元高斯模型用的想多少一些，但它在捕捉特征间的关系方面有着很多优点。</p><p>原始模型：</p><ul><li><p>需要手动创建一些新特征来捕捉异常的组合值，因为比如x1和x2组合值出现异常的时候，可能x1和x2本身看起来都是很正常的值，如果花时间来手动创建这样一个新特征，原始模型就=才能运行的更好</p></li><li><p>原始模型最大的优势在于它的计算成本比较低（换言之，它能适应巨大规模n）</p></li><li><p>即使有一个较小的有一定相关性的训练集，也能顺利运行，这是一个较小的无标签样本用来拟合模型p(x)。</p></li></ul><p>多元高斯模型：</p><ul><li><p>对于之下，多元高斯模型就能自动地捕捉这种不同特征之间的关系</p></li><li><p>但是它的计算成本比较高（即：能适应的n值的范围比较小）</p></li><li><p>必须使训练样本数量m大于特征数量n，如果在参数估计时不能满足这个条件的话，协方差矩阵就会不可逆（1.m &lt; n，2.有冗余特征），将无法使用多元高斯模型</p></li></ul><h3 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h3><p>这一章的内容就比较简单了，下面继续总结</p><p>这一章中将讨论推荐系统的有关内容，它是在机器学习中的一个重要应用。</p><p>机器学习领域的一个伟大思想：对于某些问题，有一些算法可以自动地学习一系列合适的特征，比起手动设计或编写特征更有效率。这是目前做的比较多的研究，有一些环境能让你开发某个算法来学习使用那些特征。</p><p>接下里让我们通过推荐系统的学习来领略一些特征学习的思想。</p><p>推荐系统预测电影评分的问题：</p><p>某些公司让用户对不同的电影进行评价，用0到5星来评级，下面是用户的评价情况：</p><p><img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/11.png" alt=""></p><p>符号介绍：</p><pre><code>nu表示用户的数量、nm表示电影的数量、r(i,j)等于1时表示用户j给电影i进行了评价、y(i,j)表示当评价后会得到的具体的评价星值。</code></pre><p>推荐系统就是在给出了r(i,j)和y(i,j)的值后，会去查找那些没有被评级的电影，并试图预测这些电影的评价星级。例如，在上述图中给出的结果，从Alice和Bob的评价中，他们给爱情片的评价比较高，可能预测他们没看过的电影也是4到5星；而Carol和Dave的评价中，对爱情片的评价低，而动作片的评价不错，所以他们对没看过的爱情片的评价会是0，而对动作片的评价可能会是4到5星。</p><p><img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/12.jpg" alt=""></p><p>因此，如果想开发一个推荐系统，那么需要想出一个学习算法—能自动填补这些缺失值的算法。这样就可以看到该用户还有哪些电影没看过，并推荐新的电影给该用户，可以去预测什么是用户感兴趣的内容。</p><p>这就是推荐系统问题的主要形式，接下来将讨论一个学习算法来解决这个问题。</p><p><strong>基于内容的推荐算法</strong></p><p>这一节中将介绍一种建立推荐系统的方法：基于内容的推荐算法。</p><p>例如上一节中提到的预测电影的例子，如何预测未评价的电影呢？假设对于每一部电影，都有一个对应的特征集，如下图所示：</p><p><img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/13.png" alt=""></p><p>现在为了做出预测，可以把每个用户的评价预测值看作是一个线性回归问题，特别规定，对于每一个用户j，要学习参数向量，通常来说，是一个n+1维，其中n是特征的数量。然后要预测用户j评价电影i的值，也就是参数向量与特征向量的内积（）。</p><p>假如想预测用户1-Alice对电影3的评价，那么电影3会有一个参数向量，假如用某种方法得到了Alice的参数向量，那么Alice对电影3的预测就等于，如下图的计算：</p><p><img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/14.png" alt=""></p><p>所以，上的操作就是对每一个用户应用了一个不同的线性回归的副本，上述给出了Alice的线性方程。同样的，其他用户都有一个不同的线性方程，这就是预测评价的方法。</p><p>接下来给出问题的正式定义：</p><p>如果用户j给出了电影i的评价，就将r(i,j)记为1，而y(i,j)是对电影的评价的具体评价值；是每个用户的一个参数，而是特定电影的一个特征向量，对于每个用户和电影，会给出预测。</p><p>用表示用户j评价电影的数量，那么为了学习参数向量要怎么做呢？</p><p>这就像是线性回归，最小化参数向量，加上正则化项，如下所示：</p><p><img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/15.png" alt=""></p><p>通过这个方法可以得到对参数向量theta的估计值，当然，当构建推荐系统时，不仅要学习单个用户的参数，要学习所有用户的参数（ni个用户），可以得到公式：</p><p><img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/16.png" alt=""></p><p>为了实现最小化，推导梯度下降的更新公式如下：</p><p><img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/17.png" alt=""></p><p>这就是如何将变量应用到线性回归中来预测不同用户对不同电影的评级，这里描述电影内容的特征量来做预测，这个特殊的算法叫做基于内容的推荐算法，接下来将介绍另一种推荐算法。</p><p><strong>协同过滤</strong></p><p>这一节中将讨论用来构建系统的方法，叫做协同过滤，这种算法能够自行学习要使用的特征。</p><p>这里还是用上一节中的预测电影的例子，但是不知道特征量具体的值，如下图所示：</p><p><img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/18.png" alt=""></p><p>假设这里采访了上述4个用户，他们告知了喜欢爱情电影的程度以及喜欢动作电影的程度，于是每个用户有了对应的下面四个变量：</p><p><img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/19.png" alt=""></p><p>根据上面的特征值，我们需要优化的目标函数是：</p><p><img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/20.png" alt=""></p><p>当然，我们不仅要学习单个电影的特征，还要学习所有电影的所有特征（个电影），可以得到公式：</p><p><img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/21.png" alt=""></p><p>总结一下协同过滤的方法：</p><p>如果已知电影的特征量，可以根据不同电影的特征，学习参数θ，已知这些特征，就能学习出不同用户的参数θ；如果用户愿意提供这些参数θ，那么就能估计出各种电影的特征值。</p><p>这样的迭代过程，会得到更好的θ和x，如果重复这个过程，算法将会收敛到一组合理的电影特征以及一组合理的对不同用户的参数估计，这就是协同过滤。</p><p>这一节中，探讨了最基本的协同过滤算法，它指的是当你执行算法时，要观察大量的用户的实际行为来协同地得到更佳的每个人对电影的评分值，协同的另一层含义就是每个用户都在帮助算法更好地进行特征学习。</p><p><strong>协同过滤算法</strong></p><p>这一节中将会结合前两节中讲述到的概念来设计协同过滤算法。</p><p>我们并不需要不停地重复计算，解出θ解出x再解出θ再解出x，实际上存在更有效率的算法可以将θ和同时计算出来，定义新的优化目标函数J，如下图所示：</p><p><img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/22.png" alt=""></p><p>这个代价函数将关于θ和x的两个代价函数合并起来，为了提出一个综合的优化目标问题，要做的是将这个代价函数视为特征x和用户参数θ的函数，对它的整体最小化：</p><p><img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/23.png" alt=""></p><p>所以，将上述所讲的结合起来，就得到了协同过滤算法：</p><ul><li><p>首先会把θ和x初始化为小的随机值</p></li><li><p>接下来用梯度下降或其他高级优化算法把代价函数Jcost 最小化</p></li><li><p>最后，如果用户具有一些参数θ，电影带有已知的特征x，就可以预测出该用户给这部电影的评分会是:</p></li></ul><p>这就是协同过滤算法，可以使用其同时计算出 theat和x</p><p><strong>矢量化：低秩矩阵分解</strong></p><p>这一节中将介绍协同过滤算法的向量化实现。</p><p>给定了数据如下</p><p><img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/25.png" alt=""></p><p>将图中的数据写入矩阵中，会得到一个5行4列的矩阵</p><p><img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/26.png" alt=""></p><p>矩阵中的Y(i,j)就是第j个用户给第i部电影的评分，用户j给第i部电影的评分的预测由公式给出，因此，预测评分的矩阵如下所示：</p><p><img src="/2020/03/14/Anomaly-detection-and-Recommender-Systems/27.png" alt=""></p><p>给定矩阵X和矩阵Θ的定义如下：</p><p>这个协同过滤算法有另一个名字叫做低秩矩阵分解。</p><p>接下来再介绍一个问题：利用已学到的属性来找到相关的电影。</p><p>假如有电影i，想要找到另一部与电影i相关的电影j，换个角度来说，如果用户正在看电影j，看完后推荐哪一部电影比较合理？</p><p>比如电影i有一个特征向量x，如果找到另一个电影特征向量xj， xi和 xj的距离很小，即 xi - xj 很小，那么很明显标明电影j和i相似，从这个意义上面来说，喜欢看电影j的人也很有可能喜欢看电影i</p><p>因此，希望通过本节的学习，能够知道如何用一个向量化的实现来计算所有用户对所有电影的评分预测值，也可以实现利用已学到的特征，找到彼此相类似的电影。</p><p>最后一个知识点 是均值归一化，前面有讲过，所以这个地方不再过多赘述了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
      <category term="机器学习" scheme="http://wsx1128.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="异常检测" scheme="http://wsx1128.cn/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
      <category term="推荐系统" scheme="http://wsx1128.cn/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>Kaggle Dog vs Cat</title>
    <link href="http://wsx1128.cn/2020/03/13/Kaggle-Dog-vs-Cat/"/>
    <id>http://wsx1128.cn/2020/03/13/Kaggle-Dog-vs-Cat/</id>
    <published>2020-03-13T13:13:53.000Z</published>
    <updated>2020-03-13T19:20:04.319Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>这是我接触的第一个Kaggle上面的比赛使用深度学习框架去完成，毕竟是一个图片识别的问题</p><p>原题目<a href="https://www.kaggle.com/c/dogs-vs-cats/overview" target="_blank" rel="noopener">在这里</a></p><p>The training archive contains 25,000 images of dogs and cats. Train your algorithm on these files and predict the labels for test1.zip (1 = dog, 0 = cat).</p><p>其实说白了，这道题也是比较简单的一道图片的二分类问题，最终的目的就是要输出这张图片究竟是狗狗还是猫猫</p><p>按照 步骤来做，以下的步骤基于<strong>深度学习框架 Pytorch</strong></p><h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><p>这里是直接去封装成一个dataset来读取操作图片</p><p>下面给出代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms <span class="keyword">as</span> T</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DogVsCat</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, root, transforms, train, test)</span>:</span></span><br><span class="line">        self.test = test</span><br><span class="line">        imgs = [os.path.join(root, img) <span class="keyword">for</span> img <span class="keyword">in</span> os.listdir(root)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.test:</span><br><span class="line">            <span class="comment"># ...../data/1001.jpg</span></span><br><span class="line">            imgs = sorted(imgs, key=<span class="keyword">lambda</span> x: int(x.split(<span class="string">'.'</span>)[<span class="number">-2</span>].split(<span class="string">'/'</span>)[<span class="number">-1</span>]))</span><br><span class="line">            self.imgs = imgs</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># ...../data/dog.1001.jpg</span></span><br><span class="line">            imgs = sorted(imgs, key=<span class="keyword">lambda</span> x: int(x.split(<span class="string">'.'</span>)[<span class="number">-2</span>]))</span><br><span class="line">        imgs_num = len(imgs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.test: <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">elif</span> train:</span><br><span class="line">            self.imgs = imgs[:int(<span class="number">0.7</span> * imgs_num)]</span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            self.imgs = imgs[int(<span class="number">0.7</span> * imgs_num):]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> transforms <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            normalize = T.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.test <span class="keyword">or</span> <span class="keyword">not</span> train:</span><br><span class="line">                self.transforms = T.Compose([</span><br><span class="line">                    T.Resize(<span class="number">224</span>),</span><br><span class="line">                    T.CenterCrop(<span class="number">224</span>),</span><br><span class="line">                    T.ToTensor(),</span><br><span class="line">                    normalize</span><br><span class="line">                ])</span><br><span class="line">            <span class="keyword">else</span> :</span><br><span class="line">                self.transforms = T.Compose([</span><br><span class="line">                    T.Resize(<span class="number">256</span>),</span><br><span class="line">                    T.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">                    T.RandomHorizontalFlip(),</span><br><span class="line">                    T.ToTensor(),</span><br><span class="line">                    normalize</span><br><span class="line">                ])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        img_path = self.imgs[index]</span><br><span class="line">        <span class="comment"># return the label if you get img</span></span><br><span class="line">        <span class="keyword">if</span> self.test:</span><br><span class="line">            <span class="comment"># ...../data/1001.jpg</span></span><br><span class="line">            label = int(img_path.split(<span class="string">'.'</span>)[<span class="number">-2</span>].split(<span class="string">'/'</span>)[<span class="number">-1</span>])</span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            <span class="comment"># ...../data/dog.1001.jpg</span></span><br><span class="line">            <span class="comment"># dog : 1 , cat : 0</span></span><br><span class="line">            label = <span class="number">1</span> <span class="keyword">if</span> <span class="string">'dog'</span> <span class="keyword">in</span> img_path <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        img = Image.open(img_path)</span><br><span class="line">        img = self.transforms(img)</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.imgs)</span><br></pre></td></tr></table></figure><p>需要注意的三个地方</p><ul><li><p>在getitem里面再去将图片保存在内存里，其他的时候保存图片的路径，并且进行排序</p></li><li><p>区分以下图片的测试集，与训练集，验证集，label返回的值，图片数量，以及图片优化都有所不同</p></li><li><p>最后一次写的时候竟然还是忘记在读取图片的过程中进行一个优化</p></li></ul><p>当处理完数据，并且封装了一个相应的数据集合之后，接下来的任务就是 建立卷积网络模型，这里应用的是包的深度学习网络，并且采取的预训练参数</p><p>当然，如果自己去调试一个模型的话，可能要花上一个月左右的时间，所以这里直接借用其他人的模型，来帮助自己进行调试相关的模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torchvision.models <span class="keyword">import</span> squeezenet1_1</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SqueezeNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes=<span class="number">2</span>)</span>:</span></span><br><span class="line">        super(SqueezeNet, self).__init__()</span><br><span class="line">        self.model_name = <span class="string">'squeezenet'</span></span><br><span class="line">        self.model = squeezenet1_1(pretrained=<span class="keyword">True</span>)</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">            <span class="comment"># 相当于全链接层</span></span><br><span class="line">            nn.Conv2d(<span class="number">512</span>, num_classes, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(<span class="keyword">True</span>),</span><br><span class="line">            nn.AvgPool2d(<span class="number">13</span>, stride=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.model(x)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_optim</span><span class="params">(self, lr, weight_decay)</span>:</span></span><br><span class="line">        <span class="comment"># 这里使用的是预训练模型，所以只有优化后面的参数即可</span></span><br><span class="line">        <span class="keyword">return</span> Adam(self.model.classifier.parameters(),lr=lr, weight_decay=weight_decay)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(self, path)</span>:</span></span><br><span class="line">        self.load_state_dict(t.load(path))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self, name=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> (name <span class="keyword">is</span> <span class="keyword">None</span>):</span><br><span class="line">            prefix = <span class="string">'checkpoints/'</span> + self.model_name + <span class="string">'_'</span></span><br><span class="line">            name = time.strftime(prefix + <span class="string">'%m%d_%H:%M:%S.pth'</span>)</span><br><span class="line">        t.save(self.state_dict(), name)</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">load</span><span class="params">(self, path)</span>:</span></span><br><span class="line">            self.load_state_dict(t.load(path))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span><span class="params">(self, name=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> (name <span class="keyword">is</span> <span class="keyword">None</span>):</span><br><span class="line">            prefix = <span class="string">'checkpoints/'</span> + self.model_name + <span class="string">'_'</span></span><br><span class="line">            name = time.strftime(prefix + <span class="string">'%m%d_%H:%M:%S.pth'</span>)</span><br><span class="line">        t.save(self.state_dict(), name)</span><br><span class="line">        <span class="keyword">return</span> name</span><br></pre></td></tr></table></figure><p>同样要记住两点</p><ul><li><p>记住在优化的过程中所有的 in_feature 与 out_feature 必须一一对应清楚</p></li><li><p>由于这里直接使用了预训练模型，所以就直接优化后面的模型就可以了</p></li></ul><p>接下来就可以开始着手写 配置函数和主函数了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> ipdb</span><br><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> fire</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">from</span> torchnet <span class="keyword">import</span> meter</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">from</span> dataset <span class="keyword">import</span> DogVsCat</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> SqueezeNet</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> Visualizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Config</span><span class="params">(object)</span>:</span></span><br><span class="line">    env = <span class="string">'Dog or Cat'</span></span><br><span class="line"></span><br><span class="line">    train_root = <span class="string">'./data/train'</span></span><br><span class="line">    test_root = <span class="string">'./data/test1'</span></span><br><span class="line">    load_model_path = <span class="keyword">None</span></span><br><span class="line">    use_gpu = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">    lr = <span class="number">0.001</span></span><br><span class="line">    lr_decay = <span class="number">0.5</span></span><br><span class="line">    weight_decay = <span class="number">0e-5</span></span><br><span class="line">    batch_size = <span class="number">32</span></span><br><span class="line">    num_works = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    max_epoch = <span class="number">10</span></span><br><span class="line">    plot_every = <span class="number">20</span></span><br><span class="line">    debug_path = <span class="string">'tmp/debug'</span></span><br><span class="line">    device = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">opt = Config()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">(**kwargs)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> kwargs.items():</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> hasattr(opt, k):</span><br><span class="line">            warnings.warn(<span class="string">"Warning: opt has not attribut %s"</span> % k)</span><br><span class="line">        setattr(opt, k, v)</span><br><span class="line">    model = SqueezeNet()</span><br><span class="line">    <span class="keyword">if</span> opt.load_model_path:</span><br><span class="line">        model.load(opt.load_model_path)</span><br><span class="line">    device = t.device(<span class="string">'cuda'</span>) <span class="keyword">if</span> opt.use_gpu <span class="keyword">else</span> t.device(<span class="string">'cpu'</span>)</span><br><span class="line">    model.to(opt.device)</span><br><span class="line"></span><br><span class="line">    test_data = DogVsCat(opt.test_root,train=<span class="keyword">False</span>,test=<span class="keyword">True</span>)</span><br><span class="line">    test_dataloader = DataLoader(test_data,batch_size=opt.batch_size,shuffle=<span class="keyword">False</span>,num_workers=opt.num_works)</span><br><span class="line">    results = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ii,(data, label) <span class="keyword">in</span> tqdm.tqdm(enumerate(test_dataloader)):</span><br><span class="line">        input = data.to(opt.device)</span><br><span class="line">        score = model(input)</span><br><span class="line">        probability = t.nn.functional.softmax(score,dim=<span class="number">1</span>)[:,<span class="number">0</span>].detach().tolist()</span><br><span class="line">        <span class="keyword">if</span> ii == <span class="number">0</span> :</span><br><span class="line">            print(score)</span><br><span class="line"></span><br><span class="line">        res = [(path_.item(), probability_) <span class="keyword">for</span> path_,probability_ <span class="keyword">in</span> zip(label, probability)]</span><br><span class="line">        results.append(res)</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'result.csv'</span>,<span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        writer = csv.writer(f)</span><br><span class="line">        writer.writerow([<span class="string">'id'</span>, <span class="string">'label'</span>])</span><br><span class="line">        writer.writerows(results)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(**kwargs)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> kwargs.items():</span><br><span class="line">        setattr(opt, k, v)</span><br><span class="line"></span><br><span class="line">    opt.device = t.device(<span class="string">'cuda'</span>) <span class="keyword">if</span> opt.use_gpu <span class="keyword">else</span> t.device(<span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line">    vis = Visualizer(env=opt.env)</span><br><span class="line">    train_dataset = DogVsCat(opt.train_root, train=<span class="keyword">True</span>, test=<span class="keyword">False</span>)</span><br><span class="line">    train_dataloader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=<span class="keyword">True</span>, num_workers=opt.num_works)</span><br><span class="line"></span><br><span class="line">    cv_dataset = DogVsCat(opt.train_root, train=<span class="keyword">False</span>, test=<span class="keyword">False</span>)</span><br><span class="line">    cv_dataloader = DataLoader(cv_dataset, batch_size=opt.batch_size, shuffle=<span class="keyword">False</span>, num_workers=opt.num_works)</span><br><span class="line"></span><br><span class="line">    model = SqueezeNet()</span><br><span class="line">    <span class="keyword">if</span> opt.load_model_path:</span><br><span class="line">        model.load(opt.load_model_path)</span><br><span class="line">    model.to(opt.device)</span><br><span class="line"></span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    lr = opt.lr</span><br><span class="line">    optim = model.get_optim(lr=lr, weight_decay=opt.weight_decay)</span><br><span class="line"></span><br><span class="line">    loss_meter = meter.AverageValueMeter()</span><br><span class="line">    confusion_matrix = meter.ConfusionMeter(<span class="number">2</span>)  <span class="comment"># 二分类问题，所以就使用这样的写法</span></span><br><span class="line">    previous_loss = <span class="number">1e10</span></span><br><span class="line">    <span class="comment"># 训练集操作</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(opt.max_epoch):</span><br><span class="line">        loss_meter.reset()</span><br><span class="line">        confusion_matrix.reset()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> ii, (data, label) <span class="keyword">in</span> tqdm.tqdm(enumerate(train_dataloader)):</span><br><span class="line">            input = data.to(opt.device)</span><br><span class="line">            target = label.to(opt.device)</span><br><span class="line">            output = model(input)</span><br><span class="line">            optim.zero_grad()</span><br><span class="line">            loss = criterion(output, target)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optim.step()</span><br><span class="line"></span><br><span class="line">            loss_meter.add(loss.item())</span><br><span class="line">            confusion_matrix.add(output.detach(), target.detach())</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (ii + <span class="number">1</span>) % opt.plot_every == <span class="number">0</span>:</span><br><span class="line">                vis.plot(<span class="string">'loss'</span>, loss_meter.value()[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> os.path.exists(opt.debug_path):</span><br><span class="line">                    ipdb.set_trace()</span><br><span class="line"></span><br><span class="line">        model.save()</span><br><span class="line">        <span class="comment"># 验证集开始验证</span></span><br><span class="line">        cv_val, cv_accuracy = val(model, cv_dataloader)</span><br><span class="line">        vis.plot(<span class="string">'val_accuracy'</span>, cv_accuracy)</span><br><span class="line">        vis.log(<span class="string">"epoch:&#123;epoch&#125;,lr:&#123;lr&#125;,loss:&#123;loss&#125;,train_cm:&#123;train_cm&#125;,val_cm:&#123;val_cm&#125;"</span>.format(</span><br><span class="line">            epoch=epoch, loss=loss_meter.value()[<span class="number">0</span>], val_cm=str(cv_val), train_cm=str(confusion_matrix.value()),</span><br><span class="line">            lr=opt.lr))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> loss_meter.value()[<span class="number">0</span>] &gt; previous_loss:</span><br><span class="line">            lr = lr * opt.lr_decay</span><br><span class="line">            <span class="keyword">for</span> param_group <span class="keyword">in</span> optim.param_groups:</span><br><span class="line">                param_group[<span class="string">'lr'</span>] = lr</span><br><span class="line">        previous_loss = loss_meter.value()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">val</span><span class="params">(model, cv_dataloader)</span>:</span></span><br><span class="line">    model.eval()</span><br><span class="line">    confusion_matrix = meter.ConfusionMeter(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span> ii, (data, label) <span class="keyword">in</span> tqdm.tqdm(enumerate(cv_dataloader)):</span><br><span class="line">        input = data.to(opt.device)</span><br><span class="line">        target = label.to(opt.device)</span><br><span class="line">        output = model(input)</span><br><span class="line">        confusion_matrix.add(output.detach().squeeze(), target.type(t.LongTensor))</span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line">    cv_val = confusion_matrix.value()</span><br><span class="line">    cv_accuracy = <span class="number">100</span> * (cv_val[<span class="number">0</span>][<span class="number">0</span>] + cv_val[<span class="number">1</span>][<span class="number">1</span>] / cv_val.sum())</span><br><span class="line">    <span class="keyword">return</span> cv_val, cv_accuracy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    fire.Fire()</span><br></pre></td></tr></table></figure><p>总体来说代码还是比较简单的，由于对py的语法不是特别熟悉，所以需要注意五个地方</p><ul><li><p>注意**kwargs 需要将其的item拿出来赋值，要不然就必须全部赋值</p></li><li><p>这里混淆矩阵有办法帮着看而分类问题</p></li><li><p>时时刻刻都要注意代码运行的地方到底实在cpu还是在gpu</p></li><li><p>记住对特征值的处理才能够得到答案</p></li><li><p>也是最重要的一点，上面模型生成结果之后，需要去应用softmax进行一个二分类问题</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
      <category term="机器学习" scheme="http://wsx1128.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://wsx1128.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Pytorch" scheme="http://wsx1128.cn/tags/Pytorch/"/>
    
      <category term="卷积网络" scheme="http://wsx1128.cn/tags/%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Clustering and Dimensionality Reduction</title>
    <link href="http://wsx1128.cn/2020/03/13/Clustering-and-Dimensionality-Reduction/"/>
    <id>http://wsx1128.cn/2020/03/13/Clustering-and-Dimensionality-Reduction/</id>
    <published>2020-03-13T08:57:52.000Z</published>
    <updated>2020-03-13T10:05:44.004Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h2><p>终于步入到了，非监督学习，前面所涉及到的课程，全部都是监督学习。</p><p>所谓监督学习与非监督学习之间的区别，最简单的区别就是前者有标签值，后者没有标签值。</p><p>就像下面两副图片<br><img src="/2020/03/13/Clustering-and-Dimensionality-Reduction/1.png" alt=""></p><p><img src="/2020/03/13/Clustering-and-Dimensionality-Reduction/2.png" alt=""></p><p>二者的区别就一目了然</p><p>课上，就直接将这群没有标签值的数据 叫做 cluster。 意思是每一个数据都是没有差别，其举的例子，在应用上有 市场分管，社交网络，天文学，还有很多。</p><p>但是，每一个数据本身的特征会使有相近特征的人互相靠近，举个例子，社交网络上，你加的好友，大多数是你的亲人，你的现实生活中的朋友，这些都是你现实中的社交网络</p><p>那么，如何将给定的数据进行一个有效的分类呢？</p><p>这个时候就引出了 K-means算法</p><p>通过给出的一大堆数据点，随机选取标签点，然后根据与标签的相对距离，来完成一个近似的分类过程</p><p>这里给出三张图片来完成这样一个过程：</p><p><img src="/2020/03/13/Clustering-and-Dimensionality-Reduction/3.png" alt=""></p><p><img src="/2020/03/13/Clustering-and-Dimensionality-Reduction/4.png" alt=""></p><p><img src="/2020/03/13/Clustering-and-Dimensionality-Reduction/5.png" alt=""></p><p><img src="/2020/03/13/Clustering-and-Dimensionality-Reduction/6.png" alt=""></p><p>在这个算法里，输入的是 K个族群分布标签，和训练集</p><p>下面就是这个算法的全过程了</p><p><img src="/2020/03/13/Clustering-and-Dimensionality-Reduction/7.png" alt=""></p><p>翻译成中文的意思</p><ol><li><p>随机选取K个标签数据</p></li><li><p>然后再去遍历每一个数据，计算与哪一个标签值是最近的，并且将其归类</p></li><li><p>归类结束之后，计算每一群的均值，并且试图将其最小化</p></li></ol><p>一直重复2，3，一直到mean值最小为止</p><p>其的应用就主要体现在了 衣服的选择上</p><p>人类去选择衣服，根据身高体重划分衣服为 S，M，L码</p><h3 id="K-mean的优化方式"><a href="#K-mean的优化方式" class="headerlink" title="K-mean的优化方式"></a>K-mean的优化方式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Randomly initialize K cluster centroids mu(<span class="number">1</span>), mu(<span class="number">2</span>), ..., mu(K)</span><br><span class="line">Repeat:</span><br><span class="line">   <span class="keyword">for</span> i = <span class="number">1</span> to m:</span><br><span class="line">      c(i):= index (<span class="keyword">from</span> <span class="number">1</span> to K) of cluster centroid closest to x(i)</span><br><span class="line">   <span class="keyword">for</span> k = <span class="number">1</span> to K:</span><br><span class="line">      mu(k):= average (mean) of points assigned to cluster k</span><br></pre></td></tr></table></figure><p><img src="/2020/03/13/Clustering-and-Dimensionality-Reduction/8.png" alt=""></p><p>算法分为两个步骤，第一个for循环是赋值步骤，即：对于每一个样例i，计算其应该属于的类。第二个for循环是聚类中心的移动，即：对于每一个类 K重新计算该类的质心。<br>在K-means算法中的变量在上面已经被标明。</p><p>回顾刚才给出的，k-mean均值迭代算法，我们知道，第一个循环是用于减小 ${c}^{(i)}$ 引起的代价，而第二个循环则是用于减小  引起代价。迭代的过程一定会减小代价函数，要不然就是出现了错误。</p><h3 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h3><p>在运行K-均值算法的之前，我们首先要随机初始化所有的聚类中心点，下面介绍怎样做：</p><ul><li><p>我们应该选择 K &lt; m，即聚类中心点的个数要小于所有训练集实例的数量</p></li><li><p>随机选择 K 个训练实例。</p></li><li><p>令 K 个聚类中心分别与这 K个训练实例相等</p></li></ul><p>K-均值的一个问题在于它有可能会停留在<strong>一个局部最小值处</strong>，而这取决于初始化的情况。为了解决这个问题，我们通常需要多次运行K-均值算法，每一次都重新进行随机初始化，最后再比较多次运行K-均值的结果，选择代价函数最小的结果。这种方法在K较小的时候（2—10）还是可行的，但是K如果较大，这么做也可能不会有明显地改善。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i = <span class="number">1</span> to <span class="number">100</span>:</span><br><span class="line">   randomly initialize k-means</span><br><span class="line">   run k-means to get <span class="string">'c'</span> <span class="keyword">and</span> <span class="string">'m'</span></span><br><span class="line">   compute the cost function (distortion) J(c,m)</span><br><span class="line">pick the clustering that gave us the lowest cost</span><br></pre></td></tr></table></figure><h3 id="选择聚类数"><a href="#选择聚类数" class="headerlink" title="选择聚类数"></a>选择聚类数</h3><p>K的选择往往是任意的或者说是模糊不清的，通常是需要根据不同的问题人工进行选择的。选择的时候我们需要思考运用K-均值算法聚类的动机是什么，然后选择能最好服务于该目的标聚类数。</p><p>一般都是什么问题，就需要去怎么考虑的选择聚类数</p><p>当人们在讨论，选择聚类数目的方法时，有一个可能会谈及的方法叫作“肘部法则”。将代价J和聚类K画在图中。代价函数会随着K的增加而降低然后趋于平缓，我们要做的就是找到J开始趋于平缓时的K。然而很多时候，曲线经常是很平缓的，这时的肘部就很不明显。（note：J一般都是随着K的增加而降低，但如果K出现了错误的局部最优则会导致不一样的结果）。</p><p>这个地方需要去注意的是： 等到cost函数值稳定了之后再记入相关的函数中：</p><p><img src="/2020/03/13/Clustering-and-Dimensionality-Reduction/9.png" alt=""></p><h2 id="Dimensionality-Reduction"><a href="#Dimensionality-Reduction" class="headerlink" title="Dimensionality Reduction"></a>Dimensionality Reduction</h2><h3 id="使用降维的动机"><a href="#使用降维的动机" class="headerlink" title="使用降维的动机"></a>使用降维的动机</h3><p>有几个不同的的原因使你可能想要做降维。一是数据压缩，如果我们有大量多余的数据时，我们可能想降低特征的维度，为此可以找到两个高度相关的特征，将其画出图像然后做一条直线来同时描述这两个特征。二是数据可视化，因为数据超过三维就很难可视化了，因此有时候需要将维度下降到3或者以下来达到可视化的目的。</p><h3 id="主成分分析问题"><a href="#主成分分析问题" class="headerlink" title="主成分分析问题"></a>主成分分析问题</h3><p>主成分分析(PCA)是最常见的降维算法。其定义是想把数据从n维降到k维（k小于n），就在这个空间里面找k个单位向量来表示数据，使得数据点投影到这个面上的误差最小。如下例子：2到1和3到2：</p><p><img src="/2020/03/13/Clustering-and-Dimensionality-Reduction/10.png" alt=""></p><p>在二维变成一维的时候</p><p>虽然同是一条直线拟合，但PCA和线性回归是不同的：</p><ul><li><p>计算loss的方式有所不同</p></li><li><p>PCA没有标签Y（非监督）</p></li></ul><h3 id="PCA算法"><a href="#PCA算法" class="headerlink" title="PCA算法"></a>PCA算法</h3><p>PCA 减少 n 维到 k 维：</p><ol><li><p>均值归一化。我们需要计算出所有特征的均值，然后令x_j=x_j-μ_j。如果特征是在不同的数量级上，我们还需要将其除以标准差$σ^2$。</p></li><li><p>计算协方差矩阵（covariance matrix）$Σ ：\sum=\frac {1}{m}\sum^{n}_{i=1}\left( x^{(i)}\right) \left( x^{(i)}\right) ^{T}$</p></li><li><p>计算协方差矩阵Σ的特征向量（eigenvectors）<br>在 Octave 里我们可以利用奇异值分解（singular value decomposition）来求解，[U, S, V]= svd(sigma)。<br>$\Sigma=\frac {1}{m}\sum^{n}_{i=1}\left( x^{(i)}\right) \left( x^{(i)}\right) ^{T}$</p></li></ol><p>对于一个 n×n 维度的矩阵，上式中的 U 是一个具有与数据之间最小投射误差的方向向量构成的矩阵。如果我们希望将数据从 n 维降至 k 维，我们只需要从U中选取前k个向量，获得一个n×k维度的矩阵，我们$U_{reduce}$用表示，然后通过如下计算获得要求的新特征向量</p><p>$z^{(i)}=U^{T}_{reduce}*x^{(i)}$</p><p>其中x是n×1维的，因此结果为 k×1 维度。我们不对方差特征进行处理。</p><p><img src="/2020/03/13/Clustering-and-Dimensionality-Reduction/11.png" alt=""></p><h3 id="选择主成分的数量"><a href="#选择主成分的数量" class="headerlink" title="选择主成分的数量"></a>选择主成分的数量</h3><p>在PCA算法中我们把n维特征变量降维到k维特征变量。这个数字k也被称作主成分的数量或者说是我们保留的主成分的数量。我们先来思考两个值：</p><ul><li><p>PCA所做的是尽量最小化平均平方映射误差(Average Squared Projection Error)。</p></li><li><p>我还要定义一下数据的总变差(Total Variation)。它的意思是 “平均来看我的训练样本距离零向量多远？</p></li></ul><p>我们把两个数的比值作为衡量PCA算法的有效性</p><p>定义一个阈值然后实验k，看看那个最小的k合适。计算步骤如</p><p><img src="/2020/03/13/Clustering-and-Dimensionality-Reduction/12.png" alt=""></p><p>这里有个技巧：svd函数会返回一个对角矩阵S，他的元素可以很快的计算这个阈值。</p><h3 id="主成分分析法的应用建议"><a href="#主成分分析法的应用建议" class="headerlink" title="主成分分析法的应用建议"></a>主成分分析法的应用建议</h3><p>PCA算法主要有以下用途：</p><ul><li><p>压缩：</p><ul><li><p>减少内存和磁盘的占用</p></li><li><p>提升算法的速度</p></li></ul></li><li><p>可视化：</p><ul><li>降维到二维或者三维</li></ul></li></ul><p>有些人觉的PCA也可以用来防止过拟合，但是这是不对的。<strong>应该用正则化</strong>。正则化使用y标签最小化损失函数，使用了y标签信息。而PCA只单纯的看x的分部就删除了一些特征，损失率很多信息。另一个常见的错误是，默认地将主要成分分析作为学习过程中的一部分，这虽然很多时候有效果，最好还是从所有原始特征开始，只在有必要的时候（<strong>算法运行太慢或者占用太多内存</strong>）才考虑采用主要成分分析。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
      <category term="机器学习" scheme="http://wsx1128.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://wsx1128.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="非监督学习" scheme="http://wsx1128.cn/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="维度下降" scheme="http://wsx1128.cn/tags/%E7%BB%B4%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    
  </entry>
  
  <entry>
    <title>Support Vector Machines</title>
    <link href="http://wsx1128.cn/2020/03/13/Support-Vector-Machines/"/>
    <id>http://wsx1128.cn/2020/03/13/Support-Vector-Machines/</id>
    <published>2020-03-13T05:56:38.000Z</published>
    <updated>2020-03-13T08:53:54.825Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>首先，支持向量机的由来（在吴恩达机器学习的课程中，是从逻辑回归课延伸过来的）</p><p>其实 支持向量机就是一个二分类模型，目的就是为了寻找一个超平面对样本进行一个线性分割，分割的员额是间隔最大化， 从简单到困难的模型包括：</p><ul><li><p>当训练样本线性可分时，通过硬间隔最大化，学习一个线性可分支持向量机</p></li><li><p>当训练样本近似线性可分时，通过软间隔最大化，学习一个线性支持向量机</p></li><li><p>当训练样本线性不可分时，通过核技巧和软间隔最大化，学习一个非线性支持向量机</p></li></ul><h3 id="初识向量机"><a href="#初识向量机" class="headerlink" title="初识向量机"></a>初识向量机</h3><p><img src="/2020/03/13/Support-Vector-Machines/1.png" alt=""></p><p>首先从这个函数y一点点的进行修改，取这里的z=1点，先画出将要用的代价函数，新的代价函数将会水平的从这里到右边，然后再画一条同逻辑回归非常相似的直线，但是在这里是一条直线，也就是 用紫红色画的曲线。那么到了这里已经非常接近逻辑回归中使用的代价函数了，只是这里是由两条线段组成 即位于右边的水平部分和位于左边的直线部分。先别过多的考虑左边直线部分的斜率，这并不是很重要，但是 这里我们将使用的新的代价函数是在y=1的前提下的。你也许能想到这应该能做同逻辑回归中类似的事情但事实上在之后的的优化问题中这会变得更坚定，并且为支持向量机带来计算上的优势。</p><p>通过对边界的划分，引出支持向量机</p><p>其实我刚刚开始的理解，对于支持向量机的理解也并不是特别到位。</p><p>只知道支持向量机 更改以往机器学习处理数据的方式，支持向量机不太在乎数据的维度、</p><p>所谓机器学习的本质就是将问题真实模型无限逼近，这个与问题真实解之间的误差，就叫做风险（更严格的说，误差的累积叫做风险）。我们选择了一个假设之后（更直观点说，我们得到了一个分类器以后），真实误差无从得知，但我们可以用某些可以掌握的量来逼近它。最直观的想法就是使用分类器在样本数据上的分类的结果与真实结果（因为样本是已经标注过的数据，是准确的数据）之间的差值来表示。这个差值叫做经验风险Remp(w)。以前的机器学习方法都把经验风险最小化作为努力的目标，但后来发现很多分类函数能够在样本集上轻易达到100%的正确率，在真实分类时却一塌糊涂（即所谓的推广能力差，或泛化能力差）。此时的情况便是选择了一个足够复杂的分类函数（它的VC维很高），能够精确的记住每一个样本，但对样本之外的数据一律分类错误。回头看看经验风险最小化原则我们就会发现，此原则适用的大前提是经验风险要确实能够逼近真实风险才行（行话叫一致），但实际上能逼近么？答案是不能，因为样本数相对于现实世界要分类的文本数来说简直九牛一毛，经验风险最小化原则只在这占很小比例的样本上做到没有误差，当然不能保证在更大比例的真实文本上也没有误差。</p><p>这里引入了一个概念：</p><p>统计学习因此而引入了<strong>泛化误差界</strong>的概念，就是指真实风险应该由两部分内容刻画，一是经验风险，代表了分类器在给定样本上的误差；二是置信风险，代表了我们在多大程度上可以信任分类器在未知文本上分类的结果。很显然，第二部分是没有办法精确计算的，因此只能给出一个估计的区间，也使得整个误差只能计算上界，而无法计算准确的值（所以叫做泛化误差界，而不叫泛化误差）</p><p>所谓VC维是对函数类的一种度量，可以简单的理解为问题的复杂程度，VC维越高，一个问题就越复杂。正是因为SVM关注的是VC维，后面我们可以看到，SVM解决问题的时候，和样本的维数是无关的（甚至样本是上万维的都可以，这使得SVM很适合用来解决文本分类的问题，当然，有这样的能力也因为引入了核函数）。</p><p>置信风险与两个量有关，一是样本数量，显然给定的样本数量越大，我们的学习结果越有可能正确，此时置信风险越小；二是分类函数的VC维，显然VC维越大，推广能力越差，置信风险会变大。</p><p>泛化误差界的公式为：</p><p><img src="/2020/03/13/Support-Vector-Machines/2.png" alt=""></p><p>SVM正是这样一种努力最小化结构风险的算法。</p><p>SVM其他的特点就比较容易理解了。</p><ul><li><p>小样本，并不是说样本的绝对数量少（实际上，对任何算法来说，更多的样本几乎总是能带来更好的效果），而是说与问题的复杂度比起来，SVM算法要求的样本数是相对比较少的。</p></li><li><p>非线性，是指SVM擅长应付样本数据线性不可分的情况，主要通过松弛变量（也有人叫惩罚变量）和核函数技术来实现，这一部分是SVM的精髓，以后会详细讨论。多说一句，关于文本分类这个问题究竟是不是线性可分的，尚没有定论，因此不能简单的认为它是线性可分的而作简化处理，在水落石出之前，只好先当它是线性不可分的（反正线性可分也不过是线性不可分的一种特例而已，我们向来不怕方法过于通用）。</p></li><li><p>高维模式识别是指样本维数很高，例如文本的向量表示，如果没有经过另一系列文章（《文本分类入门》）中提到过的降维处理，出现几万维的情况很正常，其他算法基本就没有能力应付了，SVM却可以，主要是因为SVM 产生的分类器很简洁，用到的样本信息很少（仅仅用到那些称之为“支持向量”的样本，此为后话），使得即使样本维数很高，也不会给存储和计算带来大麻烦（相对照而言，kNN算法在分类时就要用到所有样本，样本数巨大，每个样本维数再一高，这日子就没法过了……）。</p></li></ul><p>其实在文本数据的处理上，目前来最好的使用方法就是深度学习中的RNN和词向量的结合</p><h3 id="直观上对大间隔的理解"><a href="#直观上对大间隔的理解" class="headerlink" title="直观上对大间隔的理解"></a>直观上对大间隔的理解</h3><p>下图中的公式表示了支持向量机模型的代价函数，左边的图中，画出了z的代价函数，此函数适用于正样本，右边画的z的代价函数，适用于负样本。</p><p><img src="/2020/03/13/Support-Vector-Machines/3.png" alt=""></p><p><img src="/2020/03/13/Support-Vector-Machines/4.png" alt=""></p><p>上面介绍了间隔的概念。</p><p>具体的例子，可以直接看下面这样一个数据集：</p><p><img src="/2020/03/13/Support-Vector-Machines/5.png" alt=""></p><p>其中有正样本也有负样本，可以看到这个数据是线性可分的，即存在一条直线把正负样本分开，且这里存在有多条不同的直线可以把正负样本分开。支持向量机会选择图中黑色的这条线来作为决策边界，相对于其他的线来看要更合理些。从数学上来讲，这条黑线和训练样本之间有着更大的最短距离，这个距离被称作支持向量机的间距，这是支持向量机具有鲁棒性的原因，因为它努力用一个最大间距来分离样本。因此SVM经常被称作是一种大间距分类器（Large Margin Intuition）。</p><p>在实际的使用中，我们可能会出现一些异常点数据，比如下图：</p><p><img src="/2020/03/13/Support-Vector-Machines/6.png" alt=""></p><p>由于异常点的存在，决策边界可能就变成了图中倾斜的那条线，这显然是不明智的。因此，如果C设置的非常大，这也是支持向量机将会做的，它从图中较为竖直的线变为了倾斜的那条，但如果C的值设置的小一点，最终将会得到图中较为竖直的线。当然，如果数据不是线性可分的，支持向量机也可以将它们分开。因此大间距分类器仅仅是从直观上给出了正则化参数C非常大的情形。同时要提醒你C的作用类似于 1 / lambda, lambda 是我们之前使用过的正则化参数，这只是C非常大的情形或者等价 lambda 非常小的情形。实际上当C不是非常大时，支持向量机可以忽略掉一些异常点的影响，得到更好的决策边界，甚至当数据不是线性可分的时候也能给出很好的结果。</p><h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>假设有个数据集如下图所示，我们希望拟合一个非线性的决策边界来区分正负样本，一种方法是构造多项式特征变量如图中所示：</p><p><img src="/2020/03/13/Support-Vector-Machines/7.png" alt=""></p><p>这就是一种新的函数了，直接嵌合在支持向量机内部函数里面。作为相应的分类依据，计算新的特征变量</p><p>当然，这个时候再次考虑到一个问题，有没有比这些高阶项更好的特征变量呢？</p><p><img src="/2020/03/13/Support-Vector-Machines/8.png" alt=""></p><p>就是相当于力了几根柱子，与后面的k-mean有点相像，并且类似。</p><p><img src="/2020/03/13/Support-Vector-Machines/9.png" alt=""></p><p><img src="/2020/03/13/Support-Vector-Machines/10.png" alt=""></p><p>通过上图可以看出，sigmoid越大收敛越慢，反之收敛越快。</p><p>那么现在有个问题，上述的标记点是随机取得，这很显然没那么简单，有一种较好的方法，是将训练集中的正样本作为标记点，对于每一个训练集中的数据，我们都有一个m+1维向量与之对应。 这里的fo默认为1。如下图所示：</p><p><img src="/2020/03/13/Support-Vector-Machines/11.png" alt=""></p><p><img src="/2020/03/13/Support-Vector-Machines/12.png" alt=""></p><p>下面是支持向量机的两个参数 C 和 sigmoid 的影响(C可以看做)：<br>C较大时，相当于sigmoid较小，可能会导致过拟合，高方差；<br>C较小时，相当于sigmoid较大，可能会导致低拟合，高偏差；<br>sigmoid 较大时，可能会导致低方差，高偏差；<br>sigmoid 较小时，可能会导致低偏差，高方差。</p><h3 id="实践SVM"><a href="#实践SVM" class="headerlink" title="实践SVM"></a>实践SVM</h3><p>在实际工作中，我们往往使用已有的工具包中所包含的SVM。在使用时，我们需要注意其中的参数选定和选择想要使用的核函数。其中一个选择是不需要使用任何内核参数，这也叫作线性核函数。因此如果有人说使用了线性核函数的SVM，这就意味着使用了不带有核函数的SVM。<br>从逻辑回归模型，我们得到了支持向量机模型，在两者之间，我们应该如何选择呢？</p><ul><li>如果n很大，接近m，那么使用Logistic回归或者线性SVM；</li><li>如果n很小，m大小适中，使用高斯核函数；</li><li>如果n很小，m很大，则可以创建新的特征然后使用logistic回归或者线性SVM</li><li>神经网络在上面几种情况下都可能有较好的表现，但训练神经网络非常慢。</li></ul><p>这一章 需要在今后有大量大块大块时间的时候去弄清楚 背后的数学逻辑</p><p>当然在sklearn这个包里面就已经内置好了相应的库了。</p><p>目前的要求，就是会去使用就行了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
      <category term="机器学习" scheme="http://wsx1128.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://wsx1128.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="支持向量机" scheme="http://wsx1128.cn/tags/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络</title>
    <link href="http://wsx1128.cn/2020/03/12/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://wsx1128.cn/2020/03/12/卷积神经网络/</id>
    <published>2020-03-12T15:52:01.000Z</published>
    <updated>2020-03-12T19:17:23.428Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>首先声明 一下博客内容，全部来自于吴恩达 deep learning 中的内容，这是第四门课的，卷积神经网络的总结。</p><p>另外再次声明本篇所用到的所有图片是来自各大博客，由于latex公式还不是特别熟悉，所以遇到数学公式多的知识点，就直接截图别人的总结了。</p><p>前面目前遇到的所有机器学习问题，只是涉及到数据，数字上面的变化，但是真实世界中不仅有一维的数字，还有高维度图片，甚至是语音，以及视频。</p><p>这个就是后面所研究的最重要的地方了。</p><h2 id="卷积神经网络的基础"><a href="#卷积神经网络的基础" class="headerlink" title="卷积神经网络的基础"></a>卷积神经网络的基础</h2><h3 id="计算机视觉"><a href="#计算机视觉" class="headerlink" title="计算机视觉"></a>计算机视觉</h3><p>计算机视觉（Computer Vision）包含很多不同类别的问题，如图片分类、目标检测、图片风格迁移等等。</p><p><img src="/2020/03/12/卷积神经网络/1.jpg" alt=""></p><p>对于小尺寸的图片问题，也许我们用深度神经网络的结构可以较为简单的解决一定的问题。但是当应用在大尺寸的图片上，输入规模将变得十分庞大，使用神经网络将会有非常多的参数需要去学习，这个时候神经网络就不再适用。</p><p>卷积神经网络在计算机视觉问题上是一个非常好的网络结构。</p><h3 id="边缘检测实例"><a href="#边缘检测实例" class="headerlink" title="边缘检测实例"></a>边缘检测实例</h3><p>卷积运算是卷积神经网络的基本组成部分。下面以边缘检测的例子来介绍卷积运算。</p><p>所谓边缘检测，在下面的图中，分别通过垂直边缘检测和水平边缘检测得到不同的结果：</p><p><img src="/2020/03/12/卷积神经网络/2.jpg" alt=""></p><h4 id="垂直边缘检测"><a href="#垂直边缘检测" class="headerlink" title="垂直边缘检测"></a>垂直边缘检测</h4><p>假设对于一个 6 <em> 6 大小的图片（以数字表示），以及一个3 </em> 3大小的 filter（卷积核） 进行卷积运算，以“*” 符号表示。图片和垂直边缘检测器分别如左和中矩阵所示：</p><p>下面就相当于一个最简单最基本的卷积网络的使用</p><p><img src="/2020/03/12/卷积神经网络/3.jpg" alt=""></p><p>filter 不断地和其大小相同的部分做对应元素的乘法运算并求和，最终得到的数字相当于新图片的一个像素值，如右矩阵所示，最终得到一个4*4大小的图片。</p><p>说实话就是将图片中的像素点，进行一系列的数学变换，从而保留出相应的图片像素点组合，当然，其中有很多权重需要去学习。</p><h4 id="边缘检测原理"><a href="#边缘检测原理" class="headerlink" title="边缘检测原理"></a>边缘检测原理</h4><p>以一个有一条垂直边缘线的简单图片来说明。通过垂直边缘 filter 我们得到的最终结果图片可以明显地将边缘和非边缘区分出来：</p><p><img src="/2020/03/12/卷积神经网络/4.jpg" alt=""></p><p>当然，有更多样式的卷积核，这种卷积核就是弄好的，在卷积神经网络中，需要自己去训练其中的参数</p><p><img src="/2020/03/12/卷积神经网络/5.jpg" alt=""></p><p>对于复杂的图片，我们可以直接将 filter 中的数字直接看作是需要学习的参数，其可以学习到对于图片检测相比上面filter更好的更复杂的 filter ，如相对于水平和垂直检测器，我们训练的 filter 参数也许可以知道不同角度的边缘。</p><p>通过卷积运算，在卷积神经网络中通过反向传播算法，可以学习到相应于目标结果的 filter，将其应用于整个图片，输出其提取到的所有有用的特征。</p><h3 id="卷积和互相关"><a href="#卷积和互相关" class="headerlink" title="卷积和互相关"></a>卷积和互相关</h3><p>在数学定义上，矩阵的卷积操作为首先将卷积核同时在水平核垂直方向上进行翻转，构成一个卷积核的镜像，然后使用该镜像在和前面的矩阵进行移动核相乘求和操作，如下面例子所示：</p><p><img src="/2020/03/12/卷积神经网络/6.jpg" alt=""></p><p>在深度学习中，我们称为的卷积运算实则没有卷积核变换为镜像的这一步操作，因为在权重学习的角度，变换是没有必要的。深度学习的卷积操作在数学上准确度来说称为互相关（cross-correlation）。</p><h3 id="padding"><a href="#padding" class="headerlink" title="padding"></a>padding</h3><p>没有Padding的缺点：</p><ul><li><p>每次卷积操作，图片就会缩小</p></li><li><p>角落和边缘位置的像素可能进行的卷积运算次数少，可能会丢失有用的信息。</p></li></ul><p>如果加padding</p><p>为了解决上面的两个缺点，我们在进行卷积运算前为图片加padding，包围角落和边缘的像素，使得通过filter的卷积运算后，图片大小不变，也不会丢失角落和边沿的信息。</p><p><img src="/2020/03/12/卷积神经网络/7.png" alt=""></p><p>Valid/Same卷积</p><p>前者是没有padding操作，后者就是加入padding后，正好输出图片与输入图片相同。</p><p>在计算机视觉中，一般来说padding的值为奇数</p><h3 id="卷积步长（stride）"><a href="#卷积步长（stride）" class="headerlink" title="卷积步长（stride）"></a>卷积步长（stride）</h3><p>卷积的步长是构建卷积神经网络的一个基本的操作。</p><p>如前面的例子中，我们使用的 stride=1，每次的卷积运算以1个步长进行移动。下面是 stride=2 时对图片进行卷积的结果：</p><p><img src="/2020/03/12/卷积神经网络/9.png" alt=""></p><p><strong>当然，这只是一维的情况，正常的彩色图片，应该是三个维度的</strong></p><h3 id="立体卷积和卷积网络"><a href="#立体卷积和卷积网络" class="headerlink" title="立体卷积和卷积网络"></a>立体卷积和卷积网络</h3><p>卷积核的通道数，要对于之前的输入的通道数，而卷积核的个数，最后对应输出的通道数</p><p>这里需要注意的是 单卷积核与多卷积核的区别，不过多赘述了</p><p>其实主要就是记住公式，然后将卷积操作看清楚就行了，正常的深度学习框架都已经给你封装好了操作</p><p><img src="/2020/03/12/卷积神经网络/10.png" alt=""></p><p>下面直接借用一张简单的卷积图来完成对其总结</p><p><img src="/2020/03/12/卷积神经网络/11.jpg" alt=""></p><h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><p>池化层就分为最大池化层，和平均池化层</p><p><img src="/2020/03/12/卷积神经网络/12.jpg" alt=""></p><p>这里的公式与上面卷积层基本一样</p><p>不过与上面最大的不同就是 池化层里面没有需要学习的参数</p><p>下面直接展示两张图片，来介绍一下卷积神经网络，并且讲解一下，为什么会使用卷积神经网络</p><p><img src="/2020/03/12/卷积神经网络/13.jpg" alt=""></p><p>LeNet-5的参数如下：<br><img src="/2020/03/12/卷积神经网络/14.jpg" alt=""></p><h3 id="使用卷积神经网络的优势"><a href="#使用卷积神经网络的优势" class="headerlink" title="使用卷积神经网络的优势"></a>使用卷积神经网络的优势</h3><ul><li><p>参数少，跟同期的全链接层相比，参数是大大的减少，意思就是可以大大的增加卷积神经网络</p></li><li><p>参数共享：一个特征检测器（filter）对图片的一部分有用的同时也有可能对图片的另外一部分有用。</p></li><li><p>连接的稀疏性：在每一层中，每个输出值只取决于少量的输入。</p></li></ul><p>我们将训练集输入到卷积神经网络中，对网络进行训练。利用梯度下降（Adam、momentum等优化算法）最小化代价函数来寻找网络的最优参</p><h2 id="深度卷积模型介绍"><a href="#深度卷积模型介绍" class="headerlink" title="深度卷积模型介绍"></a>深度卷积模型介绍</h2><h3 id="经典的卷积网络"><a href="#经典的卷积网络" class="headerlink" title="经典的卷积网络"></a>经典的卷积网络</h3><p>下面介绍几种经典的卷积网络，分别是LeNet, AlexNet, VGGNet</p><h4 id="LeNet-5-针对灰度图片"><a href="#LeNet-5-针对灰度图片" class="headerlink" title="LeNet-5:针对灰度图片"></a>LeNet-5:针对灰度图片</h4><p>LeNet-5主要是针对灰度设计的，所以其输入较小，为32 <em> 32 </em> 1，其结构如下：</p><p><img src="/2020/03/12/卷积神经网络/15.jpg" alt=""></p><p>在LetNet中，存在的经典模式：</p><ul><li><p>随着网络的深度增加，图像的大小在缩小，与此同时，通道的数量却在增加；</p></li><li><p>每个卷积层后面接一个池化层。</p></li></ul><h4 id="AlexNet-一般用于彩色大图片"><a href="#AlexNet-一般用于彩色大图片" class="headerlink" title="AlexNet 一般用于彩色大图片"></a>AlexNet 一般用于彩色大图片</h4><p><img src="/2020/03/12/卷积神经网络/16.jpg" alt=""></p><ul><li>与LeNet相似，但网络结构更大，参数更多，表现更加出色；</li><li>使用了Relu；</li><li>使用了多个GPUs；</li><li>LRN（后来发现用处不大，丢弃了）</li></ul><p>AlexNet使得深度学习在计算机视觉方面受到极大的重视。</p><h4 id="VGG-16"><a href="#VGG-16" class="headerlink" title="VGG-16"></a>VGG-16</h4><p>VGG卷积层和池化层均具有相同的卷积核大小，都使用 3 <em> 3， stride=1 的卷积和 2 </em> 2，stride = 2的池化。其结构如下：</p><p><img src="/2020/03/12/卷积神经网络/17.jpg" alt=""></p><h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><p>ResNet是由残差块组成的</p><p><strong>残差块</strong><br>下面是一个普通的神经网络块的传输：</p><p><img src="/2020/03/12/卷积神经网络/18.jpg" alt=""></p><p>而ResNet块则将其传播过程增加了一个从 [公式] 直接到 [公式] 的连接，将其称之为“short cut”或者“skip connection”：</p><p>直接在中间添加了一条捷径，原因是为了解决 深层次的神经网络容易造成梯度消失，以至于后面梯度为0，就不好去更新参数</p><p><img src="/2020/03/12/卷积神经网络/19.jpg" alt=""></p><p>增加“short cut”后，成为残差块的网络结构：</p><p><img src="/2020/03/12/卷积神经网络/20.jpg" alt=""></p><p>Residual Network：</p><p>多个残差块堆积起来构成ResNet的网络结构，其结构如下所示：</p><p><img src="/2020/03/12/卷积神经网络/21.jpg" alt=""></p><ul><li><p>在没有残差的普通神经网络中，训练的误差实际上是随着网络层数的加深，先减小再增加；</p></li><li><p>在有残差的ResNet中，即使网络再深，训练误差都会随着网络层数的加深逐渐减小。</p></li></ul><p>ResNet对于中间的激活函数来说，有助于能够达到更深的网络，解决梯度消失和梯度爆炸的问题。</p><h3 id="ResNet表现好的原因"><a href="#ResNet表现好的原因" class="headerlink" title="ResNet表现好的原因"></a>ResNet表现好的原因</h3><p>下面就直接贴从网站上面找到的相关原因</p><p><img src="/2020/03/12/卷积神经网络/22.png" alt=""></p><p>将普通深度神经网络变为ResNet：</p><p><img src="/2020/03/12/卷积神经网络/23.jpg" alt=""></p><h3 id="1-1-卷积"><a href="#1-1-卷积" class="headerlink" title="1 * 1 卷积"></a>1 * 1 卷积</h3><p>在二维上的卷积相当于图片的每个元素和一个卷积核数字相乘。</p><p>但是在三维上，与 nc 卷积核进行卷积，相当于三维图像上的 1 <em> 1 </em> nc 的切片，也就是 nc 个点乘以卷积数值权重，通过Relu函数后，输出对应的结果。而不同的卷积核则相当于不同的隐层神经元结点与切片上的点进行一一连接。</p><p>所以根本上 1 * 1 卷积核相当于对一个切片上的 nc 个单元都应用了一个全连接的神经网络。</p><p><img src="/2020/03/12/卷积神经网络/24.jpg" alt=""></p><p>1x1卷积应用：</p><p>维度压缩：使用目标维度的 1 <em> 1 的卷积核个数。<br>增加非线性：保持与原维度相同的 1 </em> 1 的卷积核个数</p><p><img src="/2020/03/12/卷积神经网络/25.jpg" alt=""></p><h3 id="Inception-Network"><a href="#Inception-Network" class="headerlink" title="Inception Network"></a>Inception Network</h3><p>Inception Network 的作用就是使我们无需去考虑在构建深度卷积神经网络时，使用多大的卷积核以及是否添加池化层等问题。</p><p>Inception主要结构：</p><p><img src="/2020/03/12/卷积神经网络/26.jpg" alt=""></p><p><strong>计算成本的问题</strong></p><p>对于上面的 5 * 5 大小卷积核的计算成本：</p><p><img src="/2020/03/12/卷积神经网络/27.jpg" alt=""></p><p>对于 1 * 1 大小卷积核用作过渡的计算成本，也将下面的中间的层叫做“bottleneck layer”：<br><img src="/2020/03/12/卷积神经网络/28.jpg" alt=""></p><p>所以 1 * 1 卷积核作为“bottleneck layer”的过渡层能够有效减小卷积神经网的计算成本。事实证明，只要合理地设置“bottleneck layer”，既可以显著减小上层的规模，同时又能降低计算成本，从而不会影响网络的性能。</p><p><strong>Inception 模块：</strong></p><p>将上面说介绍的两种主要思想和模式结合到一起构成 Inception 模块，如下：</p><p><img src="/2020/03/12/卷积神经网络/29.jpg" alt=""></p><p><strong>Inception Network：</strong></p><p>多个Inception 模块的堆叠构成Inception Network，下面是GoogleNet的结构：</p><p><img src="/2020/03/12/卷积神经网络/30.jpg" alt=""></p><h3 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h3><p><strong>小数据集：</strong></p><p>如今在深度学习领域，许多研究者都会将他们的工作共享到网络上。在我们实施自己的工作的时候，比如说做某种物体的识别分类，但是只有少量的数据集，对于从头开始训练一个深度网络结构是远远不够的。</p><p>但是我们可以应用迁移学习，应用其他研究者建立的模型和参数，用少量的数据仅训练最后自定义的softmax网络。从而能够在小数据集上达到很好的效果。</p><p>其实就是pytorch上面 pretrain=True</p><p>如果是自己的数据，那么就建议用他人预训练好的模型，自己再训练自己的数据就会非常省事</p><p><strong>大数据集：</strong></p><p>如果我们在自己的问题上也拥有大量的数据集，我们可以多训练后面的几层。总之随着数据集的增加，我们需要“ freeze”的层数越来越少。最后如果我们有十分庞大的数据集，那么我们可以训练网络模型的所有参数，将其他研究者训练的模型参数作为参数的初始化来替代随机初始化，来加速我们模型的训练。</p><p>就像最后一次pytorch训练 识别图片上面在做的事情一样，将最后一层进行一个更改就可以了。</p><h3 id="数据增强-以及数据扩充"><a href="#数据增强-以及数据扩充" class="headerlink" title="数据增强 以及数据扩充"></a>数据增强 以及数据扩充</h3><p>与其他机器学习问题相比，在计算机视觉领域当下最主要的问题是没有办法得到充足的数据。所以在我们训练计算机数据模型的时候，数据的扩充就是会非常有用。</p><p><strong>数据扩充的方法：</strong></p><ul><li>镜像翻转（Mirroring）</li><li>随机剪裁（Random Cropping）</li><li>色彩转换（Color shifting）</li></ul><p>为图片的RGB三个色彩通道进行增减值，如（R：+20，G：-20，B：+20）；PCA颜色增强：对图片的主色的变化较大，图片的次色变化较小，使总体的颜色保持一致。</p><p><strong>为了节省时间，数据扩充的过程和训练过程可以多CPU多线程来并行的实现。</strong></p><p><strong>数据和手工工程：</strong></p><p>在有大量数据的时候，我们更倾向于使用简单的算法和更少的手工工程。因为此时有大量的数据，我们不需要为这个问题来精心设计特征，我们使用一个大的网络结果或者更简单的模型就能够解决。</p><p>相反，在有少量数据的时候，我们从事更多的是手工工程。因为数据量太少，较大的网络结构或者模型很难从这些少量的数据中获取足够的特征，而手工工程实际上是获得良好表现的最佳方式。</p><p>对于机器学习应用：</p><ul><li>标记数据，（x,y）</li><li>手工特征工程/网络结构/其他构建</li></ul><p>在基准研究和比赛中，下面的tips可能会有较好的表现：</p><ul><li><p>Ensembling：独立地训练多个网络模型，输出平均结果或加权平均结果；</p></li><li><p>测试时的 Multi-crop：在测试图片的多种版本上运行分类器，输出平均结果。</p></li></ul><h2 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h2><h3 id="目标定位和特征点检测"><a href="#目标定位和特征点检测" class="headerlink" title="目标定位和特征点检测"></a>目标定位和特征点检测</h3><p><strong>图片检测问题：</strong></p><ul><li>分类问题：判断图中是否为汽车；</li><li>目标定位：判断是否为汽车，并确定具体位置；</li><li>目标检测：检测不同物体并定位。</li></ul><p><img src="/2020/03/12/卷积神经网络/31.jpg" alt=""></p><p><strong>目标分类和定位：</strong></p><p>对于目标定位问题，我们卷积神经网络模型结构可能如下：</p><p><img src="/2020/03/12/卷积神经网络/32.jpg" alt=""></p><p>输出：包含图片中存在的对象及定位框</p><p>行人，0 or 1；<br>汽车，0 or 1；<br>摩托车，0 or 1；<br>图片背景，0 or 1；</p><p>然后根据图片中的定位框：</p><p>其中bx，by表示汽车中点，bh，bw分别表示定位框的。以左上图为（0，0），以右下图为（1，1），这些数字均为位置或长度</p><p><img src="/2020/03/12/卷积神经网络/33.png" alt=""></p><p><img src="/2020/03/12/卷积神经网络/34.png" alt=""></p><p><strong>特征点检测：</strong></p><p>由前面的目标定位问题，我们可以知道，神经网络可以通过输出图片上特征点的坐标（x,y），来实现对目标特征的识别和定位标记。</p><p>如对于人脸表情识别的问题中，我们通过标定训练数据集中特征点的位置信息，来对人脸进行不同位置不同特征的定位和标记。AR的应用就是基于人脸表情识别来设计的，如脸部扭曲、增加头部配饰等。</p><p>在人体姿态检测中，同样可以通过对人体不同的特征位置关键点的标注，来记录人体的姿态。</p><h3 id="目标检测-1"><a href="#目标检测-1" class="headerlink" title="目标检测"></a>目标检测</h3><p>目标检测采用的是基于滑动窗口的检测算法。</p><p><img src="/2020/03/12/卷积神经网络/35.png" alt=""></p><h4 id="滑动窗口目标检测："><a href="#滑动窗口目标检测：" class="headerlink" title="滑动窗口目标检测："></a>滑动窗口目标检测：</h4><p><img src="/2020/03/12/卷积神经网络/36.jpg" alt=""></p><ul><li><p>首先选定一个特定大小的窗口，将窗口内的图片输入到模型中进行预测</p></li><li><p>以固定步幅滑动该窗口，遍历图像的每个区域，对窗内的各个小图不断输入模型进行预测</p></li><li><p>继续选取一个更大的窗口，再次遍历图像的每个区域，对区域内是否有车进行预测</p></li><li><p>遍历整个图像，可以保证在每个位置都能检测到是否有车</p></li></ul><p>缺点：计算成本巨大，每个窗口的小图都要进行卷积运算，（但在神经网络兴起之前，使用的是线性分类器，所以滑动窗口算法的计算成本较低）。</p><h4 id="卷积层替代全连接层："><a href="#卷积层替代全连接层：" class="headerlink" title="卷积层替代全连接层："></a>卷积层替代全连接层：</h4><p>对于卷积网络中全连接层，我们可以利用1 * 1大小卷积核的卷积层来替代。</p><p>就像下面一样:</p><p><img src="/2020/03/12/卷积神经网络/37.jpg" alt=""></p><p><strong>滑动窗口卷积实现</strong></p><p>在我们实现了以卷积层替代全部的全连接层以后，在该基础上进行滑动窗口在卷积层上的操作。下面以一个小的图片为例：</p><p><img src="/2020/03/12/卷积神经网络/38.jpg" alt=""></p><p>我们以上面训练好的模型，输入一个16 <em> 16 </em> 3大小的整幅图片，图中蓝色部分代表滑动窗口的大小。我们以2为大小的步幅滑动窗口，分别与卷积核进行卷积运算，最后得到4幅 10 <em> 10 </em> 16 大小的特征图，然而因为在滑动窗口的操作时，输入部分有大量的重叠，也就是有很多重复的运算，导致在下一层中的特征图值也存在大量的重叠，所以最后得到的第二层激活值（特征图）构成一副 12 <em> 12 </em> 16大小的特征图。对于后面的池化层和全连接层也是同样的过程。</p><p>那么由此可知，滑动窗口在整幅图片上进行滑动卷积的操作过程，就等同于在该图片上直接进行卷积运算的过程。所以卷积层实现滑动窗口的这个过程，我们不需要把输入图片分割成四个子集分别执行前向传播，而是把他们作为一张图片输入到卷积神经网络中进行计算，其中的重叠部分（公共区域）可以共享大量的计算。</p><p><strong>汽车目标检测：</strong></p><p>依据上面的方法，我们将整张图片输入到训练好的卷积神经网络中。无需再利用滑动窗口分割图片，只需一次前向传播，我们就可以同时得到所有图片子集的预测值</p><p><img src="/2020/03/12/卷积神经网络/39.jpg" alt=""></p><h3 id="Bounding-Box-预测"><a href="#Bounding-Box-预测" class="headerlink" title="Bounding Box 预测"></a>Bounding Box 预测</h3><p>前面一节的卷积方式实现的滑动窗口算法，使得在预测时计算的效率大大提高。但是其存在的问题是：不能输出最精准的边界框（Bounding Box）。</p><p>在滑动窗口算法中，我们取的一些离散的图片子集的位置，在这种情况下，有可能我们没有得到一个能够完美匹配汽车位置的窗口，也有可能真实汽车的边界框为一个长方形。所以我们需要寻找更加精确的边界框。</p><p><img src="/2020/03/12/卷积神经网络/40.jpg" alt=""></p><p><strong>YOLO：</strong></p><p>YOLO算法可以使得滑动窗口算法寻找到更加精准的边界框。</p><p><img src="/2020/03/12/卷积神经网络/41.png" alt=""></p><p><strong>YOLO notation：</strong></p><ul><li><p>将对象分配到一个格子的过程是：观察对象的中点，将该对象分配到其中点所在的格子中，（即使对象横跨多个格子，也只分配到中点所在的格子中，其他格子记为无该对象，即标记为“0”）；</p></li><li><p>YOLO显式地输出边界框，使得其可以具有任意宽高比，并且能输出更精确的坐标，不受滑动窗口算法滑动步幅大小的限制；</p></li><li><p>YOLO是一次卷积实现，并不是在 n * n网格上进行 n 的平方运算，而是单次卷积实现，算法实现效率高，运行速度快，可以实现实时识别。</p></li></ul><p><strong>bounding boxes 细节：</strong></p><p>利用YOLO算法实现目标探测的时候，对于存在目标对象的网格中，定义训练标签Y的时候，边界框的指定参数的不同对其预测精度有很大的影响。这里给出一个较为合理的约定：（其他参数指定方式可阅读论文）</p><p><img src="/2020/03/12/卷积神经网络/42.png" alt=""></p><h3 id="交并比（Intersection-over-Union）"><a href="#交并比（Intersection-over-Union）" class="headerlink" title="交并比（Intersection-over-Union）"></a>交并比（Intersection-over-Union）</h3><p>交并比函数用来评价目标检测算法是否运作良好。</p><p>对于理想的边界框和目标探测算法预测得到的边界框，交并比函数计算两个边界框交集和并集之比。</p><p>IoU = 交集面积 / 并集面积</p><p>一般在目标检测任务中，约定如果IoU &gt;= 0.5 ，那么就说明检测正确。当然标准越大，则对目标检测算法越严格。得到的IoU值越大越好。</p><h3 id="非最大值抑制（non-max-suppression，NMS）"><a href="#非最大值抑制（non-max-suppression，NMS）" class="headerlink" title="非最大值抑制（non-max suppression，NMS）"></a>非最大值抑制（non-max suppression，NMS）</h3><p>对于我们前面提到的目标检测算法，可能会对同一个对象做出多次的检测，非最大值抑制可以确保我们的算法对每个对象只检测一次。</p><p><strong>多网格检测同一物体：</strong></p><p>对于汽车目标检测的例子中，我们将图片分成很多精细的格子。最终预测输出的结果中，可能会有相邻的多个格子里均检测出都具有同一个对象。</p><p><img src="/2020/03/12/卷积神经网络/43.jpg" alt=""></p><p><img src="/2020/03/12/卷积神经网络/44.png" alt=""></p><p><img src="/2020/03/12/卷积神经网络/45.png" alt=""></p><h3 id="Anchor-box"><a href="#Anchor-box" class="headerlink" title="Anchor box"></a>Anchor box</h3><p>通过上面的各种方法，目前我们的目标检测算法在每个格子上只能检测出一个对象。使用Anchor box可以同时检测出多个对象。</p><p><img src="/2020/03/12/卷积神经网络/46.png" alt=""></p><p><strong>Anchor box 的选择：</strong></p><ul><li><p>一般人工指定Anchor box 的形状，选择5~10个以覆盖到多种不同的形状，可以涵盖我们想要检测的对象的形状；</p></li><li><p>高级方法：K-means 算法：将不同对象形状进行聚类，用聚类后的结果来选择一组最具代表性的Anchor box，以此来代表我们想要检测对象的形状。</p></li></ul><h3 id="YOLO算法目标检测"><a href="#YOLO算法目标检测" class="headerlink" title="YOLO算法目标检测"></a>YOLO算法目标检测</h3><p>假设我们要在图片中检测三种目标：行人、汽车和摩托车，同时使用两种不同的Anchor box。</p><p><img src="/2020/03/12/卷积神经网络/47.png" alt=""></p><p><strong>模型预测：</strong></p><p>输入与训练集中相同大小的图片，同时得到每个格子中不同的输出结果：3 <em> 3 </em> 2 * 8</p><p><strong>运行非最大值抑制（NMS）：</strong></p><p>假设使用了2个Anchor box，那么对于每一个网格，我们都会得到预测输出的2个bounding boxes，其中一个 Pc 比较高</p><p><img src="/2020/03/12/卷积神经网络/48.jpg" alt=""></p><p>抛弃概率 Pc 值低的预测bounding boxes<br><img src="/2020/03/12/卷积神经网络/49.jpg" alt=""></p><p>对每个对象（如行人、汽车、摩托车）分别使用NMS算法得到最终的预测边界框<br><img src="/2020/03/12/卷积神经网络/50.jpg" alt=""></p><h3 id="候选区域（region-proposals）"><a href="#候选区域（region-proposals）" class="headerlink" title="候选区域（region proposals）"></a>候选区域（region proposals）</h3><p><strong>R-CNN：</strong></p><p>R-CNN（Regions with convolutional networks），会在我们的图片中选出一些目标的候选区域，从而避免了传统滑动窗口在大量无对象区域的无用运算。</p><p>所以在使用了R-CNN后，我们不会再针对每个滑动窗口运算检测算法，而是只选择一些候选区域的窗口，在少数的窗口上运行卷积网络。</p><p>具体实现：运用图像分割算法，将图片分割成许多不同颜色的色块，然后在这些色块上放置窗口，将窗口中的内容输入网络，从而减小需要处理的窗口数量。</p><p><img src="/2020/03/12/卷积神经网络/51.jpg" alt=""></p><p><strong>更快的算法：</strong></p><ul><li><p>R-CNN：给出候选区域，不使用滑动窗口，对每个候选区域进行分类识别，输出对象 标签 和 bounding box，从而在确实存在对象的区域得到更精确的边界框，但速度慢；</p></li><li><p>Fast R-CNN：给出候选区域，使用滑动窗口的卷积实现去分类所有的候选区域，但得到候选区的聚类步骤仍然非常慢；</p></li><li><p>Faster R-CNN：使用卷积网络给出候选区域。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
      <category term="机器学习" scheme="http://wsx1128.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://wsx1128.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://wsx1128.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="卷积网络" scheme="http://wsx1128.cn/tags/%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Kaggle Titanic 生存预测</title>
    <link href="http://wsx1128.cn/2020/03/11/Kaggle-Titanic-%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B/"/>
    <id>http://wsx1128.cn/2020/03/11/Kaggle-Titanic-生存预测/</id>
    <published>2020-03-11T15:02:08.000Z</published>
    <updated>2020-03-12T14:45:30.189Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>这是一篇入门的kaggle的初级案例。</p><p>本来自己第一遍去做的时候，发现预测的正确率只能够达到77%，经过了无数次调参，最后也只能到达79%，于是自己决定花点时间按照流程写一遍，并且记录下来</p><p>题目比较具有趣味性。给你两个数据集合，其中一个有标签值，另外一个没有标签值，然后给出了姓名，性别，船费，年龄，以及船仓位相关的数据</p><p>然后预测这个人是否能够存活</p><p>按照相关的步骤来：</p><h3 id="分析题目"><a href="#分析题目" class="headerlink" title="分析题目"></a>分析题目</h3><p>首先这道题就是一个让你预测test数据组里面的人能否存活，其实就是一个二分类问题，如果存活最后的结果输出1，如果没有存活，那么最后的结果输出0</p><p>确认这是一个二分类问题，接下来就可以去整理数据了</p><h3 id="数据总览"><a href="#数据总览" class="headerlink" title="数据总览"></a>数据总览</h3><p>Titanic 生存模型预测，其中包含了两组数据：train.csv 和 test.csv，分别为训练集合和测试集合。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面这函数就是在jupyter notbook里面，</span></span><br><span class="line"><span class="comment"># 可以不必写show函数，可以直接显示出来</span></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><p>运用pandas里面的函数，去进行观测数据，并且开始着手数据的处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_data = pd.read_csv(<span class="string">'data/train.csv'</span>)</span><br><span class="line">test_data = pd.read_csv(<span class="string">'data/test.csv'</span>)</span><br><span class="line"></span><br><span class="line">sns.set_style(<span class="string">'whitegrid'</span>)</span><br><span class="line">train_data.head()</span><br></pre></td></tr></table></figure><p><img src="/2020/03/11/Kaggle-Titanic-生存预测/1.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_data.info()</span><br><span class="line">print(<span class="string">"-"</span> * <span class="number">40</span>)</span><br><span class="line">test_data.info()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">pandas</span>.<span class="title">core</span>.<span class="title">frame</span>.<span class="title">DataFrame</span>'&gt;</span></span><br><span class="line"><span class="class">    <span class="title">RangeIndex</span>:</span> <span class="number">891</span> entries, <span class="number">0</span> to <span class="number">890</span></span><br><span class="line">    Data columns (total <span class="number">12</span> columns):</span><br><span class="line">    PassengerId    <span class="number">891</span> non-null int64</span><br><span class="line">    Survived       <span class="number">891</span> non-null int64</span><br><span class="line">    Pclass         <span class="number">891</span> non-null int64</span><br><span class="line">    Name           <span class="number">891</span> non-null object</span><br><span class="line">    Sex            <span class="number">891</span> non-null object</span><br><span class="line">    Age            <span class="number">714</span> non-null float64</span><br><span class="line">    SibSp          <span class="number">891</span> non-null int64</span><br><span class="line">    Parch          <span class="number">891</span> non-null int64</span><br><span class="line">    Ticket         <span class="number">891</span> non-null object</span><br><span class="line">    Fare           <span class="number">891</span> non-null float64</span><br><span class="line">    Cabin          <span class="number">204</span> non-null object</span><br><span class="line">    Embarked       <span class="number">889</span> non-null object</span><br><span class="line">    dtypes: float64(<span class="number">2</span>), int64(<span class="number">5</span>), object(<span class="number">5</span>)</span><br><span class="line">    memory usage: <span class="number">83.6</span>+ KB</span><br><span class="line">    ----------------------------------------</span><br><span class="line">    &lt;<span class="class"><span class="keyword">class</span> '<span class="title">pandas</span>.<span class="title">core</span>.<span class="title">frame</span>.<span class="title">DataFrame</span>'&gt;</span></span><br><span class="line"><span class="class">    <span class="title">RangeIndex</span>:</span> <span class="number">418</span> entries, <span class="number">0</span> to <span class="number">417</span></span><br><span class="line">    Data columns (total <span class="number">11</span> columns):</span><br><span class="line">    PassengerId    <span class="number">418</span> non-null int64</span><br><span class="line">    Pclass         <span class="number">418</span> non-null int64</span><br><span class="line">    Name           <span class="number">418</span> non-null object</span><br><span class="line">    Sex            <span class="number">418</span> non-null object</span><br><span class="line">    Age            <span class="number">332</span> non-null float64</span><br><span class="line">    SibSp          <span class="number">418</span> non-null int64</span><br><span class="line">    Parch          <span class="number">418</span> non-null int64</span><br><span class="line">    Ticket         <span class="number">418</span> non-null object</span><br><span class="line">    Fare           <span class="number">417</span> non-null float64</span><br><span class="line">    Cabin          <span class="number">91</span> non-null object</span><br><span class="line">    Embarked       <span class="number">418</span> non-null object</span><br><span class="line">    dtypes: float64(<span class="number">2</span>), int64(<span class="number">4</span>), object(<span class="number">5</span>)</span><br><span class="line">    memory usage: <span class="number">36.0</span>+ KB</span><br></pre></td></tr></table></figure><p>从上面我们可以看出，Age、Cabin、Embarked、Fare几个特征存在缺失值。</p><p>绘制存活的比例：</p><p>下面先来一个可视化：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">'Survived'</span>].value_counts().plot.pie(autopct = <span class="string">'%1.2f%%'</span>)</span><br></pre></td></tr></table></figure></p><p><img src="/2020/03/11/Kaggle-Titanic-生存预测/2.jpg" alt=""></p><p>由上面这张图，我们可以得知 活下来的人与没有活下来的人总体比例，就开始对数据有一个整体的认知</p><h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><p>接下来，我们就要开始准备数据的处理，一般数据存在的问题，要么就是离群点问题，要么就是缺失值问题，要么就是数据变换的问题。</p><h4 id="数值变换"><a href="#数值变换" class="headerlink" title="数值变换"></a>数值变换</h4><p><strong>变量转换的目的是将数据转换为适用于模型使用的数据，不同模型接受不同类型的数据，Scikit-learn要求数据都是数字型numeric，所以我们要将一些非数字型的原始数据转换为数字型numeric。</strong></p><p>所有的数据可以分为两类：</p><ol><li>定性(Quantitative)变量可以以某种方式排序，Age就是一个很好的列子。</li><li>定量(Qualitative)变量描述了物体的某一（不能被数学表示的）方面，Embarked就是一个例子。</li></ol><p>针对上面不可以被数学数字表示的数据，下面采取：</p><h5 id="Dummy-Variables"><a href="#Dummy-Variables" class="headerlink" title="Dummy Variables"></a>Dummy Variables</h5><p>就是类别变量或者二元变量，当qualitative variable是一些频繁出现的几个独立变量时，Dummy Variables比较适合使用。我们以Embarked为例，Embarked只包含三个值’S’,’C’,’Q’，我们可以使用下面的代码将其转换为dummies:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">embark_dummies  = pd.get_dummies(train_data[<span class="string">'Embarked'</span>])</span><br><span class="line">train_data = train_data.join(embark_dummies)</span><br><span class="line">train_data.drop([<span class="string">'Embarked'</span>], axis=<span class="number">1</span>,inplace=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">embark_dummies = train_data[[<span class="string">'S'</span>, <span class="string">'C'</span>, <span class="string">'Q'</span>]]</span><br><span class="line">embark_dummies.head()</span><br></pre></td></tr></table></figure><p><img src="/2020/03/11/Kaggle-Titanic-生存预测/3.jpg" alt=""></p><p>其实上面的这种方法有点特别像RNN中词向量。给每一个词加上相应的特征，应用one-hot编码形式</p><h5 id="Factorizing"><a href="#Factorizing" class="headerlink" title="Factorizing"></a>Factorizing</h5><p>dummy不好处理Cabin（船舱号）这种标称属性，因为他出现的变量比较多。所以Pandas有一个方法叫做factorize()，它可以创建一些数字，来表示类别变量，对每一个类别映射一个ID，这种映射最后只生成一个特征，不像dummy那样生成多个特征。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Replace missing values with "U0"</span></span><br><span class="line">train_data[<span class="string">'Cabin'</span>][train_data.Cabin.isnull()] = <span class="string">'U0'</span></span><br><span class="line"><span class="comment"># create feature for the alphabetical part of the cabin number</span></span><br><span class="line">train_data[<span class="string">'CabinLetter'</span>] = train_data[<span class="string">'Cabin'</span>].map( <span class="keyword">lambda</span> x : re.compile(<span class="string">"([a-zA-Z]+)"</span>).search(x).group())</span><br><span class="line"><span class="comment"># convert the distinct cabin letters with incremental integer values</span></span><br><span class="line">train_data[<span class="string">'CabinLetter'</span>] = pd.factorize(train_data[<span class="string">'CabinLetter'</span>])[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">train_data[<span class="string">'CabinLetter'</span>].head()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"> <span class="number">0</span>    <span class="number">0</span></span><br><span class="line"> <span class="number">1</span>    <span class="number">1</span></span><br><span class="line"> <span class="number">2</span>    <span class="number">0</span></span><br><span class="line"> <span class="number">3</span>    <span class="number">1</span></span><br><span class="line"> <span class="number">4</span>    <span class="number">0</span></span><br><span class="line"> Name: CabinLetter, dtype: int64</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line"><span class="comment">##### Scaling</span></span><br><span class="line">Scaling可以将一个很大范围的数值映射到一个很小的范围(通常是<span class="number">-1</span> - <span class="number">1</span>，或则是<span class="number">0</span> - <span class="number">1</span>)，很多情况下我们需要将数值做Scaling使其范围大小一样，否则大范围数值特征将会由更高的权重。比如：Age的范围可能只是<span class="number">0</span><span class="number">-100</span>，而income的范围可能是<span class="number">0</span><span class="number">-10000000</span>，在某些对数组大小敏感的模型中会影响其结果。</span><br><span class="line"></span><br><span class="line">下面对Age进行Scaling：</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> np.size(train_data[<span class="string">'Age'</span>]) == <span class="number">891</span></span><br><span class="line"><span class="comment"># StandardScaler will subtract the mean from each value then scale to the unit variance</span></span><br><span class="line">scaler = preprocessing.StandardScaler()</span><br><span class="line">train_data[<span class="string">'Age_scaled'</span>] = scaler.fit_transform(train_data[<span class="string">'Age'</span>].values.reshape(<span class="number">-1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">'Age_scaled'</span>].head()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0</span>   <span class="number">-0.558449</span></span><br><span class="line"><span class="number">1</span>    <span class="number">0.606773</span></span><br><span class="line"><span class="number">2</span>   <span class="number">-0.267144</span></span><br><span class="line"><span class="number">3</span>    <span class="number">0.388293</span></span><br><span class="line"><span class="number">4</span>    <span class="number">0.388293</span></span><br><span class="line">Name: Age_scaled, dtype: float64</span><br></pre></td></tr></table></figure><h5 id="Binning"><a href="#Binning" class="headerlink" title="Binning"></a>Binning</h5><p>Binning通过观察“邻居”(即周围的值)将连续数据离散化。存储的值被分布到一些“桶”或“箱“”中，就像直方图的bin将数据划分成几块一样。下面的代码对Fare进行Binning。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Divide all fares into quartiles</span></span><br><span class="line">train_data[<span class="string">'Fare_bin'</span>] = pd.qcut(train_data[<span class="string">'Fare'</span>], <span class="number">5</span>)</span><br><span class="line">train_data[<span class="string">'Fare_bin'</span>].head()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0</span>      (<span class="number">-0.001</span>, <span class="number">7.854</span>]</span><br><span class="line"><span class="number">1</span>    (<span class="number">39.688</span>, <span class="number">512.329</span>]</span><br><span class="line"><span class="number">2</span>        (<span class="number">7.854</span>, <span class="number">10.5</span>]</span><br><span class="line"><span class="number">3</span>    (<span class="number">39.688</span>, <span class="number">512.329</span>]</span><br><span class="line"><span class="number">4</span>        (<span class="number">7.854</span>, <span class="number">10.5</span>]</span><br><span class="line">Name: Fare_bin, dtype: category</span><br><span class="line">Categories (<span class="number">5</span>, interval[float64]): [(<span class="number">-0.001</span>, <span class="number">7.854</span>] &lt; (<span class="number">7.854</span>, <span class="number">10.5</span>] &lt; (<span class="number">10.5</span>, <span class="number">21.679</span>] &lt; (<span class="number">21.679</span>, <span class="number">39.688</span>] &lt; (<span class="number">39.688</span>, <span class="number">512.329</span>]]</span><br></pre></td></tr></table></figure><p>一般这里比较好的使用这个算法的位置是给范围值进行等级的划分，这样以来，然而就会有更好的结果</p><p>在将数据Bining化后，要么将数据factorize化，要么dummies化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># factorize</span></span><br><span class="line">train_data[<span class="string">'Fare_bin_id'</span>] = pd.factorize(train_data[<span class="string">'Fare_bin'</span>])[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># dummies</span></span><br><span class="line">fare_bin_dummies_df = pd.get_dummies(train_data[<span class="string">'Fare_bin'</span>]).rename(columns=<span class="keyword">lambda</span> x: <span class="string">'Fare_'</span> + str(x))</span><br><span class="line">train_data = pd.concat([train_data, fare_bin_dummies_df], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h4 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h4><p>一些机器学习算法能够处理缺失值，比如神经网络，一些则不能。对于缺失值，一般有以下几种处理方法：</p><ul><li>如果数据集很多，但有很少的缺失值，可以删掉带缺失值的行；</li><li>如果该属性相对学习来说不是很重要，可以对缺失值赋均值或者众数。比如在哪儿上船Embarked这一属性（共有三个上船地点），缺失俩值，可以用众数赋值</li></ul><p>因为“Embarked”项的缺失值不多，所以这里我们以众数来填充：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">combined_train_test[<span class="string">'Embarked'</span>].fillna(combined_train_test[<span class="string">'Embarked'</span>].mode().iloc[<span class="number">0</span>], inplace=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>对于三种不同的港口，由上面介绍的数值转换，我们知道可以有两种特征处理方式：dummy和facrorizing。因为只有三个港口，所以我们可以直接用dummy来处理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为了后面的特征分析，这里我们将 Embarked 特征进行facrorizing</span></span><br><span class="line">combined_train_test[<span class="string">'Embarked'</span>] = pd.factorize(combined_train_test[<span class="string">'Embarked'</span>])[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 pd.get_dummies 获取one-hot 编码</span></span><br><span class="line">emb_dummies_df = pd.get_dummies(combined_train_test[<span class="string">'Embarked'</span>], prefix=combined_train_test[[<span class="string">'Embarked'</span>]].columns[<span class="number">0</span>])</span><br><span class="line">combined_train_test = pd.concat([combined_train_test, emb_dummies_df], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>因为Age项的缺失值较多，所以不能直接填充age的众数或者平均数。</p><p>常见的有两种对年龄的填充方式：一种是根据Title中的称呼，如Mr，Master、Miss等称呼不同类别的人的平均年龄来填充；一种是综合几项如Sex、Title、Pclass等其他没有缺失值的项，使用机器学习算法来预测Age。</p><p>这里我们使用后者来处理。以Age为目标值，将Age完整的项作为训练集，将Age缺失的项作为测试集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">missing_age_df = pd.DataFrame(combined_train_test[</span><br><span class="line"> [<span class="string">'Age'</span>, <span class="string">'Embarked'</span>, <span class="string">'Sex'</span>, <span class="string">'Title'</span>, <span class="string">'Name_length'</span>, <span class="string">'Family_Size'</span>, <span class="string">'Family_Size_Category'</span>,<span class="string">'Fare'</span>, <span class="string">'Fare_bin_id'</span>, <span class="string">'Pclass'</span>]])</span><br><span class="line"></span><br><span class="line">missing_age_train = missing_age_df[missing_age_df[<span class="string">'Age'</span>].notnull()]</span><br><span class="line">missing_age_test = missing_age_df[missing_age_df[<span class="string">'Age'</span>].isnull()]</span><br><span class="line"></span><br><span class="line">missing_age_test.head()</span><br></pre></td></tr></table></figure><p><img src="/2020/03/11/Kaggle-Titanic-生存预测/4.jpg" alt=""></p><p>建立Age的预测模型，我们可以多模型预测，然后再做模型的融合，提高预测的精度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> ensemble</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fill_missing_age</span><span class="params">(missing_age_train, missing_age_test)</span>:</span></span><br><span class="line">    missing_age_X_train = missing_age_train.drop([<span class="string">'Age'</span>], axis=<span class="number">1</span>)</span><br><span class="line">    missing_age_Y_train = missing_age_train[<span class="string">'Age'</span>]</span><br><span class="line">    missing_age_X_test = missing_age_test.drop([<span class="string">'Age'</span>], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># model 1  gbm</span></span><br><span class="line">    gbm_reg = GradientBoostingRegressor(random_state=<span class="number">42</span>)</span><br><span class="line">    gbm_reg_param_grid = &#123;<span class="string">'n_estimators'</span>: [<span class="number">2000</span>], <span class="string">'max_depth'</span>: [<span class="number">4</span>], <span class="string">'learning_rate'</span>: [<span class="number">0.01</span>], <span class="string">'max_features'</span>: [<span class="number">3</span>]&#125;</span><br><span class="line">    gbm_reg_grid = model_selection.GridSearchCV(gbm_reg, gbm_reg_param_grid, cv=<span class="number">10</span>, n_jobs=<span class="number">25</span>, verbose=<span class="number">1</span>, scoring=<span class="string">'neg_mean_squared_error'</span>)</span><br><span class="line">    gbm_reg_grid.fit(missing_age_X_train, missing_age_Y_train)</span><br><span class="line">    print(<span class="string">'Age feature Best GB Params:'</span> + str(gbm_reg_grid.best_params_))</span><br><span class="line">    print(<span class="string">'Age feature Best GB Score:'</span> + str(gbm_reg_grid.best_score_))</span><br><span class="line">    print(<span class="string">'GB Train Error for "Age" Feature Regressor:'</span> + str(gbm_reg_grid.score(missing_age_X_train, missing_age_Y_train)))</span><br><span class="line">    missing_age_test.loc[:, <span class="string">'Age_GB'</span>] = gbm_reg_grid.predict(missing_age_X_test)</span><br><span class="line">    print(missing_age_test[<span class="string">'Age_GB'</span>][:<span class="number">4</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># model 2 rf</span></span><br><span class="line">    rf_reg = RandomForestRegressor()</span><br><span class="line">    rf_reg_param_grid = &#123;<span class="string">'n_estimators'</span>: [<span class="number">200</span>], <span class="string">'max_depth'</span>: [<span class="number">5</span>], <span class="string">'random_state'</span>: [<span class="number">0</span>]&#125;</span><br><span class="line">    rf_reg_grid = model_selection.GridSearchCV(rf_reg, rf_reg_param_grid, cv=<span class="number">10</span>, n_jobs=<span class="number">25</span>, verbose=<span class="number">1</span>, scoring=<span class="string">'neg_mean_squared_error'</span>)</span><br><span class="line">    rf_reg_grid.fit(missing_age_X_train, missing_age_Y_train)</span><br><span class="line">    print(<span class="string">'Age feature Best RF Params:'</span> + str(rf_reg_grid.best_params_))</span><br><span class="line">    print(<span class="string">'Age feature Best RF Score:'</span> + str(rf_reg_grid.best_score_))</span><br><span class="line">    print(<span class="string">'RF Train Error for "Age" Feature Regressor'</span> + str(rf_reg_grid.score(missing_age_X_train, missing_age_Y_train)))</span><br><span class="line">    missing_age_test.loc[:, <span class="string">'Age_RF'</span>] = rf_reg_grid.predict(missing_age_X_test)</span><br><span class="line">    print(missing_age_test[<span class="string">'Age_RF'</span>][:<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># two models merge</span></span><br><span class="line">    print(<span class="string">'shape1'</span>, missing_age_test[<span class="string">'Age'</span>].shape, missing_age_test[[<span class="string">'Age_GB'</span>, <span class="string">'Age_RF'</span>]].mode(axis=<span class="number">1</span>).shape)</span><br><span class="line">    <span class="comment"># missing_age_test['Age'] = missing_age_test[['Age_GB', 'Age_LR']].mode(axis=1)</span></span><br><span class="line"></span><br><span class="line">    missing_age_test.loc[:, <span class="string">'Age'</span>] = np.mean([missing_age_test[<span class="string">'Age_GB'</span>], missing_age_test[<span class="string">'Age_RF'</span>]])</span><br><span class="line">    print(missing_age_test[<span class="string">'Age'</span>][:<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">    missing_age_test.drop([<span class="string">'Age_GB'</span>, <span class="string">'Age_RF'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> missing_age_test</span><br></pre></td></tr></table></figure><p>利用融合模型预测的结果填充Age的缺失值：</p><p>其实这个地方需要注意的，上面是将trainset与testset进行一个有效的拼接</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">combined_train_test.loc[(combined_train_test.Age.isnull()), <span class="string">'Age'</span>] = fill_missing_age(missing_age_train, missing_age_test)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">Fitting <span class="number">10</span> folds <span class="keyword">for</span> each of <span class="number">1</span> candidates, totalling <span class="number">10</span> fits</span><br><span class="line"></span><br><span class="line"> [Parallel(n_jobs=<span class="number">25</span>)]: Done   <span class="number">5</span> out of  <span class="number">10</span> | elapsed:    <span class="number">3.9</span>s remaining:    <span class="number">3.9</span>s</span><br><span class="line"> [Parallel(n_jobs=<span class="number">25</span>)]: Done  <span class="number">10</span> out of  <span class="number">10</span> | elapsed:    <span class="number">6.9</span>s finished</span><br><span class="line"></span><br><span class="line"> Age feature Best GB Params:&#123;<span class="string">'n_estimators'</span>: <span class="number">2000</span>, <span class="string">'learning_rate'</span>: <span class="number">0.01</span>, <span class="string">'max_features'</span>: <span class="number">3</span>, <span class="string">'max_depth'</span>: <span class="number">4</span>&#125;</span><br><span class="line"> Age feature Best GB Score:<span class="number">-130.295677599</span></span><br><span class="line"> GB Train Error <span class="keyword">for</span> <span class="string">"Age"</span> Feature Regressor:<span class="number">-64.6566961723</span></span><br><span class="line"> <span class="number">5</span>     <span class="number">35.773942</span></span><br><span class="line"> <span class="number">17</span>    <span class="number">31.489153</span></span><br><span class="line"> <span class="number">19</span>    <span class="number">34.113840</span></span><br><span class="line"> <span class="number">26</span>    <span class="number">28.621281</span></span><br><span class="line"> Name: Age_GB, dtype: float64</span><br><span class="line"> Fitting <span class="number">10</span> folds <span class="keyword">for</span> each of <span class="number">1</span> candidates, totalling <span class="number">10</span> fits</span><br><span class="line"></span><br><span class="line"> [Parallel(n_jobs=<span class="number">25</span>)]: Done   <span class="number">5</span> out of  <span class="number">10</span> | elapsed:    <span class="number">6.2</span>s remaining:    <span class="number">6.2</span>s</span><br><span class="line"> [Parallel(n_jobs=<span class="number">25</span>)]: Done  <span class="number">10</span> out of  <span class="number">10</span> | elapsed:   <span class="number">10.7</span>s finished</span><br><span class="line"></span><br><span class="line"> Age feature Best RF Params:&#123;<span class="string">'n_estimators'</span>: <span class="number">200</span>, <span class="string">'random_state'</span>: <span class="number">0</span>, <span class="string">'max_depth'</span>: <span class="number">5</span>&#125;</span><br><span class="line"> Age feature Best RF Score:<span class="number">-119.094956052</span></span><br><span class="line"> RF Train Error <span class="keyword">for</span> <span class="string">"Age"</span> Feature Regressor<span class="number">-96.0603148448</span></span><br><span class="line"> <span class="number">5</span>     <span class="number">33.459421</span></span><br><span class="line"> <span class="number">17</span>    <span class="number">33.076798</span></span><br><span class="line"> <span class="number">19</span>    <span class="number">34.855942</span></span><br><span class="line"> <span class="number">26</span>    <span class="number">28.146718</span></span><br><span class="line"> Name: Age_RF, dtype: float64</span><br><span class="line"> shape1 (<span class="number">263</span>,) (<span class="number">263</span>, <span class="number">2</span>)</span><br><span class="line"> <span class="number">5</span>     <span class="number">30.000675</span></span><br><span class="line"> <span class="number">17</span>    <span class="number">30.000675</span></span><br><span class="line"> <span class="number">19</span>    <span class="number">30.000675</span></span><br><span class="line"> <span class="number">26</span>    <span class="number">30.000675</span></span><br><span class="line"> Name: Age, dtype: float64</span><br></pre></td></tr></table></figure><p>最后一项缺失值处理</p><p>因为Cabin项的缺失值确实太多了，我们很难对其进行分析，或者预测。所以这里我们可以直接将Cabin这一项特征去除。但通过上面的分析，可以知道，该特征信息的有无也与生存率有一定的关系，所以这里我们暂时保留该特征，并将其分为有和无两类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">combined_train_test.loc[combined_train_test.Cabin.isnull(), <span class="string">'Cabin'</span>] = <span class="string">'U0'</span></span><br><span class="line">combined_train_test[<span class="string">'Cabin'</span>] = combined_train_test[<span class="string">'Cabin'</span>].apply(<span class="keyword">lambda</span> x: <span class="number">0</span> <span class="keyword">if</span> x == <span class="string">'U0'</span> <span class="keyword">else</span> <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="特征处理"><a href="#特征处理" class="headerlink" title="特征处理"></a>特征处理</h3><p>上面已经处理了一些数据，现在我需要增强一些特征，以便与后面特征的变化</p><h4 id="Sex"><a href="#Sex" class="headerlink" title="Sex"></a>Sex</h4><p>对Sex也进行one-hot编码，也就是dummy处理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为了后面的特征分析，这里我们也将 Sex 特征进行facrorizing</span></span><br><span class="line">combined_train_test[<span class="string">'Sex'</span>] = pd.factorize(combined_train_test[<span class="string">'Sex'</span>])[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">sex_dummies_df = pd.get_dummies(combined_train_test[<span class="string">'Sex'</span>], prefix=combined_train_test[[<span class="string">'Sex'</span>]].columns[<span class="number">0</span>])</span><br><span class="line">combined_train_test = pd.concat([combined_train_test, sex_dummies_df], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h4 id="Name"><a href="#Name" class="headerlink" title="Name"></a>Name</h4><p>首先先从名字中提取各种称呼：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># what is each person's title? </span></span><br><span class="line">combined_train_test[<span class="string">'Title'</span>] = combined_train_test[<span class="string">'Name'</span>].map(<span class="keyword">lambda</span> x: re.compile(<span class="string">", (.*?)\."</span>).findall(x)[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p>将各式称呼进行统一化处理：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">title_Dict = &#123;&#125;</span><br><span class="line">title_Dict.update(dict.fromkeys([<span class="string">'Capt'</span>, <span class="string">'Col'</span>, <span class="string">'Major'</span>, <span class="string">'Dr'</span>, <span class="string">'Rev'</span>], <span class="string">'Officer'</span>))</span><br><span class="line">title_Dict.update(dict.fromkeys([<span class="string">'Don'</span>, <span class="string">'Sir'</span>, <span class="string">'the Countess'</span>, <span class="string">'Dona'</span>, <span class="string">'Lady'</span>], <span class="string">'Royalty'</span>))</span><br><span class="line">title_Dict.update(dict.fromkeys([<span class="string">'Mme'</span>, <span class="string">'Ms'</span>, <span class="string">'Mrs'</span>], <span class="string">'Mrs'</span>))</span><br><span class="line">title_Dict.update(dict.fromkeys([<span class="string">'Mlle'</span>, <span class="string">'Miss'</span>], <span class="string">'Miss'</span>))</span><br><span class="line">title_Dict.update(dict.fromkeys([<span class="string">'Mr'</span>], <span class="string">'Mr'</span>))</span><br><span class="line">title_Dict.update(dict.fromkeys([<span class="string">'Master'</span>,<span class="string">'Jonkheer'</span>], <span class="string">'Master'</span>))</span><br><span class="line"></span><br><span class="line">combined_train_test[<span class="string">'Title'</span>] = combined_train_test[<span class="string">'Title'</span>].map(title_Dict)</span><br></pre></td></tr></table></figure></p><p>使用dummy对不同的称呼进行分列：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为了后面的特征分析，这里我们也将 Title 特征进行facrorizing</span></span><br><span class="line">combined_train_test[<span class="string">'Title'</span>] = pd.factorize(combined_train_test[<span class="string">'Title'</span>])[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">title_dummies_df = pd.get_dummies(combined_train_test[<span class="string">'Title'</span>], prefix=combined_train_test[[<span class="string">'Title'</span>]].columns[<span class="number">0</span>])</span><br><span class="line">combined_train_test = pd.concat([combined_train_test, title_dummies_df], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>下面开始就不准备贴代码了，可以去GitHub上面去看</p><h4 id="Fare"><a href="#Fare" class="headerlink" title="Fare"></a>Fare</h4><p>由前面分析可以知道，Fare项在测试数据中缺少一个值，所以需要对该值进行填充。</p><p>我们按照一二三等舱各自的均价来填充：</p><p>下面transform将函数np.mean应用到各个group中</p><p>通过对Ticket数据的分析，我们可以看到部分票号数据有重复，同时结合亲属人数及名字的数据，和票价船舱等级对比，我们可以知道购买的票中有家庭票和团体票，所以我们需要将团体票的票价分配到每个人的头上。</p><p>使用binning给票价分等级，对于5个等级的票价我们也可以继续使用dummy为票价等级分列。</p><h4 id="Pclass"><a href="#Pclass" class="headerlink" title="Pclass"></a>Pclass</h4><p>这里是神来之笔，我看到别人在这个地方有出奇的特征增强方法：</p><p>Pclass这一项，其实已经可以不用继续处理了，我们只需要将其转换为dummy形式即可。</p><p>但是为了更好的分析问题，我们这里假设对于不同等级的船舱，各船舱内部的票价也说明了各等级舱的位置，那么也就很有可能与逃生的顺序有关系。所以这里分出每等舱里的高价和低价位。</p><h4 id="Parch-and-SibSp"><a href="#Parch-and-SibSp" class="headerlink" title="Parch and SibSp"></a>Parch and SibSp</h4><p>对于这两个参数，大佬们的处理就是 <strong>将这两个特征整合成一个特征，牛逼了大佬</strong></p><h3 id="特征间相关性分析"><a href="#特征间相关性分析" class="headerlink" title="特征间相关性分析"></a>特征间相关性分析</h3><p>我们挑选一些主要的特征，生成特征之间的关联图，查看特征与特征之间的相关性：<br>将上述所有特征去看看相互之间的关系或者与结果之间的关系</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Correlation = pd.DataFrame(combined_train_test[</span><br><span class="line"> [<span class="string">'Embarked'</span>, <span class="string">'Sex'</span>, <span class="string">'Title'</span>, <span class="string">'Name_length'</span>, <span class="string">'Family_Size'</span>, <span class="string">'Family_Size_Category'</span>,<span class="string">'Fare'</span>, <span class="string">'Fare_bin_id'</span>, <span class="string">'Pclass'</span>, </span><br><span class="line">  <span class="string">'Pclass_Fare_Category'</span>, <span class="string">'Age'</span>, <span class="string">'Ticket_Letter'</span>, <span class="string">'Cabin'</span>]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">colormap = plt.cm.viridis</span><br><span class="line">plt.figure(figsize=(<span class="number">14</span>,<span class="number">12</span>))</span><br><span class="line">plt.title(<span class="string">'Pearson Correlation of Features'</span>, y=<span class="number">1.05</span>, size=<span class="number">15</span>)</span><br><span class="line">sns.heatmap(Correlation.astype(float).corr(),linewidths=<span class="number">0.1</span>,vmax=<span class="number">1.0</span>, square=<span class="keyword">True</span>, cmap=colormap, linecolor=<span class="string">'white'</span>, annot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p><img src="/2020/03/11/Kaggle-Titanic-生存预测/5.jpg" alt=""></p><p><strong>特征之间的数据分布图</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">g = sns.pairplot(combined_train_test[[<span class="string">u'Survived'</span>, <span class="string">u'Pclass'</span>, <span class="string">u'Sex'</span>, <span class="string">u'Age'</span>, <span class="string">u'Fare'</span>, <span class="string">u'Embarked'</span>,</span><br><span class="line">    <span class="string">u'Family_Size'</span>, <span class="string">u'Title'</span>, <span class="string">u'Ticket_Letter'</span>]], hue=<span class="string">'Survived'</span>, palette = <span class="string">'seismic'</span>,size=<span class="number">1.2</span>,diag_kind = <span class="string">'kde'</span>,diag_kws=dict(shade=<span class="keyword">True</span>),plot_kws=dict(s=<span class="number">10</span>) )</span><br><span class="line">g.set(xticklabels=[])</span><br></pre></td></tr></table></figure><p><img src="/2020/03/11/Kaggle-Titanic-生存预测/6.jpg" alt=""></p><h4 id="输入模型前的一些处理"><a href="#输入模型前的一些处理" class="headerlink" title="输入模型前的一些处理"></a>输入模型前的一些处理</h4><p>这里我们将Age和fare进行<strong>正则化</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scale_age_fare = preprocessing.StandardScaler().fit(combined_train_test[[<span class="string">'Age'</span>,<span class="string">'Fare'</span>, <span class="string">'Name_length'</span>]])</span><br><span class="line">combined_train_test[[<span class="string">'Age'</span>,<span class="string">'Fare'</span>, <span class="string">'Name_length'</span>]] = scale_age_fare.transform(combined_train_test[[<span class="string">'Age'</span>,<span class="string">'Fare'</span>, <span class="string">'Name_length'</span>]])</span><br></pre></td></tr></table></figure><p>丢弃掉一些无用的特征</p><p>对于上面的特征工程中，我们从一些原始的特征中提取出了很多要融合到模型中的特征，但是我们需要剔除那些原本的我们用不到的或者非数值特征。</p><p><strong>首先对我们的数据先进行一下备份，以便后期的再次分析：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">combined_data_backup = combined_train_test</span><br><span class="line">combined_train_test.drop([<span class="string">'PassengerId'</span>, <span class="string">'Embarked'</span>, <span class="string">'Sex'</span>, <span class="string">'Name'</span>, <span class="string">'Title'</span>, <span class="string">'Fare_bin_id'</span>, <span class="string">'Pclass_Fare_Category'</span>, </span><br><span class="line">                       <span class="string">'Parch'</span>, <span class="string">'SibSp'</span>, <span class="string">'Family_Size_Category'</span>, <span class="string">'Ticket'</span>],axis=<span class="number">1</span>,inplace=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>将训练数据和测试数据分开：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">train_data = combined_train_test[:<span class="number">891</span>]</span><br><span class="line">test_data = combined_train_test[<span class="number">891</span>:]</span><br><span class="line"></span><br><span class="line">titanic_train_data_X = train_data.drop([<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line">titanic_train_data_Y = train_data[<span class="string">'Survived'</span>]</span><br><span class="line">titanic_test_data_X = test_data.drop([<span class="string">'Survived'</span>],axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">titanic_train_data_X.shape</span><br></pre></td></tr></table></figure><h3 id="模型融合以及测试"><a href="#模型融合以及测试" class="headerlink" title="模型融合以及测试"></a>模型融合以及测试</h3><p>需要按照下面的这些步骤来进行：</p><h4 id="利用不同的模型来对特征进行筛选，选出较为重要的特征："><a href="#利用不同的模型来对特征进行筛选，选出较为重要的特征：" class="headerlink" title="利用不同的模型来对特征进行筛选，选出较为重要的特征："></a>利用不同的模型来对特征进行筛选，选出较为重要的特征：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> ExtraTreesClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_top_n_features</span><span class="params">(titanic_train_data_X, titanic_train_data_Y, top_n_features)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># random forest</span></span><br><span class="line">    rf_est = RandomForestClassifier(random_state=<span class="number">0</span>)</span><br><span class="line">    rf_param_grid = &#123;<span class="string">'n_estimators'</span>: [<span class="number">500</span>], <span class="string">'min_samples_split'</span>: [<span class="number">2</span>, <span class="number">3</span>], <span class="string">'max_depth'</span>: [<span class="number">20</span>]&#125;</span><br><span class="line">    rf_grid = model_selection.GridSearchCV(rf_est, rf_param_grid, n_jobs=<span class="number">25</span>, cv=<span class="number">10</span>, verbose=<span class="number">1</span>)</span><br><span class="line">    rf_grid.fit(titanic_train_data_X, titanic_train_data_Y)</span><br><span class="line">    print(<span class="string">'Top N Features Best RF Params:'</span> + str(rf_grid.best_params_))</span><br><span class="line">    print(<span class="string">'Top N Features Best RF Score:'</span> + str(rf_grid.best_score_))</span><br><span class="line">    print(<span class="string">'Top N Features RF Train Score:'</span> + str(rf_grid.score(titanic_train_data_X, titanic_train_data_Y)))</span><br><span class="line">    feature_imp_sorted_rf = pd.DataFrame(&#123;<span class="string">'feature'</span>: list(titanic_train_data_X),</span><br><span class="line">                                          <span class="string">'importance'</span>: rf_grid.best_estimator_.feature_importances_&#125;).sort_values(<span class="string">'importance'</span>, ascending=<span class="keyword">False</span>)</span><br><span class="line">    features_top_n_rf = feature_imp_sorted_rf.head(top_n_features)[<span class="string">'feature'</span>]</span><br><span class="line">    print(<span class="string">'Sample 10 Features from RF Classifier'</span>)</span><br><span class="line">    print(str(features_top_n_rf[:<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># AdaBoost</span></span><br><span class="line">    ada_est =AdaBoostClassifier(random_state=<span class="number">0</span>)</span><br><span class="line">    ada_param_grid = &#123;<span class="string">'n_estimators'</span>: [<span class="number">500</span>], <span class="string">'learning_rate'</span>: [<span class="number">0.01</span>, <span class="number">0.1</span>]&#125;</span><br><span class="line">    ada_grid = model_selection.GridSearchCV(ada_est, ada_param_grid, n_jobs=<span class="number">25</span>, cv=<span class="number">10</span>, verbose=<span class="number">1</span>)</span><br><span class="line">    ada_grid.fit(titanic_train_data_X, titanic_train_data_Y)</span><br><span class="line">    print(<span class="string">'Top N Features Best Ada Params:'</span> + str(ada_grid.best_params_))</span><br><span class="line">    print(<span class="string">'Top N Features Best Ada Score:'</span> + str(ada_grid.best_score_))</span><br><span class="line">    print(<span class="string">'Top N Features Ada Train Score:'</span> + str(ada_grid.score(titanic_train_data_X, titanic_train_data_Y)))</span><br><span class="line">    feature_imp_sorted_ada = pd.DataFrame(&#123;<span class="string">'feature'</span>: list(titanic_train_data_X),</span><br><span class="line">                                           <span class="string">'importance'</span>: ada_grid.best_estimator_.feature_importances_&#125;).sort_values(<span class="string">'importance'</span>, ascending=<span class="keyword">False</span>)</span><br><span class="line">    features_top_n_ada = feature_imp_sorted_ada.head(top_n_features)[<span class="string">'feature'</span>]</span><br><span class="line">    print(<span class="string">'Sample 10 Feature from Ada Classifier:'</span>)</span><br><span class="line">    print(str(features_top_n_ada[:<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ExtraTree</span></span><br><span class="line">    et_est = ExtraTreesClassifier(random_state=<span class="number">0</span>)</span><br><span class="line">    et_param_grid = &#123;<span class="string">'n_estimators'</span>: [<span class="number">500</span>], <span class="string">'min_samples_split'</span>: [<span class="number">3</span>, <span class="number">4</span>], <span class="string">'max_depth'</span>: [<span class="number">20</span>]&#125;</span><br><span class="line">    et_grid = model_selection.GridSearchCV(et_est, et_param_grid, n_jobs=<span class="number">25</span>, cv=<span class="number">10</span>, verbose=<span class="number">1</span>)</span><br><span class="line">    et_grid.fit(titanic_train_data_X, titanic_train_data_Y)</span><br><span class="line">    print(<span class="string">'Top N Features Best ET Params:'</span> + str(et_grid.best_params_))</span><br><span class="line">    print(<span class="string">'Top N Features Best ET Score:'</span> + str(et_grid.best_score_))</span><br><span class="line">    print(<span class="string">'Top N Features ET Train Score:'</span> + str(et_grid.score(titanic_train_data_X, titanic_train_data_Y)))</span><br><span class="line">    feature_imp_sorted_et = pd.DataFrame(&#123;<span class="string">'feature'</span>: list(titanic_train_data_X),</span><br><span class="line">                                          <span class="string">'importance'</span>: et_grid.best_estimator_.feature_importances_&#125;).sort_values(<span class="string">'importance'</span>, ascending=<span class="keyword">False</span>)</span><br><span class="line">    features_top_n_et = feature_imp_sorted_et.head(top_n_features)[<span class="string">'feature'</span>]</span><br><span class="line">    print(<span class="string">'Sample 10 Features from ET Classifier:'</span>)</span><br><span class="line">    print(str(features_top_n_et[:<span class="number">10</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># GradientBoosting</span></span><br><span class="line">    gb_est =GradientBoostingClassifier(random_state=<span class="number">0</span>)</span><br><span class="line">    gb_param_grid = &#123;<span class="string">'n_estimators'</span>: [<span class="number">500</span>], <span class="string">'learning_rate'</span>: [<span class="number">0.01</span>, <span class="number">0.1</span>], <span class="string">'max_depth'</span>: [<span class="number">20</span>]&#125;</span><br><span class="line">    gb_grid = model_selection.GridSearchCV(gb_est, gb_param_grid, n_jobs=<span class="number">25</span>, cv=<span class="number">10</span>, verbose=<span class="number">1</span>)</span><br><span class="line">    gb_grid.fit(titanic_train_data_X, titanic_train_data_Y)</span><br><span class="line">    print(<span class="string">'Top N Features Best GB Params:'</span> + str(gb_grid.best_params_))</span><br><span class="line">    print(<span class="string">'Top N Features Best GB Score:'</span> + str(gb_grid.best_score_))</span><br><span class="line">    print(<span class="string">'Top N Features GB Train Score:'</span> + str(gb_grid.score(titanic_train_data_X, titanic_train_data_Y)))</span><br><span class="line">    feature_imp_sorted_gb = pd.DataFrame(&#123;<span class="string">'feature'</span>: list(titanic_train_data_X),</span><br><span class="line">                                           <span class="string">'importance'</span>: gb_grid.best_estimator_.feature_importances_&#125;).sort_values(<span class="string">'importance'</span>, ascending=<span class="keyword">False</span>)</span><br><span class="line">    features_top_n_gb = feature_imp_sorted_gb.head(top_n_features)[<span class="string">'feature'</span>]</span><br><span class="line">    print(<span class="string">'Sample 10 Feature from GB Classifier:'</span>)</span><br><span class="line">    print(str(features_top_n_gb[:<span class="number">10</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># DecisionTree</span></span><br><span class="line">    dt_est = DecisionTreeClassifier(random_state=<span class="number">0</span>)</span><br><span class="line">    dt_param_grid = &#123;<span class="string">'min_samples_split'</span>: [<span class="number">2</span>, <span class="number">4</span>], <span class="string">'max_depth'</span>: [<span class="number">20</span>]&#125;</span><br><span class="line">    dt_grid = model_selection.GridSearchCV(dt_est, dt_param_grid, n_jobs=<span class="number">25</span>, cv=<span class="number">10</span>, verbose=<span class="number">1</span>)</span><br><span class="line">    dt_grid.fit(titanic_train_data_X, titanic_train_data_Y)</span><br><span class="line">    print(<span class="string">'Top N Features Best DT Params:'</span> + str(dt_grid.best_params_))</span><br><span class="line">    print(<span class="string">'Top N Features Best DT Score:'</span> + str(dt_grid.best_score_))</span><br><span class="line">    print(<span class="string">'Top N Features DT Train Score:'</span> + str(dt_grid.score(titanic_train_data_X, titanic_train_data_Y)))</span><br><span class="line">    feature_imp_sorted_dt = pd.DataFrame(&#123;<span class="string">'feature'</span>: list(titanic_train_data_X),</span><br><span class="line">                                          <span class="string">'importance'</span>: dt_grid.best_estimator_.feature_importances_&#125;).sort_values(<span class="string">'importance'</span>, ascending=<span class="keyword">False</span>)</span><br><span class="line">    features_top_n_dt = feature_imp_sorted_dt.head(top_n_features)[<span class="string">'feature'</span>]</span><br><span class="line">    print(<span class="string">'Sample 10 Features from DT Classifier:'</span>)</span><br><span class="line">    print(str(features_top_n_dt[:<span class="number">10</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># merge the three models</span></span><br><span class="line">    features_top_n = pd.concat([features_top_n_rf, features_top_n_ada, features_top_n_et, features_top_n_gb, features_top_n_dt], </span><br><span class="line">                               ignore_index=<span class="keyword">True</span>).drop_duplicates()</span><br><span class="line">    </span><br><span class="line">    features_importance = pd.concat([feature_imp_sorted_rf, feature_imp_sorted_ada, feature_imp_sorted_et, </span><br><span class="line">                                   feature_imp_sorted_gb, feature_imp_sorted_dt],ignore_index=<span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> features_top_n , features_importance</span><br></pre></td></tr></table></figure><p>上面的代码不需要看着害怕，其实无非就是调用一些神奇的函数而已，上面的代码就是在 多个模型中去寻找重要的特征，并且进行排序，为后面模型训练作出贡献。</p><h4 id="依据我们筛选出的特征构建训练集和测试集"><a href="#依据我们筛选出的特征构建训练集和测试集" class="headerlink" title="依据我们筛选出的特征构建训练集和测试集"></a>依据我们筛选出的特征构建训练集和测试集</h4><p>但如果在进行特征工程的过程中，产生了大量的特征，而特征与特征之间会存在一定的相关性。太多的特征一方面会影响模型训练的速度，另一方面也可能会使得模型过拟合。所以在特征太多的情况下，我们可以利用不同的模型对特征进行筛选，选取出我们想要的前n个特征。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">feature_to_pick = <span class="number">30</span></span><br><span class="line">feature_top_n, feature_importance = get_top_n_features(titanic_train_data_X, titanic_train_data_Y, feature_to_pick)</span><br><span class="line">titanic_train_data_X = pd.DataFrame(titanic_train_data_X[feature_top_n])</span><br><span class="line">titanic_test_data_X = pd.DataFrame(titanic_test_data_X[feature_top_n])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line">Fitting <span class="number">10</span> folds <span class="keyword">for</span> each of <span class="number">2</span> candidates, totalling <span class="number">20</span> fits</span><br><span class="line"></span><br><span class="line"> [Parallel(n_jobs=<span class="number">25</span>)]: Done  <span class="number">13</span> out of  <span class="number">20</span> | elapsed:   <span class="number">13.7</span>s remaining:    <span class="number">7.3</span>s</span><br><span class="line"> [Parallel(n_jobs=<span class="number">25</span>)]: Done  <span class="number">20</span> out of  <span class="number">20</span> | elapsed:   <span class="number">19.2</span>s finished</span><br><span class="line"></span><br><span class="line"> Top N Features Best RF Params:&#123;<span class="string">'n_estimators'</span>: <span class="number">500</span>, <span class="string">'min_samples_split'</span>: <span class="number">3</span>, <span class="string">'max_depth'</span>: <span class="number">20</span>&#125;</span><br><span class="line"> Top N Features Best RF Score:<span class="number">0.822671156004</span></span><br><span class="line"> Top N Features RF Train Score:<span class="number">0.979797979798</span></span><br><span class="line"> Sample <span class="number">10</span> Features <span class="keyword">from</span> RF Classifier</span><br><span class="line"> <span class="number">15</span>      Name_length</span><br><span class="line"> <span class="number">0</span>               Age</span><br><span class="line"> <span class="number">2</span>              Fare</span><br><span class="line"> <span class="number">7</span>             Sex_0</span><br><span class="line"> <span class="number">9</span>           Title_0</span><br><span class="line"> <span class="number">8</span>             Sex_1</span><br><span class="line"> <span class="number">27</span>      Family_Size</span><br><span class="line"> <span class="number">3</span>            Pclass</span><br><span class="line"> <span class="number">31</span>    Ticket_Letter</span><br><span class="line"> <span class="number">11</span>          Title_2</span><br><span class="line"> Name: feature, dtype: object</span><br><span class="line"> Fitting <span class="number">10</span> folds <span class="keyword">for</span> each of <span class="number">2</span> candidates, totalling <span class="number">20</span> fits</span><br><span class="line"></span><br><span class="line"> [Parallel(n_jobs=<span class="number">25</span>)]: Done  <span class="number">13</span> out of  <span class="number">20</span> | elapsed:   <span class="number">10.3</span>s remaining:    <span class="number">5.5</span>s</span><br><span class="line"> [Parallel(n_jobs=<span class="number">25</span>)]: Done  <span class="number">20</span> out of  <span class="number">20</span> | elapsed:   <span class="number">14.9</span>s finished</span><br><span class="line"></span><br><span class="line"> Top N Features Best Ada Params:&#123;<span class="string">'n_estimators'</span>: <span class="number">500</span>, <span class="string">'learning_rate'</span>: <span class="number">0.01</span>&#125;</span><br><span class="line"> Top N Features Best Ada Score:<span class="number">0.81593714927</span></span><br><span class="line"> Top N Features Ada Train Score:<span class="number">0.820426487093</span></span><br><span class="line"> Sample <span class="number">10</span> Feature <span class="keyword">from</span> Ada Classifier:</span><br><span class="line"> <span class="number">9</span>                    Title_0</span><br><span class="line"> <span class="number">2</span>                       Fare</span><br><span class="line"> <span class="number">27</span>               Family_Size</span><br><span class="line"> <span class="number">7</span>                      Sex_0</span><br><span class="line"> <span class="number">3</span>                     Pclass</span><br><span class="line"> <span class="number">28</span>    Family_Size_Category_0</span><br><span class="line"> <span class="number">1</span>                      Cabin</span><br><span class="line"> <span class="number">8</span>                      Sex_1</span><br><span class="line"> <span class="number">15</span>               Name_length</span><br><span class="line"> <span class="number">0</span>                        Age</span><br><span class="line"> Name: feature, dtype: object</span><br><span class="line"> Fitting <span class="number">10</span> folds <span class="keyword">for</span> each of <span class="number">2</span> candidates, totalling <span class="number">20</span> fits</span><br><span class="line"></span><br><span class="line"> [Parallel(n_jobs=<span class="number">25</span>)]: Done  <span class="number">13</span> out of  <span class="number">20</span> | elapsed:    <span class="number">9.8</span>s remaining:    <span class="number">5.3</span>s</span><br><span class="line"> [Parallel(n_jobs=<span class="number">25</span>)]: Done  <span class="number">20</span> out of  <span class="number">20</span> | elapsed:   <span class="number">14.2</span>s finished</span><br><span class="line"></span><br><span class="line"> Top N Features Best ET Params:&#123;<span class="string">'n_estimators'</span>: <span class="number">500</span>, <span class="string">'min_samples_split'</span>: <span class="number">4</span>, <span class="string">'max_depth'</span>: <span class="number">20</span>&#125;</span><br><span class="line"> Top N Features Best ET Score:<span class="number">0.828282828283</span></span><br><span class="line"> Top N Features ET Train Score:<span class="number">0.971941638608</span></span><br><span class="line"> Sample <span class="number">10</span> Features <span class="keyword">from</span> ET Classifier:</span><br><span class="line"> <span class="number">9</span>           Title_0</span><br><span class="line"> <span class="number">8</span>             Sex_1</span><br><span class="line"> <span class="number">7</span>             Sex_0</span><br><span class="line"> <span class="number">15</span>      Name_length</span><br><span class="line"> <span class="number">0</span>               Age</span><br><span class="line"> <span class="number">2</span>              Fare</span><br><span class="line"> <span class="number">1</span>             Cabin</span><br><span class="line"> <span class="number">31</span>    Ticket_Letter</span><br><span class="line"> <span class="number">11</span>          Title_2</span><br><span class="line"> <span class="number">10</span>          Title_1</span><br><span class="line"> Name: feature, dtype: object</span><br><span class="line"> Fitting <span class="number">10</span> folds <span class="keyword">for</span> each of <span class="number">2</span> candidates, totalling <span class="number">20</span> fits</span><br><span class="line"></span><br><span class="line"> [Parallel(n_jobs=<span class="number">25</span>)]: Done  <span class="number">13</span> out of  <span class="number">20</span> | elapsed:   <span class="number">25.9</span>s remaining:   <span class="number">13.9</span>s</span><br><span class="line"> [Parallel(n_jobs=<span class="number">25</span>)]: Done  <span class="number">20</span> out of  <span class="number">20</span> | elapsed:   <span class="number">27.9</span>s finished</span><br><span class="line"></span><br><span class="line"> Top N Features Best GB Params:&#123;<span class="string">'n_estimators'</span>: <span class="number">500</span>, <span class="string">'learning_rate'</span>: <span class="number">0.1</span>, <span class="string">'max_depth'</span>: <span class="number">20</span>&#125;</span><br><span class="line"> Top N Features Best GB Score:<span class="number">0.789001122334</span></span><br><span class="line"> Top N Features GB Train Score:<span class="number">0.996632996633</span></span><br><span class="line"> Sample <span class="number">10</span> Feature <span class="keyword">from</span> GB Classifier:</span><br><span class="line"> <span class="number">0</span>               Age</span><br><span class="line"> <span class="number">2</span>              Fare</span><br><span class="line"> <span class="number">15</span>      Name_length</span><br><span class="line"> <span class="number">31</span>    Ticket_Letter</span><br><span class="line"> <span class="number">9</span>           Title_0</span><br><span class="line"> <span class="number">27</span>      Family_Size</span><br><span class="line"> <span class="number">23</span>         Pclass_2</span><br><span class="line"> <span class="number">3</span>            Pclass</span><br><span class="line"> <span class="number">18</span>           Fare_2</span><br><span class="line"> <span class="number">14</span>          Title_5</span><br><span class="line"> Name: feature, dtype: object</span><br><span class="line"> Fitting <span class="number">10</span> folds <span class="keyword">for</span> each of <span class="number">2</span> candidates, totalling <span class="number">20</span> fits</span><br><span class="line"></span><br><span class="line"> [Parallel(n_jobs=<span class="number">25</span>)]: Done  <span class="number">13</span> out of  <span class="number">20</span> | elapsed:    <span class="number">6.3</span>s remaining:    <span class="number">3.3</span>s</span><br><span class="line"> [Parallel(n_jobs=<span class="number">25</span>)]: Done  <span class="number">20</span> out of  <span class="number">20</span> | elapsed:    <span class="number">9.6</span>s finished</span><br><span class="line"></span><br><span class="line"> Top N Features Best DT Params:&#123;<span class="string">'min_samples_split'</span>: <span class="number">4</span>, <span class="string">'max_depth'</span>: <span class="number">20</span>&#125;</span><br><span class="line"> Top N Features Best DT Score:<span class="number">0.784511784512</span></span><br><span class="line"> Top N Features DT Train Score:<span class="number">0.959595959596</span></span><br><span class="line"> Sample <span class="number">10</span> Features <span class="keyword">from</span> DT Classifier:</span><br><span class="line"> <span class="number">9</span>           Title_0</span><br><span class="line"> <span class="number">0</span>               Age</span><br><span class="line"> <span class="number">2</span>              Fare</span><br><span class="line"> <span class="number">15</span>      Name_length</span><br><span class="line"> <span class="number">27</span>      Family_Size</span><br><span class="line"> <span class="number">14</span>          Title_5</span><br><span class="line"> <span class="number">26</span>         Pclass_5</span><br><span class="line"> <span class="number">3</span>            Pclass</span><br><span class="line"> <span class="number">31</span>    Ticket_Letter</span><br><span class="line"> <span class="number">23</span>         Pclass_2</span><br><span class="line"> Name: feature, dtype: object</span><br></pre></td></tr></table></figure><p>上面就是对之前的一个函数进行的一个调整</p><p>用视图可视化不同算法筛选的特征排序：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">rf_feature_imp = feature_importance[:<span class="number">10</span>]</span><br><span class="line">Ada_feature_imp = feature_importance[<span class="number">32</span>:<span class="number">32</span>+<span class="number">10</span>].reset_index(drop=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># make importances relative to max importance</span></span><br><span class="line">rf_feature_importance = <span class="number">100.0</span> * (rf_feature_imp[<span class="string">'importance'</span>] / rf_feature_imp[<span class="string">'importance'</span>].max())</span><br><span class="line">Ada_feature_importance = <span class="number">100.0</span> * (Ada_feature_imp[<span class="string">'importance'</span>] / Ada_feature_imp[<span class="string">'importance'</span>].max())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the indexes of all features over the importance threshold</span></span><br><span class="line">rf_important_idx = np.where(rf_feature_importance)[<span class="number">0</span>]</span><br><span class="line">Ada_important_idx = np.where(Ada_feature_importance)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Adapted from Gradient Boosting regression</span></span><br><span class="line">pos = np.arange(rf_important_idx.shape[<span class="number">0</span>]) + <span class="number">.5</span></span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">1</span>, figsize = (<span class="number">18</span>, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">121</span>)</span><br><span class="line">plt.barh(pos, rf_feature_importance[rf_important_idx][::<span class="number">-1</span>])</span><br><span class="line">plt.yticks(pos, rf_feature_imp[<span class="string">'feature'</span>][::<span class="number">-1</span>])</span><br><span class="line">plt.xlabel(<span class="string">'Relative Importance'</span>)</span><br><span class="line">plt.title(<span class="string">'RandomForest Feature Importance'</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">122</span>)</span><br><span class="line">plt.barh(pos, Ada_feature_importance[Ada_important_idx][::<span class="number">-1</span>])</span><br><span class="line">plt.yticks(pos, Ada_feature_imp[<span class="string">'feature'</span>][::<span class="number">-1</span>])</span><br><span class="line">plt.xlabel(<span class="string">'Relative Importance'</span>)</span><br><span class="line">plt.title(<span class="string">'AdaBoost Feature Importance'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2020/03/11/Kaggle-Titanic-生存预测/7.jpg" alt=""></p><h4 id="模型融合"><a href="#模型融合" class="headerlink" title="模型融合"></a>模型融合</h4><p>前面已经训练了多个模型，这个时候就可以直接展开融合了。</p><p>常见的模型融合方法有：Bagging、Boosting、Stacking、Blending</p><p>常见的相关介绍，直接去看笔记中</p><p><strong>Stacking框架融合:</strong></p><p>这里我们使用了两层的模型融合，Level 1使用了：RandomForest、AdaBoost、ExtraTrees、GBDT、DecisionTree、KNN、SVM ，一共7个模型，Level 2使用了XGBoost使用第一层预测的结果作为特征对最终的结果进行预测。</p><ol><li><p>Stacking框架是堆叠使用基础分类器的预测作为对二级模型的训练的输入。 然而，我们不能简单地在全部训练数据上训练基本模型，产生预测，输出用于第二层的训练。如果我们在Train Data上训练，然后在Train Data上预测，就会造成标签泄露。为了避免标签泄露，我们需要对每个基学习器使用K-fold，将K个模型对Valid Set的预测结果拼起来，作为下一层学习器的输入。</p><p> 所以这里我们建立输出K-fold预测的方法：</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"></span><br><span class="line"><span class="comment"># Some useful parameters which will come in handy later on</span></span><br><span class="line">ntrain = titanic_train_data_X.shape[<span class="number">0</span>]</span><br><span class="line">ntest = titanic_test_data_X.shape[<span class="number">0</span>]</span><br><span class="line">SEED = <span class="number">0</span> <span class="comment"># for reproducibility</span></span><br><span class="line">NFOLDS = <span class="number">7</span> <span class="comment"># set folds for out-of-fold prediction</span></span><br><span class="line">kf = KFold(n_splits = NFOLDS, random_state=SEED, shuffle=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_out_fold</span><span class="params">(clf, x_train, y_train, x_test)</span>:</span></span><br><span class="line">    oof_train = np.zeros((ntrain,))</span><br><span class="line">    oof_test = np.zeros((ntest,))</span><br><span class="line">    oof_test_skf = np.empty((NFOLDS, ntest))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, (train_index, test_index) <span class="keyword">in</span> enumerate(kf.split(x_train)):</span><br><span class="line">        x_tr = x_train[train_index]</span><br><span class="line">        y_tr = y_train[train_index]</span><br><span class="line">        x_te = x_train[test_index]</span><br><span class="line"></span><br><span class="line">        clf.fit(x_tr, y_tr)</span><br><span class="line"></span><br><span class="line">        oof_train[test_index] = clf.predict(x_te)</span><br><span class="line">        oof_test_skf[i, :] = clf.predict(x_test)</span><br><span class="line"></span><br><span class="line">    oof_test[:] = oof_test_skf.mean(axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> oof_train.reshape(<span class="number">-1</span>, <span class="number">1</span>), oof_test.reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure></li></ol><pre><code>构建不同的基学习器，这里我们使用了RandomForest、AdaBoost、ExtraTrees、GBDT、DecisionTree、KNN、SVM 七个基学习器：（这里的模型可以使用如上面的GridSearch方法对模型的超参数进行搜索选择）上面GridSearch就是对参数的搜索选择<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line">rf = RandomForestClassifier(n_estimators=<span class="number">500</span>, warm_start=<span class="keyword">True</span>, max_features=<span class="string">'sqrt'</span>,max_depth=<span class="number">6</span>, </span><br><span class="line">                            min_samples_split=<span class="number">3</span>, min_samples_leaf=<span class="number">2</span>, n_jobs=<span class="number">-1</span>, verbose=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">ada = AdaBoostClassifier(n_estimators=<span class="number">500</span>, learning_rate=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">et = ExtraTreesClassifier(n_estimators=<span class="number">500</span>, n_jobs=<span class="number">-1</span>, max_depth=<span class="number">8</span>, min_samples_leaf=<span class="number">2</span>, verbose=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">gb = GradientBoostingClassifier(n_estimators=<span class="number">500</span>, learning_rate=<span class="number">0.008</span>, min_samples_split=<span class="number">3</span>, min_samples_leaf=<span class="number">2</span>, max_depth=<span class="number">5</span>, verbose=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">dt = DecisionTreeClassifier(max_depth=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors = <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">svm = SVC(kernel=<span class="string">'linear'</span>, C=<span class="number">0.025</span>)</span><br></pre></td></tr></table></figure>将pandas转换为arrays：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_train = titanic_train_data_X.values <span class="comment"># Creates an array of the train data</span></span><br><span class="line">x_test = titanic_test_data_X.values <span class="comment"># Creats an array of the test data</span></span><br><span class="line">y_train = titanic_train_data_Y.values</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rf_oof_train, rf_oof_test = get_out_fold(rf, x_train, y_train, x_test) <span class="comment"># Random Forest</span></span><br><span class="line">ada_oof_train, ada_oof_test = get_out_fold(ada, x_train, y_train, x_test) <span class="comment"># AdaBoost </span></span><br><span class="line">et_oof_train, et_oof_test = get_out_fold(et, x_train, y_train, x_test) <span class="comment"># Extra Trees</span></span><br><span class="line">gb_oof_train, gb_oof_test = get_out_fold(gb, x_train, y_train, x_test) <span class="comment"># Gradient Boost</span></span><br><span class="line">dt_oof_train, dt_oof_test = get_out_fold(dt, x_train, y_train, x_test) <span class="comment"># Decision Tree</span></span><br><span class="line">knn_oof_train, knn_oof_test = get_out_fold(knn, x_train, y_train, x_test) <span class="comment"># KNeighbors</span></span><br><span class="line">svm_oof_train, svm_oof_test = get_out_fold(svm, x_train, y_train, x_test) <span class="comment"># Support Vector</span></span><br></pre></td></tr></table></figure></code></pre><ol><li><p>预测并提交文件。我们利用XGBoost，使用第一层预测的结果作为特征对最终的结果进行预测。</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</span><br><span class="line"></span><br><span class="line">gbm = XGBClassifier( n_estimators= <span class="number">2000</span>, max_depth= <span class="number">4</span>, min_child_weight= <span class="number">2</span>, gamma=<span class="number">0.9</span>, subsample=<span class="number">0.8</span>, </span><br><span class="line">                    colsample_bytree=<span class="number">0.8</span>, objective= <span class="string">'binary:logistic'</span>, nthread= <span class="number">-1</span>, scale_pos_weight=<span class="number">1</span>).fit(x_train, y_train)</span><br><span class="line">predictions = gbm.predict(x_test)</span><br></pre></td></tr></table></figure> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">StackingSubmission = pd.DataFrame(&#123;<span class="string">'PassengerId'</span>: PassengerId, <span class="string">'Survived'</span>: predictions&#125;)</span><br><span class="line">StackingSubmission.to_csv(<span class="string">'StackingSubmission.csv'</span>,index=<span class="keyword">False</span>,sep=<span class="string">','</span>)</span><br></pre></td></tr></table></figure></li></ol><p>最后的步骤就是验证了</p><h3 id="验证：学习曲线"><a href="#验证：学习曲线" class="headerlink" title="验证：学习曲线"></a>验证：学习曲线</h3><p>在我们对数据不断地进行特征工程，产生的特征越来越多，用大量的特征对模型进行训练，会使我们的训练集拟合得越来越好，但同时也可能会逐渐丧失泛化能力，从而在测试数据上表现不佳，发生过拟合现象。</p><p>当然我们建立的模型可能不仅在预测集上表型不好，也很可能是因为在训练集上的表现就不佳，处于欠拟合状态。</p><p><strong>所以我们通过学习曲线观察模型处于什么样的状态。从而决定对模型进行如何的操作。当然，我们把验证放到最后，并不是是这一步是在最后去做。对于我们的Stacking框架中第一层的各个基学习器我们都应该对其学习曲线进行观察，从而去更好地调节超参数，进而得到更好的最终结果。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.learning_curve <span class="keyword">import</span> learning_curve</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_learning_curve</span><span class="params">(estimator, title, X, y, ylim=None, cv=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                        n_jobs=<span class="number">1</span>, train_sizes=np.linspace<span class="params">(<span class="number">.1</span>, <span class="number">1.0</span>, <span class="number">5</span>)</span>, verbose=<span class="number">0</span>)</span>:</span></span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.title(title)</span><br><span class="line">    <span class="keyword">if</span> ylim <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        plt.ylim(*ylim)</span><br><span class="line">    plt.xlabel(<span class="string">"Training examples"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"Score"</span>)</span><br><span class="line">    train_sizes, train_scores, test_scores = learning_curve(</span><br><span class="line">        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)</span><br><span class="line">    train_scores_mean = np.mean(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    train_scores_std = np.std(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_scores_mean = np.mean(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_scores_std = np.std(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    plt.grid()</span><br><span class="line"></span><br><span class="line">    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,</span><br><span class="line">                     train_scores_mean + train_scores_std, alpha=<span class="number">0.1</span>,</span><br><span class="line">                     color=<span class="string">"r"</span>)</span><br><span class="line">    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,</span><br><span class="line">                     test_scores_mean + test_scores_std, alpha=<span class="number">0.1</span>, color=<span class="string">"g"</span>)</span><br><span class="line">    plt.plot(train_sizes, train_scores_mean, <span class="string">'o-'</span>, color=<span class="string">"r"</span>,</span><br><span class="line">             label=<span class="string">"Training score"</span>)</span><br><span class="line">    plt.plot(train_sizes, test_scores_mean, <span class="string">'o-'</span>, color=<span class="string">"g"</span>,</span><br><span class="line">             label=<span class="string">"Cross-validation score"</span>)</span><br><span class="line"></span><br><span class="line">    plt.legend(loc=<span class="string">"best"</span>)</span><br><span class="line">    <span class="keyword">return</span> plt</span><br></pre></td></tr></table></figure><p>逐一观察不同模型的学习曲线</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">X = x_train</span><br><span class="line">Y = y_train</span><br><span class="line"></span><br><span class="line"><span class="comment"># RandomForest</span></span><br><span class="line">rf_parameters = &#123;<span class="string">'n_jobs'</span>: <span class="number">-1</span>, <span class="string">'n_estimators'</span>: <span class="number">500</span>, <span class="string">'warm_start'</span>: <span class="keyword">True</span>, <span class="string">'max_depth'</span>: <span class="number">6</span>, <span class="string">'min_samples_leaf'</span>: <span class="number">2</span>, </span><br><span class="line">              <span class="string">'max_features'</span> : <span class="string">'sqrt'</span>,<span class="string">'verbose'</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># AdaBoost</span></span><br><span class="line">ada_parameters = &#123;<span class="string">'n_estimators'</span>:<span class="number">500</span>, <span class="string">'learning_rate'</span>:<span class="number">0.1</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># ExtraTrees</span></span><br><span class="line">et_parameters = &#123;<span class="string">'n_jobs'</span>: <span class="number">-1</span>, <span class="string">'n_estimators'</span>:<span class="number">500</span>, <span class="string">'max_depth'</span>: <span class="number">8</span>, <span class="string">'min_samples_leaf'</span>: <span class="number">2</span>, <span class="string">'verbose'</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># GradientBoosting</span></span><br><span class="line">gb_parameters = &#123;<span class="string">'n_estimators'</span>: <span class="number">500</span>, <span class="string">'max_depth'</span>: <span class="number">5</span>, <span class="string">'min_samples_leaf'</span>: <span class="number">2</span>, <span class="string">'verbose'</span>: <span class="number">0</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># DecisionTree</span></span><br><span class="line">dt_parameters = &#123;<span class="string">'max_depth'</span>:<span class="number">8</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># KNeighbors</span></span><br><span class="line">knn_parameters = &#123;<span class="string">'n_neighbors'</span>:<span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># SVM</span></span><br><span class="line">svm_parameters = &#123;<span class="string">'kernel'</span>:<span class="string">'linear'</span>, <span class="string">'C'</span>:<span class="number">0.025</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># XGB</span></span><br><span class="line">gbm_parameters = &#123;<span class="string">'n_estimators'</span>: <span class="number">2000</span>, <span class="string">'max_depth'</span>: <span class="number">4</span>, <span class="string">'min_child_weight'</span>: <span class="number">2</span>, <span class="string">'gamma'</span>:<span class="number">0.9</span>, <span class="string">'subsample'</span>:<span class="number">0.8</span>, </span><br><span class="line">               <span class="string">'colsample_bytree'</span>:<span class="number">0.8</span>, <span class="string">'objective'</span>: <span class="string">'binary:logistic'</span>, <span class="string">'nthread'</span>:<span class="number">-1</span>, <span class="string">'scale_pos_weight'</span>:<span class="number">1</span>&#125;</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">title = <span class="string">"Learning Curves"</span></span><br><span class="line">plot_learning_curve(RandomForestClassifier(**rf_parameters), title, X, Y, cv=<span class="keyword">None</span>,  n_jobs=<span class="number">4</span>, train_sizes=[<span class="number">50</span>, <span class="number">100</span>, <span class="number">150</span>, <span class="number">200</span>, <span class="number">250</span>, <span class="number">350</span>, <span class="number">400</span>, <span class="number">450</span>, <span class="number">500</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2020/03/11/Kaggle-Titanic-生存预测/8.jpg" alt=""></p><p>提交结果终于到达了87左右了，后面还是可以尝试一下在模型融合这里慢慢加强。</p><p>但是 后面就是无尽无尽的调整参数的过程咯</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
      <category term="机器学习" scheme="http://wsx1128.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://wsx1128.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Kaggle" scheme="http://wsx1128.cn/tags/Kaggle/"/>
    
  </entry>
  
  <entry>
    <title>改善深层神经网络</title>
    <link href="http://wsx1128.cn/2020/03/11/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://wsx1128.cn/2020/03/11/改善深层神经网络/</id>
    <published>2020-03-11T11:36:10.000Z</published>
    <updated>2020-03-11T14:51:57.263Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="深度学习的相关实践"><a href="#深度学习的相关实践" class="headerlink" title="深度学习的相关实践"></a>深度学习的相关实践</h2><h3 id="数据集合"><a href="#数据集合" class="headerlink" title="数据集合"></a>数据集合</h3><p>紧接着前面 关于 欠拟合和过拟合的情况，所以这个地方直接将数据集分成了，训练，验证，测试集</p><p>然后这几个集合之间的定义就是：</p><p>训练集（train set）：用训练集对算法或模型进行训练过程；<br>验证集（development set）：利用验证集或者又称为简单交叉验证集（hold-out cross validation set）进行交叉验证，选择出最好的模型；<br>测试集（test set）：最后利用测试集对模型进行测试，获取模型运行的无偏估计。</p><p><strong>小数据时代</strong><br>在小数据量的时代，如：100、1000、10000的数据量大小，可以将data做以下划分：</p><p>无验证集的情况：70% / 30%；<br>有验证集的情况：60% / 20% / 20%；<br>通常在小数据量时代，以上比例的划分是非常合理的。</p><p><strong>大数据时代</strong><br>但是在如今的大数据时代，对于一个问题，我们拥有的data的数量可能是百万级别的，所以验证集和测试集所占的比重会趋向于变得更小。</p><p>验证集的目的是为了验证不同的算法哪种更加有效，所以验证集只要足够大能够验证大约2-10种算法哪种更好就足够了，不需要使用20%的数据作为验证集。如百万数据中抽取1万的数据作为验证集就可以了。</p><p>测试集的主要目的是评估模型的效果，如在单个分类器中，往往在百万级别的数据中，我们选择其中1000条数据足以评估单个模型的效果。</p><p>100万数据量：98% / 1% / 1%；<br>超百万数据量：99.5% / 0.25% / 0.25%（或者99.5% / 0.4% / 0.1%）</p><p><strong>注意</strong></p><p>建议验证集和测试集来自于同一个分布，这样可以使得机器学习算法变得更快；<br>如果不需要用无偏估计来评估模型的性能，则可以不需要测试集。</p><h3 id="偏差、方差"><a href="#偏差、方差" class="headerlink" title="偏差、方差"></a>偏差、方差</h3><p>对应上面过拟合，与欠拟合的情况，关于如何界定的问题，也在相应的章节中有讲述，所以这里就不再过多赘述了。</p><p>只需要记住一个小小的总结即可：</p><p>如果训练集误差与验证集误差相差过大，那么一定是出现了过拟合的问题，方差过大</p><p>如果 训练集误差与验证集误差不大，但是 二者的数值较大，那么一定是出现欠拟合的问题，偏差过大。</p><h3 id="解决上述问题的方法"><a href="#解决上述问题的方法" class="headerlink" title="解决上述问题的方法"></a>解决上述问题的方法</h3><ol><li><p>是否存在High bias ?<br>增加网络结构，如增加隐藏层数目；<br>训练更长时间；<br>寻找合适的网络架构，使用更大的NN结构；</p></li><li><p>是否存在High variance？<br>获取更多的数据；<br>正则化（ regularization）；<br>寻找合适的网络结构；</p></li></ol><p>在大数据时代，深度学习对监督式学习大有裨益，使得我们不用像以前一样太过关注如何平衡偏差和方差的权衡问题，通过以上方法可以使得再不增加另一方的情况下减少一方的值。</p><h3 id="正则化（惩罚参数）"><a href="#正则化（惩罚参数）" class="headerlink" title="正则化（惩罚参数）"></a>正则化（惩罚参数）</h3><p>利用正则化来解决High variance 的问题，正则化是在 Cost function 中加入一项正则化项，惩罚模型的复杂度。</p><p><img src="/2020/03/11/改善深层神经网络/1.png" alt=""></p><p><img src="/2020/03/11/改善深层神经网络/2.png" alt=""></p><p><img src="/2020/03/11/改善深层神经网络/3.png" alt=""></p><p><strong>那为什么正则化可以减少神经网络的过拟合情况呢</strong></p><p>对于神经网络的 Cost function：加入正则化项，直观上理解，正则化因子lambd设置的足够大的情况下，为了使代价函数最小化，权重矩阵 weight 就会被设置为接近于0的值。则相当于消除了很多神经元的影响，那么图中的大的神经网络就会变成一个较小的网络。</p><p>当然上面这种解释是一种直观上的理解，但是实际上隐藏层的神经元依然存在，但是他们的影响变小了，便不会导致过拟合。</p><p>而下面则是一种 较好的数学解释，可以拿来参考借鉴：</p><p><img src="/2020/03/11/改善深层神经网络/4.png" alt=""></p><p>这里还存在一种正则化的方式 Dropout正则化</p><p>Dropout（随机失活）就是在神经网络的Dropout层，为每个神经元结点设置一个随机消除的概率，对于保留下来的神经元，我们得到一个节点较少，规模较小的网络进行训练。</p><p>意思就是随机失活类型之类的</p><p>这里解释下为什么要有最后一步：a3 /= keep_prob</p><p>依照例子中的 keep_prob = 0.8 ，那么就有大约20%的神经元被删除了，也就是说 a3 中有20%的元素被归零了，在下一层的计算中有 z4 = w4 <em> a3 + b4 ，所以为了不影响 Z4 的期望值，所以需要W4 </em> a3 的部分除以一个keep_prob。</p><p>Inverted dropout 通过对“a3 /= keep_prob”,则保证无论 keep_prob 设置为多少，都不会对W4 * a3的期望值产生影响。</p><p>Notation：在测试阶段不要用dropout，因为那样会使得预测结果变得随机。</p><p><strong>理解Dropout</strong></p><p>另外一种对于Dropout的理解。</p><p>这里我们以单个神经元入手，单个神经元的工作就是接收输入，并产生一些有意义的输出，但是加入了Dropout以后，输入的特征都是有可能会被随机清除的，所以该神经元不会再特别依赖于任何一个输入特征，也就是说不会给任何一个输入设置太大的权重。</p><p>所以通过传播过程，dropout将产生和L2范数相同的收缩权重的效果。</p><p>对于不同的层，设置的keep_prob也不同，一般来说神经元较少的层，会设 keep_prob</p><p>=1.0，神经元多的层，则会将keep_prob设置的较小。</p><p>dropout的一大缺点就是其使得 Cost function不能再被明确的定义，以为每次迭代都会随机消除一些神经元结点，所以我们无法绘制出每次迭代cost函数下降的图</p><h4 id="其他正则化的方法"><a href="#其他正则化的方法" class="headerlink" title="其他正则化的方法"></a>其他正则化的方法</h4><p><strong>数据增强</strong>，将一些数据进行一些维度意义上面的增强</p><p><strong>归一化处理输入数据</strong><br>由图可以看出不使用归一化和使用归一化前后 Cost function 的函数形状会有很大的区别。</p><p>在不使用归一化的代价函数中，如果我们设置一个较小的学习率，那么很可能我们需要很多次迭代才能到达代价函数全局最优解；如果使用了归一化，那么无论从哪个位置开始迭代，我们都能以相对很少的迭代次数找到全局最优解。</p><p><strong>提前停止数据等</strong></p><p>如果神经网络过于深，那么就一定会发生 梯度消失的问题，<br>所以这个地方就涉及到应用初始化去缓解梯度消失的问题</p><p><img src="/2020/03/11/改善深层神经网络/5.png" alt=""></p><p><img src="/2020/03/11/改善深层神经网络/6.png" alt=""></p><h3 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h3><p>下面用前面一节的方法来进行梯度检验。</p><p><img src="/2020/03/11/改善深层神经网络/7.png" alt=""></p><p><img src="/2020/03/11/改善深层神经网络/8.png" alt=""></p><p>不要在训练过程中使用梯度检验，只在debug的时候使用，使用完毕关闭梯度检验的功能；<br>如果算法的梯度检验出现了错误，要检查每一项，找出错误，也就是说要找出哪个的值相差比较大；<br>不要忘记了正则化项；<br>梯度检验不能与dropout同时使用。因为每次迭代的过程中，dropout会随机消除隐层单元的不同神经元，这时是难以计算dropout在梯度下降上的代价函数J；<br>在随机初始化的时候运行梯度检验，或许在训练几次后再进行。（没听懂大师这个点的意思？）</p><h2 id="深度学习的优化算法"><a href="#深度学习的优化算法" class="headerlink" title="深度学习的优化算法"></a>深度学习的优化算法</h2><p>这个地方很多东西 在pytorch里面已经封装好了，不需要自己去实现，但是对于这一章而言，需要弄清楚的就是，如何去用，如何去调整里面超参数</p><h3 id="Mini-batch-梯度下降法"><a href="#Mini-batch-梯度下降法" class="headerlink" title="Mini-batch 梯度下降法"></a>Mini-batch 梯度下降法</h3><p>对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候，如有500万或5000万的训练数据，处理速度就会比较慢。</p><p>但是如果每次处理训练数据的一部分，即用其子集进行梯度下降，则我们的算法速度会执行的更快。而处理的这些一小部分训练子集即称为Mini-batch。</p><p>对于普通的梯度下降法，一个epoch只能进行一次梯度下降；而对于Mini-batch梯度下降法，一个epoch可以进行Mini-batch的个数次梯度下降。</p><p>不同size大小的比较</p><p>普通的batch梯度下降法和Mini-batch梯度下降法代价函数的变化趋势，如下图所示：</p><p><img src="/2020/03/11/改善深层神经网络/9.png" alt=""></p><ul><li><p>batch梯度下降：<br>  对所有m个训练样本执行一次梯度下降，每一次迭代时间较长；<br>  Cost function 总是向减小的方向下降。</p></li><li><p>随机梯度下降：<br>  对每一个训练样本执行一次梯度下降，但是丢失了向量化带来的计算加速；<br>  Cost function总体的趋势向最小值的方向下降，但是无法到达全局最小值点，呈现波动的形式。</p></li><li><p>Mini-batch梯度下降：<br>  选择一个 size 的合适的size进行Mini-batch梯度下降，可以实现快速学习，也应用了向量化带来的好处；<br>  Cost function的下降处于前两者之间。</p></li></ul><h4 id="Mini-batch-大小的选择"><a href="#Mini-batch-大小的选择" class="headerlink" title="Mini-batch 大小的选择"></a>Mini-batch 大小的选择</h4><p>如果训练样本的大小比较小时，如 m &lt;= 2000 时 ——— 选择batch梯度下降法；<br>如果训练样本的大小比较大时，典型的大小为： 2的次方 ；<br>Mini-batch的大小要符合CPU/GPU内存。</p><h3 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h3><p>这里主要有一个指数加权平均的关键函数：<br><img src="/2020/03/11/改善深层神经网络/12.png" alt=""></p><p><img src="/2020/03/11/改善深层神经网络/10.png" alt=""></p><p><img src="/2020/03/11/改善深层神经网络/11.png" alt=""></p><p>因为，在计算当前时刻的平均值，只需要前一天的平均值和当前时刻的值，所以在数据量非常大的情况下，指数加权平均在节约计算成本的方面是一种非常有效的方式，可以很大程度上减少计算机资源存储和内存的占用。</p><p>指数加权平均的偏差修正</p><p>在我们执行指数加权平均的公式时，当beta = 0.98时，我们得到的并不是图中的绿色曲线，而是下图中的紫色曲线，其起点比较低。</p><p><img src="/2020/03/11/改善深层神经网络/13.png" alt=""></p><p><img src="/2020/03/11/改善深层神经网络/14.png" alt=""></p><h3 id="动量（Momentum）梯度下降法"><a href="#动量（Momentum）梯度下降法" class="headerlink" title="动量（Momentum）梯度下降法"></a>动量（Momentum）梯度下降法</h3><p>前面介绍了一大啪啦听不懂的东西 实际上就是为后面的动量梯度下降法做贡献的</p><p>动量梯度下降的基本思想就是计算梯度的指数加权平均数，并利用该梯度来更新权重。</p><p>在我们优化 Cost function 的时候，以下图所示的函数图为例：</p><p><img src="/2020/03/11/改善深层神经网络/15.png" alt=""></p><p>在利用梯度下降法来最小化该函数的时候，每一次迭代所更新的代价函数值如图中蓝色线所示在上下波动，而这种幅度比较大波动，减缓了梯度下降的速度，而且我们只能使用一个<strong>较小的学习率</strong>来进行迭代。</p><p>如果用较大的学习率，结果可能会如紫色线一样偏离函数的范围，所以为了避免这种情况，只能用较小的学习率。</p><p>但是我们又希望在如图的纵轴方向梯度下降的缓慢一些，不要有如此大的上下波动，在横轴方向梯度下降的快速一些，使得能够更快的到达最小值点，而这里用动量梯度下降法既可以实现，如红色线所示。</p><p><strong>其最本质的解释</strong></p><p>在对应上面的计算公式中，将Cost function想象为一个碗状，想象从顶部往下滚球，其中：</p><p>微分项 dw, db 想象为球提供的加速度；<br>动量项 Vdw,Vdb 相当于速度；<br>小球在向下滚动的过程中，因为加速度的存在使得速度会变快，但是由于 beta 的存在，其值小于1，可以认为是摩擦力，所以球不会无限加速下去。</p><h3 id="RMSprop-算法"><a href="#RMSprop-算法" class="headerlink" title="RMSprop 算法"></a>RMSprop 算法</h3><p>除了上面所说的Momentum梯度下降法，RMSprop（root mean square prop）也是一种可以加快梯度下降的算法。</p><p><img src="/2020/03/11/改善深层神经网络/16.png" alt=""></p><p>这里假设参数b的梯度处于纵轴方向，参数w的梯度处于横轴方向（当然实际中是处于高维度的情况），利用RMSprop算法，可以减小某些维度梯度更新波动较大的情况，如图中蓝色线所示，使其梯度下降的速度变得更快，如图绿色线所示。</p><p>在如图所示的实现中，RMSprop将微分项进行平方，然后使用平方根进行梯度更新，同时为了确保算法不会除以0，平方根分母中在实际使用会加入一个很小的值如 [公式] 。</p><h3 id="Adam-优化算法"><a href="#Adam-优化算法" class="headerlink" title="Adam 优化算法"></a>Adam 优化算法</h3><p>Adam （Adaptive Moment Estimation）优化算法的基本思想就是将 Momentum 和 RMSprop 结合起来形成的一种适用于不同深度学习结构的优化算法。</p><p>这个弄懂的地方 实际上需要去看相关的数学证明才可以弄清楚</p><p>这里只介绍一些调试的办法</p><p><img src="/2020/03/11/改善深层神经网络/17.png" alt=""></p><h3 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h3><p>当这个地方使用学习率去衰减变化的时候，因为训练到后期的时候，如果再去使用大的学习率，容易造成不太好的影响</p><p>在我们利用 mini-batch 梯度下降法来寻找Cost function的最小值的时候，如果我们设置一个固定的学习速率 ，则算法在到达最小值点附近后，由于不同batch中存在一定的噪声，使得不会精确收敛，而一直会在一个最小值点较大的范围内波动，如下图中蓝色线所示。</p><p>但是如果我们使用学习率衰减，逐渐减小学习速率，在算法开始的时候，学习速率还是相对较快，能够相对快速的向最小值点的方向下降。但随着学习率的减小，下降的步伐也会逐渐变小，最终会在最小值附近的一块更小的区域里波动，如图中绿色线所示。</p><p><img src="/2020/03/11/改善深层神经网络/18.png" alt=""></p><h3 id="局部最优解"><a href="#局部最优解" class="headerlink" title="局部最优解"></a>局部最优解</h3><p>建议在构造模型的时候多去尝试，因为可能在构造过程中达到了局部最优解，而不是全局最优解，所以需要初始值选取的得当，才可以。</p><p>但是，如果我们建立一个高维度的神经网络。通常梯度为零的点，并不是如左图中的局部最优点，而是右图中的鞍点（叫鞍点是因为其形状像马鞍的形状）。</p><p><img src="/2020/03/11/改善深层神经网络/19.png" alt=""></p><p>在一个具有高维度空间的函数中，如果梯度为0，那么在每个方向，Cost function可能是凸函数，也有可能是凹函数。但如果参数维度为2万维，想要得到局部最优解，那么所有维度均需要是凹函数，其概率为特别特别小，可能性非常的小。也就是说，在低维度中的局部最优点的情况，并不适用于高维度，在梯度为0的点更有可能是鞍点，而不是局部最小值点。</p><p>在高纬度的情况下：</p><ul><li>几乎不可能陷入局部最小值点；</li><li>处于鞍点的停滞区会减缓学习过程，利用如Adam等算法进行改善。</li></ul><h2 id="超参数的调试和Batch-Norm"><a href="#超参数的调试和Batch-Norm" class="headerlink" title="超参数的调试和Batch Norm"></a>超参数的调试和Batch Norm</h2><h3 id="超阐述的调试处理"><a href="#超阐述的调试处理" class="headerlink" title="超阐述的调试处理"></a>超阐述的调试处理</h3><p>前面已经讲了很多超参数，这里就要集中进行一个处理过程</p><p>在机器学习领域，超参数比较少的情况下，我们之前利用设置网格点的方式来调试超参数；<br>但在深度学习领域，超参数较多的情况下，不是设置规则的网格点，而是随机选择点进行调试。这样做是因为在我们处理问题的时候，是无法知道哪个超参数是更重要的，所以随机的方式去测试超参数点的性能，更为合理，这样可以探究更超参数的潜在价值。</p><h3 id="为超参数选择合适的范围"><a href="#为超参数选择合适的范围" class="headerlink" title="为超参数选择合适的范围"></a>为超参数选择合适的范围</h3><p><strong>Scale均匀随机</strong></p><p>在超参数选择的时候，一些超参数是在一个范围内进行均匀随机取值，如隐藏层神经元结点的个数、隐藏层的层数等。但是有一些超参数的选择做均匀随机取值是不合适的，这里需要按照一定的比例在不同的小范围内进行均匀随机取值，以学习率的选择为例，在 0.001 到 1 范围内进行选择：</p><p>如上图所示，如果在0.001～1的范围内进行进行均匀随机取值，则有90%的概率 选择范围在0.1～1之间，而只有10%的概率才能选择到0.001～0.1之间，显然是不合理的。</p><p>所以在选择的时候，在不同比例范围内进行均匀随机取值，如 0.0001～0.001 、0.001～0.01、0.01～0.1， 0.1～1范围内选择。</p><p>同样，在使用指数加权平均的时候，超参数beta 也需要用上面这种方向进行选择。</p><h3 id="超参数调试实践—Pandas-vs-Caviar"><a href="#超参数调试实践—Pandas-vs-Caviar" class="headerlink" title="超参数调试实践—Pandas vs. Caviar"></a>超参数调试实践—Pandas vs. Caviar</h3><p>不要看名字起的这么高端，其实就是一句话<br>在超参数调试的实际操作中，我们需要根据我们现有的计算资源来决定以什么样的方式去调试超参数，进而对模型进行改进。下面是不同情况下的两种方式：</p><p>在计算资源有限的情况下，使用第一种，仅调试一个模型，每天不断优化；<br>在计算资源充足的情况下，使用第二种，同时并行调试多个模型，选取其中最好的模型。</p><h3 id="网络中激活值的归一化"><a href="#网络中激活值的归一化" class="headerlink" title="网络中激活值的归一化"></a>网络中激活值的归一化</h3><p>在Logistic Regression 中，将输入特征进行归一化，可以加速模型的训练。那么对于更深层次的神经网络，我们是否可以归一化隐藏层的输出al或者经过激活函数前的zl ，以便加速神经网络的训练过程？答案是肯定的。</p><p>常用的方式是将隐藏层的经过激活函数前的zl进行归一化。</p><p>比如卷积网络和RNN中如果应用这个的化，那么就会造成不需要bias的情况，因为其已经将bias的情况给剔除了。</p><p><img src="/2020/03/11/改善深层神经网络/20.png" alt=""></p><p>这里不太需要去弄清楚其的底层实现，pytorch已经将其封装好了，可以直接去使用了</p><p>可以直接加入到输入函数和激活函数之间就可以了</p><h3 id="Batch-Norm-起作用的原因"><a href="#Batch-Norm-起作用的原因" class="headerlink" title="Batch Norm 起作用的原因"></a>Batch Norm 起作用的原因</h3><h4 id="First-Reason"><a href="#First-Reason" class="headerlink" title="First Reason"></a>First Reason</h4><p>首先Batch Norm 可以加速神经网络训练的原因和输入层的输入特征进行归一化，从而改变Cost function的形状，使得每一次梯度下降都可以更快的接近函数的最小值点，从而加速模型训练过程的原理是有相同的道理。</p><p>只是Batch Norm 不是单纯的将输入的特征进行归一化，而是将各个隐藏层的激活函数的激活值进行的归一化，并调整到另外的分布。</p><h4 id="Second-Reason"><a href="#Second-Reason" class="headerlink" title="Second Reason"></a>Second Reason</h4><p>Batch Norm 可以加速神经网络训练的另外一个原因是它可以使权重比网络更滞后或者更深层。</p><p>下面是一个判别是否是猫的分类问题，假设第一训练样本的集合中的猫均是黑猫，而第二个训练样本集合中的猫是各种颜色的猫。如果我们将第二个训练样本直接输入到用第一个训练样本集合训练出的模型进行分类判别，那么我们在很大程度上是无法保证能够得到很好的判别结果。</p><p>这是因为第一个训练集合中均是黑猫，而第二个训练集合中各色猫均有，虽然都是猫，但是很大程度上样本的分布情况是不同的，所以我们无法保证模型可以仅仅通过黑色猫的样本就可以完美的找到完整的决策边界。第二个样本集合相当于第一个样本的分布的改变，称为：Covariate shift。</p><p>上面的意思 说人话就是 每一个集合的协和特征得出来，进行归纳即可，意思就是保存了数据之间的均值和方差，不会被下一个不一样的集合所影响</p><h3 id="Batch-Norm-正则化效果"><a href="#Batch-Norm-正则化效果" class="headerlink" title="Batch Norm 正则化效果"></a>Batch Norm 正则化效果</h3><p>Batch Norm还有轻微的正则化效果。</p><p>这是因为在使用Mini-batch梯度下降的时候，每次计算均值和偏差都是在一个Mini-batch上进行计算，而不是在整个数据样集上。这样就在均值和偏差上带来一些比较小的噪声。那么用均值和偏差计算得到的 Zl 也将会加入一定的噪声。</p><p>所以和Dropout相似，其在每个隐藏层的激活值上加入了一些噪声，（这里因为Dropout以一定的概率给神经元乘上0或者1）。所以和Dropout相似，Batch Norm 也有轻微的正则化效果。</p><p>这里引入一个小的细节就是，如果使用Batch Norm ，那么使用大的Mini-batch如256，相比使用小的Mini-batch如64，会引入跟少的噪声，那么就会减少正则化的效果。</p><h3 id="在测试数据上使用-Batch-Norm"><a href="#在测试数据上使用-Batch-Norm" class="headerlink" title="在测试数据上使用 Batch Norm"></a>在测试数据上使用 Batch Norm</h3><p><img src="/2020/03/11/改善深层神经网络/21.png" alt=""></p><h3 id="Softmax-回归"><a href="#Softmax-回归" class="headerlink" title="Softmax 回归"></a>Softmax 回归</h3><p>在多分类问题中，有一种 logistic regression的一般形式，叫做Softmax regression。Softmax回归可以将多分类任务的输出转换为各个类别可能的概率，从而将最大的概率值所对应的类别作为输入样本的输出类别。</p><p>计算公式</p><p>下图是Softmax的公式以及一个简单的例子：</p><p><img src="/2020/03/11/改善深层神经网络/22.png" alt=""></p><h3 id="训练-Sotfmax-分类器"><a href="#训练-Sotfmax-分类器" class="headerlink" title="训练 Sotfmax 分类器"></a>训练 Sotfmax 分类器</h3><p>为什么叫做Softmax？我们以前面的例子为例，</p><p>通常我们判定模型的输出类别，是将输出的最大值对应的类别判定为该模型的类别，也就是说最大值为的位置1，其余位置为0，这也就是所谓的“hardmax”。而Sotfmax将模型判定的类别由原来的最大数字5，变为了一个最大的概率0.842，这相对于“hardmax”而言，输出更加“soft”而没有那么“hard”。</p><p>Sotfmax回归 将 logistic回归 从二分类问题推广到了多分类问题上。<br>说白了与全链接层类似。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
      <category term="机器学习" scheme="http://wsx1128.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://wsx1128.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://wsx1128.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://wsx1128.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>逻辑回归--深度学习网络雏型</title>
    <link href="http://wsx1128.cn/2020/03/10/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%BB%9C%E9%9B%8F%E5%9E%8B/"/>
    <id>http://wsx1128.cn/2020/03/10/逻辑回归-深度学习网络雏型/</id>
    <published>2020-03-10T08:31:27.000Z</published>
    <updated>2020-03-11T02:35:19.631Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>前面有讲过，相应的线性回归方程，一般用在对于数据的拟合和预测上，甚至还是可以放下多维度的数字。</p><p>而这里要总结的就是 逻辑回归模型，虽然这里命名为回归，其实是一个分类问题。</p><p>比如邮件的归类，以及在线的资源判断，或者是医学上面的肿瘤 良性以及恶性的判断。</p><p>一般很常用的 分类函数里面有 sigmoid函数，或者深度学习中的Tanh</p><p><img src="/2020/03/10/逻辑回归-深度学习网络雏型/1.png" alt=""></p><p>另外在机器学习中，逻辑回归，还负责一类问题，就是将多个数据群进行一个有效的划分，如果拟合的函数 太过于欠拟合的话，考虑采用复杂的函数来拟合</p><p><img src="/2020/03/10/逻辑回归-深度学习网络雏型/2，png" alt=""></p><p>甚至还会存在非线性的分界线情况，就像如下：<br><img src="/2020/03/10/逻辑回归-深度学习网络雏型/3.png" alt=""></p><p>目前这个地方只会去介绍一下 二元分类的问题，后面再去涉及到多元分类的问题</p><p>另外 关于其梯度下降的相关知识点，由于pytorch框架已经给封装好了，所以这里不多在赘述了，与前面的线性回归的梯度下降类似，就是换了一个方程而已<br><img src="/2020/03/10/逻辑回归-深度学习网络雏型/7.png" alt=""></p><p>另外，在机器学习的课程李，给出了一对多的分类方式，如果这样分类的话，每次只看一种数据，并且将其与所有的物质进行相反的链接</p><p>就像下面的图所示：</p><p><img src="/2020/03/10/逻辑回归-深度学习网络雏型/4.png" alt=""></p><p>这里又提到一个过拟合的概念，这个概念也是在后面机器学习甚至是深度学习中反复提到的，处理过拟合的方法，这个具体就在后面说，这里只会提到一点点。</p><p>具体就两种办法，要么正则化，要么就直接减少特征数（机器学习数据拟合中的特征是自己去设置的）</p><p>机器学习中 过渡到深度学习神经网络的构建是直接从非线性回归问题，直接去分类。</p><p><img src="/2020/03/10/逻辑回归-深度学习网络雏型/5.png" alt=""></p><p>上面的每一个节点都是相同的运算（拟合运算 + 激活函数） 在后面深度学习中 这个是分开的。</p><p>正是因为有神经网络这样的结构在，正好就可以满足多元分类，最后输出的节点就可以来判断出 多个分类对象。<br>（注意，这里 机器学习课程上面讲述的是hardmax是硬性分类，而一般采用的是softmax 求出概率而不是直接得出结果）</p><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>首先 先必须注意一点，输入到神经网络中数据的形状 一定是 batch_size * n(特征)</p><p>基于上面 逻辑回归，梯度下降，前向传播，反向传播，构建神经网络</p><p>下面需要弄清楚的有一点就是 python的notation</p><p>虽然在Python有广播的机制，但是在Python程序中，为了保证矩阵运算的正确性，可以使用reshape()函数来对矩阵设定所需要进行计算的维度，这是个好的习惯；</p><p>如果用下列语句来定义一个向量，则这条语句生成的a的维度为（5，），既不是行向量也不是列向量，称为秩（rank）为1的array，如果对a进行转置，则会得到a本身，这在计算中会给我们带来一些问题。</p><p>其实也可以在pytorch中使用view来进行一个更改，这里也不多赘述了，因为毕竟是基础中的基础。</p><h3 id="浅层网络"><a href="#浅层网络" class="headerlink" title="浅层网络"></a>浅层网络</h3><p>除输入层之外每层的计算输出可由下图总结出：</p><p><img src="/2020/03/10/逻辑回归-深度学习网络雏型/8.png" alt=""></p><p>其中，每个结点都对应这两个部分的运算，z运算和a运算。 在编程中，我们使用向量化去计算神经网络的输出：</p><p>假定在m个训练样本的神经网络中，计算神经网络的输出，用向量化的方法去实现可以避免在程序中使用for循环，提高计算的速度。</p><p>向量化，说白了就是线性代数的另外一种说法而已</p><p>另外需要纠结的是 激活函数的选择：</p><p>sigmoid函数和tanh函数比较：</p><p>隐藏层：tanh函数的表现要好于sigmoid函数，因为tanh取值范围为 -1～+1 ，输出分布在0值的附近，均值为0，从隐藏层到输出层数据起到了归一化（均值为0）的效果。<br>输出层：对于二分类任务的输出取值为 0～1 ，故一般会选择sigmoid函数。<br>然而sigmoid和tanh函数在当 z 很大的时候，梯度会很小，在依据梯度的算法中，更新在后期会变得很慢。在实际应用中，要使 [公式] 尽可能的落在0值附近。</p><p>ReLU弥补了前两者的缺陷，当 z 时，梯度始终为1，从而提高神经网络基于梯度算法的运算速度。然而当 [公式] 时，梯度一直为0，但是实际的运用中，该缺陷的影响不是很大。</p><p>Leaky ReLU保证在 z &lt; 0 的时候，梯度仍然不为0。</p><p>在选择激活函数的时候，如果在不知道该选什么的时候就选择ReLU，当然也没有固定答案，要依据实际问题在交叉验证集合中进行验证分析。</p><p>对于神经网络中的梯度下降法 就是直接用框架就可以去解决。</p><p>最后，初始化问题会在后面优化神经网络的办法中详细的去讲述，这里只是随机初始化，是最简单的一种初始化办法</p><p>如果在初始时，两个隐藏神经元的参数设置为相同的大小，那么两个隐藏神经元对输出单元的影响也是相同的，通过反向梯度下降去进行计算的时候，会得到同样的梯度大小，所以在经过多次迭代后，两个隐藏层单位仍然是对称的。无论设置多少个隐藏单元，其最终的影响都是相同的，那么多个隐藏神经元就没有了意义。</p><p>在初始化的时候， W 参数要进行随机初始化， b 则不存在对称性的问题它可以设置为0。 以2个输入，2个隐藏神经元为例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = np.random.rand((<span class="number">2</span>,<span class="number">2</span>))* <span class="number">0.01</span></span><br><span class="line">b = np.zero((<span class="number">2</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>这里我们将W的值乘以0.01是为了尽可能使得权重W初始化为较小的值，这是因为如果使用sigmoid函数或者tanh函数作为激活函数时，W比较小，则 Z = WX + b所得的值也比较小，处在0的附近，0点区域的附近梯度较大，能够大大提高算法的更新速度。而如果W设置的太大的话，得到的梯度较小，训练过程因此会变得很慢。</p><p>ReLU和Leaky ReLU作为激活函数时，不存在这种问题，因为在大于0的时候，梯度均为1。</p><h3 id="深度网络"><a href="#深度网络" class="headerlink" title="深度网络"></a>深度网络</h3><p>在深度网络中，因为现在有框架可以去帮助coding，以及集成好的相关操作<br>但是自己需要注意的是四点</p><ol><li>矩阵的维度，一定要在神经网络里面对应好</li></ol><p><img src="/2020/03/10/逻辑回归-深度学习网络雏型/9.png" alt=""></p><ol><li><p>深度学习的一些建构好的模型，比如后面将要学习到的CNN 以及 RNN 这都是后面将会学习到的</p></li><li><p>前向传播，以及反向传播的时候不要写错，虽然现在有框架帮忙，可以减少很多体力活，但是需要注意的是在构建自己模型的时候需要去好好深思一下</p></li><li><p>参数，超参数。 这估计就是深度学习中最核心的话题了，因为基本上一大部分时间都回去调整一下参数，</p></li></ol><p><img src="/2020/03/10/逻辑回归-深度学习网络雏型/10.png" alt=""></p><p>以上就是从逻辑回归到达 深度学习基础的内容，下一篇博客就会主要讲在构建深度学习的过程中，需要去改善，优化。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
      <category term="机器学习" scheme="http://wsx1128.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://wsx1128.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://wsx1128.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://wsx1128.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Deep learning of Structuring Machine Learning Projects</title>
    <link href="http://wsx1128.cn/2020/03/06/Deep-learning-of-Structuring-Machine-Learning-Projects/"/>
    <id>http://wsx1128.cn/2020/03/06/Deep-learning-of-Structuring-Machine-Learning-Projects/</id>
    <published>2020-03-05T17:33:18.000Z</published>
    <updated>2020-03-05T17:35:13.763Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>首先声明 本篇所涉及到的知识点 全部来自于 Courase deeplearning.ai Structuring Machine Learning Projects week one —- week two</p><p>这一门课没有太多的实践内容，帮助构建自己的机器学习项目，错误分析已经相关优化，已经调整参数，介绍一些常用的策略，也算是深度学习中非常重要的课程了。</p><h2 id="Orthogonalization（正交化）"><a href="#Orthogonalization（正交化）" class="headerlink" title="Orthogonalization（正交化）"></a>Orthogonalization（正交化）</h2><p>我们在机器学习模型建立的整个流程中，我们需要根据不同部分反映的问题，去做相应的调整，从而更加容易地判断出是在哪一个部分出现了问题，并做相应的解决措施。</p><p>正交化或正交性是一种系统设计属性，其确保修改算法的指令或部分不会对系统的其他部分产生或传播副作用。 相互独立地验证使得算法变得更简单，减少了测试和开发的时间。</p><p>而正交化表现在四个方面：</p><ol><li><p>系统在训练集上表现的好<br> 否则使用更大的神经网络、更好的优化算法</p></li><li><p>系统在开发集上表现的好<br> 否则使用正则化、更大的训练集</p></li><li><p>系统在测试集上表现的好<br> 否则使用更大的开发集</p></li><li><p>在真实的系统环境中表现的好<br> 否则修改开发测试集、修改代价函数</p></li></ol><h2 id="setting-your-goal-单一数字评估指标"><a href="#setting-your-goal-单一数字评估指标" class="headerlink" title="setting your goal(单一数字评估指标)"></a>setting your goal(单一数字评估指标)</h2><p>在训练机器学习的模型的时候，无论是调整超参数，还是尝试更好的办法，为你的目标设置一个评估指标，这样有利于你在这个方向上面前进</p><p>在二分类问题之中，通过预测可以得到真实值以及下面的预测值的表:</p><p><img src="/2020/03/06/Deep-learning-of-Structuring-Machine-Learning-Projects/1.jpg" alt=""></p><p>下面就是指标的计算方法，一般都是用这个来对付二元分类问题</p><p><img src="/2020/03/06/Deep-learning-of-Structuring-Machine-Learning-Projects/2.png" alt=""></p><p>那么 如果在多元问题的分类上面呢？如果仅仅是上面的查准率以及查全率的化反而不太准确。</p><p>如果是多元问题的化，那么就采用地区的平均值来对模型效果进行评估。</p><p><img src="/2020/03/06/Deep-learning-of-Structuring-Machine-Learning-Projects/3.jpg" alt=""></p><p>该模型在各个地区有着不同的表现，这里用地区平均值对模型效果进行评估的化，反而会显示的性能好一点。</p><h2 id="满足和优化指标"><a href="#满足和优化指标" class="headerlink" title="满足和优化指标"></a>满足和优化指标</h2><p><img src="/2020/03/06/Deep-learning-of-Structuring-Machine-Learning-Projects/4.png" alt=""></p><h2 id="训练、开发、测试集"><a href="#训练、开发、测试集" class="headerlink" title="训练、开发、测试集"></a>训练、开发、测试集</h2><ul><li>训练、开发、测试集的设置会对产品带来非常大的影响</li><li>在选择开发集和测试集时要使二者来自同一分布，且从所有数据中随机选取</li><li>所选择的开发集和测试集中的数据，要与未来想要或者能够得到的数据类似，即模型数据和未来数据要具有相似性</li><li>设置的测试集只要足够大，使其能够在过拟合的系统中给出高方差的结果就可以，也许10000左右的数目足够</li><li>设置开发集只要足够使其能够检测不同算法、不同模型之间的优劣差异就可以，百万大数据中1%的大小就足够</li></ul><p>总之 只需要记住三者之间的比例，一般较小的数据中，训练60% 然后开发、测试占%20<br>如果是深度学习，大数据的项目，那么训练980000:10000:10000 这样的比例就行了</p><p>训练集 训练模型<br>开发集 选取模型，以及模型调优<br>测试集 选择解决问题最适合的模型<br><strong>有的时候没有测试集，直接用在了测试集中</strong></p><h2 id="根据需要改变指标"><a href="#根据需要改变指标" class="headerlink" title="根据需要改变指标"></a>根据需要改变指标</h2><p>在针对某一问题我们设置开发集和评估指标后，这就像把目标定在某个位置，后面的过程就聚焦在该位置上。但有时候在这个项目的过程中，可能会发现目标的位置设置错了，所以要移动改变我们的目标。</p><p>比如课程中举出的相应的例子</p><p>评估指标：分类错误率<br>算法A： 3% 错误率<br>算法B： 5% 错误率</p><p>这样来看，算法A的表现更好。但是在实际的测试中，算法A可能因为某些原因，将很多色情图片分类成了猫。所以当我们在线上部署的时候，算法A会给爱猫人士推送更多更准确的猫的图片（因为其误差率只有 3% ），但同时也会给用户推送一些色情图片，这是不能忍受的。所以，虽然算法A的错误率很低，但是它却不是一个好的算法。</p><p>这个时候我们就需要改变开发集、测试集或者评估指标。</p><p>假设开始我们的评估指标如下：</p><p><img src="/2020/03/06/Deep-learning-of-Structuring-Machine-Learning-Projects/5.png" alt=""></p><p>该评估指标对色情图片和非色情图片一视同仁，但是我们希望，分类器不会错误将色情图片标记为猫。</p><p>修改的方法，在其中加入权重w ：</p><p><img src="/2020/03/06/Deep-learning-of-Structuring-Machine-Learning-Projects/6.png" alt=""></p><p>总结来说就是：如果评估指标无法正确评估算法的排名，则需要重新定义一个新的评估指标。</p><h2 id="与人类表现进行比较"><a href="#与人类表现进行比较" class="headerlink" title="与人类表现进行比较"></a>与人类表现进行比较</h2><p>假设针对两个问题分别具有相同的训练误差和交叉验证误差，如下所示：</p><p>对于左边的问题，人类的误差为 1% ，对于右边的问题，人类的误差为 7.5%。</p><p>对于某些任务如计算机视觉上，人类能够做到的水平和贝叶斯误差相差不远。（这里贝叶斯误差指最好的分类器的分类误差，也就是说没有分类器可以做到100%正确）。这里将人类水平误差近似为贝叶斯误差。</p><p>左边的例子： 8% 与 1% 差距较大<br>主要着手减少偏差，即减少训练集误差和人类水平误差之间的差距，来提高模型性能。</p><p>右边的例子： 8% 与 7.5% 接近<br>主要着手减少方差，即减少开发集误差和测试集误差之间的差距，来提高模型性能。</p><p>对人类水平误差有一个大概的估计，可以让我们去估计贝叶斯误差，这样可以让我们更快的做出决定：减少偏差还是减少方差。</p><p>而这个决策技巧通常都很有效果，直到系统的性能开始超越人类，那么我们对贝叶斯误差的估计就不再准确了，再从减少偏差和减少方差方面提升系统性能就会比较困难了。</p><p>如果二者与人类表现相差过大，那么应该改善偏差，如果训练集与开发集相差过大，那么应该改善方差</p><h2 id="改善模型的表现"><a href="#改善模型的表现" class="headerlink" title="改善模型的表现"></a>改善模型的表现</h2><p>根据课程上面提到的例子，以及进行的总结来看</p><p>减少偏差的办法：</p><ul><li>训练更大的模型</li><li>训练更长时间、训练更好的优化算法（Momentum、RMSprop、Adam）</li><li>寻找更好的网络架构（RNN、CNN）、寻找更好的超参数</li></ul><p>减少方差的办法：</p><ul><li>收集更多的数据</li><li>正则化（L2、dropout、数据增强）</li><li>寻找更好的网络架构（RNN、CNN）、寻找更好的超参数</li></ul><p><img src="/2020/03/06/Deep-learning-of-Structuring-Machine-Learning-Projects/7.jpg" alt=""></p><h3 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h3><p>当我们在训练一个模型的时候，如一个猫和狗分类模型，最终得到了90%的精确度，即有10%的错误率。所以我们需要对模型的一些部分做相应调整，才能更好地提升分类的精度。</p><p>如果不加分析去做，可能几个月的努力对于提升精度并没有作用。所以一个好的误差分析的流程就相当重要。</p><p><strong>下面就是错误分析的样例</strong></p><h4 id="收集错误样例"><a href="#收集错误样例" class="headerlink" title="收集错误样例"></a>收集错误样例</h4><p>在开发集（测试集）中，获取大约100个错误标记的例子，并统计其中有多少个是狗。</p><ul><li><p>假设一种情况是100个数据中，有5个样例是狗，那么如果我们对数据集的错误标记做努力去改进模型的精度，那么可以提升的上限就是 5%，即仅仅可以达到 95% 的准确率，这有时称为性能上限。那么这种情况下，可能这样耗时的努力方向就不是很值得的一件事情。</p></li><li><p>另外一种假设是100个数据中，有50多个样例是狗，那么这种情况下，我们去改进数据集的错误标记，就是一个比较值得的改进方向，可以将模型的精确度提升至 95% 。</p></li></ul><h4 id="并行分析"><a href="#并行分析" class="headerlink" title="并行分析"></a>并行分析</h4><ul><li>修改那些被分类成猫的狗狗图片标签</li><li>修改那些被错误分类的大型猫科动物，如：狮子，豹子等</li><li>提升模糊图片的质量。</li></ul><p>为了并行的分析，建立表格来进行。以单个错误分类样本为对象，分析每个样本错误分类的原因。</p><p>最后，统计错误类型的百分比，这个分析步骤可以给我们一个粗略的估计，让我们大致确定是否值得去处理每个不同的错误类型。</p><h4 id="清楚错误标记的样本"><a href="#清楚错误标记的样本" class="headerlink" title="清楚错误标记的样本"></a>清楚错误标记的样本</h4><p>下面还是以猫和狗分类问题为例子，来进行分析。如下面的分类中的几个样本：（来源于视频中的样例）</p><p>情况一：</p><p>深度学习算法对训练集中的随机误差具有相当的包容性。</p><p>只要我们标记出错的例子符合随机误差，如：做标记的人不小心错误，或按错分类键。那么像这种随机误差导致的标记错误，一般来说不管这些误差可能也没有问题。</p><p>情况二：</p><p>虽然深度学习算法对随机误差具有很好的包容性，但是对于系统误差就不是这样了。</p><p>如果做标记的人一直把如例子中的白色的狗标记成猫，那么最终导致我们的分类器就会出现错误。</p><p>dev、test集中错误标记的情况：</p><p>如果在开发集和测试集中出现了错误标记的问题，我们可以在误差分析的过程中，增加错误标记这一原因，再对错误的数据进行分析，得出修正这些标记错误的价值。</p><p><strong>修正开发、测试集上面的错误样例</strong></p><ul><li><p>对开发集和测试集上的数据进行检查，确保他们来自于相同的分布。使得我们以开发集为目标方向，更正确地将算法应用到测试集上。</p></li><li><p>考虑分类正确的成本</p></li><li><p>训练集和开发集 来自不同的分布</p></li></ul><h4 id="搭建系统"><a href="#搭建系统" class="headerlink" title="搭建系统"></a>搭建系统</h4><ul><li>设置开发、测试集和优化指标（确定方向）</li><li>快速地建立基本的系统</li><li>使用偏差方差分析、误差分析去确定后面步骤的优先步骤</li></ul><p>总的来说，如果我们想建立自己的深度学习系统，我们就需要做到：快速的建立自己的基本系统，并进行迭代。而不是想的太多，在一开始就建立一个非常复杂，难以入手的系统。</p><h4 id="不同分布上的训练以及测试"><a href="#不同分布上的训练以及测试" class="headerlink" title="不同分布上的训练以及测试"></a>不同分布上的训练以及测试</h4><p>在深度学习的时代，因为需求的数据量非常大，现在很多的团队，使用的训练数据都是和开发集和测试集来自不同的分布。</p><p>下面是一些处理训练集和测试集存在差异的最佳的做法。以前一周中的猫的分类问题为例：</p><p><img src="/2020/03/06/Deep-learning-of-Structuring-Machine-Learning-Projects/8.jpg" alt=""></p><p>我们可以从网上获取大量的高清晰的猫的图片去做分类，如200000张，但是只能获取少量利用手机拍摄的不清晰的图片，如10000张。但是我们系统的目的是应用到手机上做分类。</p><p>也就是说，我们的训练集和开发集、测试集来自于不同的分布。</p><p><strong>方法一：</strong></p><p>将两组数据合并到一起，总共得到21万张图片样本。将这些样本随机分配到训练、开发、测试集中。</p><p>好处：三个集合中的数据均来自于同一分布；<br>坏处： 我们设立开发集的目的是瞄准目标，而现在我们的目标绝大部分是为了去优化网上获取高清晰度的照片，而不是我们真正的目标。</p><p>该方法不是一个好的方法，不推荐。</p><p><strong>方法二：</strong></p><p>训练集均是来自网上下载的20万张高清图片，当然也可以加上5000张手机非高清图片；对于开发和测试集都是手机非高清图片。</p><p>好处：开发集全部来自手机图片，瞄准目标；<br>坏处：训练集和开发、测试集来自不同的分布。</p><p>从长期来看，这样的分布能够给我们带来更好的系统性能。</p><h4 id="不同分布上的方差与偏差"><a href="#不同分布上的方差与偏差" class="headerlink" title="不同分布上的方差与偏差"></a>不同分布上的方差与偏差</h4><p>通过估计学习算法的偏差和方差，可以帮助我们确定接下来应该优先努力的方向。但是当我们的训练集和开发、测试集来自不同的分布时，分析偏差和方差的方式就有一定的不同。</p><p><strong>方差和分布原由分析</strong></p><p>以猫分类为例，假设以人的分类误差 0% 作为贝叶斯误差。若我们模型的误差为</p><p>Training error： 1%<br>Dev error： 10%</p><p>如果我们的训练集和开发、测试集来自相同的分布，那么我们可以说模型存在很大的方差问题。但如果数据来自不同的分布，那么我们就不能下这样的定论了。</p><p>那么我们如何去确定是由于分布不匹配的问题导致开发集的误差，还是由于算法中存在的方差问题所致？</p><p><strong>设立“训练开发集”</strong></p><p>训练开发集，其中的数据和训练数据来自同一分布，但是却不用于训练过程。</p><p>如果最终，我们的模型得到的误差分别为：</p><p>Training error： 1%<br>Training-dev error： 9%<br>Dev error： 10%<br>那么，由于训练开发集尽管和训练集来自同一分布，但是却有很大的误差， 模型无法泛化到同分布的数据，那么说明我们的模型存在方差问题。</p><p>但如果我们的模型得到的误差分别为：</p><p>Training error： 1%<br>Training-dev error： 1.5%<br>Dev error： 10%</p><p>现在就可以明确看出来，前者是模型存在方差问题，而后者可能就是分布不匹配导致的问题</p><p><strong>分布不同的偏差方差分析</strong></p><p>通过：Human level、Training set error、Training-dev set error、Dev error、Test error 之间误差的大小，可以分别得知我们的模型，需要依次在：可避免的偏差、方差、数据分布不匹配、开发集的或拟合程度，这些方面做改进。</p><p><img src="/2020/03/06/Deep-learning-of-Structuring-Machine-Learning-Projects/9.jpg" alt=""></p><p>通常情况下来说，通过不同的集合上的误差分析，我们得出的结果会是中间一列误差由小变大，即误差上升的情况。但是也有一定的可能会出现右边一列误差在开发测试集上又表现的好的情况。</p><p>下面通过一个后视镜语音检测的例子来说明。我们以该例子建立更加一般的表格。</p><p><img src="/2020/03/06/Deep-learning-of-Structuring-Machine-Learning-Projects/10.jpg" alt=""></p><p>其中，横向分别是：普通语音识别数据、后视镜语音识别数据；纵向分别是：Human level、训练数据误差、未训练数据误差。表格中不同的位置分别代表不同的数据集。</p><p>通常情况下，我们分析误差会是一个递增的情况，但是对于我们的模型，在后视镜语音识别的数据数据上，可能已经可以达到人类水平误差的6%了，而最终的开发测试集也会6%的误差，要比训练误差和训练开发误差都要小。所以如果遇到这种情况，就要利用上表进行分析。</p><h4 id="解决数据分布不匹配问题"><a href="#解决数据分布不匹配问题" class="headerlink" title="解决数据分布不匹配问题"></a>解决数据分布不匹配问题</h4><p>如果通过上一节的误差分析，我们可以得知，模型最终在开发和测试集上的误差最终是由于数据分布不匹配而导致。那么这样的情况下如何解决？</p><p>进行人工误差分析，尝试去了解训练集和开发测试集的具体差异在哪里。如：噪音等；<br>尝试把训练数据变得更像开发集，或者收集更多的类似开发集和测试集的数据，如增加噪音；</p><h4 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h4><p>将从一个任务中学到的知识，应用到另一个独立的任务中。</p><p><img src="/2020/03/06/Deep-learning-of-Structuring-Machine-Learning-Projects/11.jpg" alt=""></p><p>迁移学习适合以下场合：迁移来源问题有很多数据，但是迁移目标问题却没有那么多的数据。</p><p>假设图像识别任务中有1百万个样本，里面的数据相当多；但对与一些特定的图像识别问题，如放射科图像，也许只有一百个样本，所以对于放射学诊断问题的数据很少。所以从图像识别训练中学到的很多知识可以迁移，来帮助我们提升放射科识别任务的性能。</p><p>同样一个例子是语音识别，可能在普通的语音识别中，我们有庞大的数据量来训练模型，所以模型从中学到了很多人类声音的特征。但是对于触发字检测任务，可能我们拥有的数据量很少，所以对于这种情况下，学习人类声音特征等知识就显得相当重要。所以迁移学习可以帮助我们建立一个很好的唤醒字检测系统。</p><p>其的意义就在于</p><p>任务A和任务B有着相同的输入；<br>任务A所拥有的数据要远远大于任务B（对于更有价值的任务B，任务A所拥有的数据要比B大很多）；<br>任务A的低层特征学习对任务B有一定的帮助。</p><h4 id="多任务学习"><a href="#多任务学习" class="headerlink" title="多任务学习"></a>多任务学习</h4><p>与迁移学习的串行学习方式不同，在多任务学习中，多个任务是并行进行学习的，同时希望各个任务对其他的任务均有一定的帮助。</p><p>假设在自动驾驶的例子中，我们需要检测的物体很多，如行人、汽车、交通灯等等。</p><p>对于现在的任务，我们的目标值变成了一个向量的形式向量中的每一个值代表检测到是否有如行人、汽车、交通灯等，一张图片有多个标签。</p><p><img src="/2020/03/06/Deep-learning-of-Structuring-Machine-Learning-Projects/12.jpg" alt=""></p><p>模型的神经网络结构如下图所示：</p><p><img src="/2020/03/06/Deep-learning-of-Structuring-Machine-Learning-Projects/13.jpg" alt=""></p><p>这个问题的cost函数为</p><p><img src="/2020/03/06/Deep-learning-of-Structuring-Machine-Learning-Projects/14.png" alt=""></p><p>对于这样的问题，我们就是在做多任务学习，因为我们建立单个神经网络，来解决多个问题。</p><p>特定的对于一些问题，例如在我们的例子中，数据集中可能只标注了部分信息，如其中一张只标注了人，汽车和交通灯的标识没有标注。那么对于这样的数据集，我们依旧可以用多任务学习来训练模型。当然要注意这里Loss function求和的时候，只对带0、1标签的 j 进行求和。</p><p>多任务学习有意义的情况：</p><p>如果训练的一组任务可以共用低层特征；<br>通常，对于每个任务大量的数据具有很大的相似性；（如，在迁移学习中由任务A“100万数据”迁移到任务B“1000数据”；多任务学习中，任务多的，每个任务均有1000个数据，合起来就有1000n个数据，共同帮助任务的训练）<br>可以训练一个足够大的神经网络并同时做好所有的任务。</p><h4 id="端到端深度学习"><a href="#端到端深度学习" class="headerlink" title="端到端深度学习"></a>端到端深度学习</h4><p>端到端学习的定义：</p><p>相对于传统的一些数据处理系统或者学习系统，它们包含了多个阶段的处理过程，而端到端的深度学习则忽略了这些阶段，用单个神经网络来替代。</p><p>语音识别例子：</p><p>在少数据集的情况下传统的特征提取方式可能会取得好的效果；如果在有足够的大量数据集情况下，端到端的深度学习会发挥巨大的价值。</p><p><img src="/2020/03/06/Deep-learning-of-Structuring-Machine-Learning-Projects/15.jpg" alt=""></p><p><strong>优缺点</strong></p><p>优点：<br>端到端学习可以直接让数据“说话”；<br>所需手工设计的组件更少。<br>缺点：<br>需要大量的数据；<br>排除了可能有用的手工设计组件。<br>应用端到端学习的 Key question：是否有足够的数据能够直接学习到从 x 映射到 y 的足够复杂的函数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
      <category term="机器学习" scheme="http://wsx1128.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://wsx1128.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://wsx1128.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning: Programming Exercise 1: Linear Regression</title>
    <link href="http://wsx1128.cn/2020/01/20/Machine-Learning-Programming-Exercise-1-Linear-Regression/"/>
    <id>http://wsx1128.cn/2020/01/20/Machine-Learning-Programming-Exercise-1-Linear-Regression/</id>
    <published>2020-01-20T08:36:31.000Z</published>
    <updated>2020-02-26T07:31:05.486Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="知识梗概"><a href="#知识梗概" class="headerlink" title="知识梗概"></a>知识梗概</h2><pre><code>这一周首先一开始就直接介绍，机器学习以及深度学习的相关概念，介绍了两个概念 一个是监督学习，另一个就是非监督学习，简要的区分了上述两种基本学习的形式。以下重点关注了线性回归，分为了两个部分来讲解，首先是一个参数的线性回归方程，另外一个就是多个参数的线性回归方程。</code></pre><h2 id="机器学习的介绍"><a href="#机器学习的介绍" class="headerlink" title="机器学习的介绍"></a>机器学习的介绍</h2><p>总的来说，简而言之就是通过数据来预测，识别，分类。 （目前自己所能接触到的地方）</p><p><strong>监督学习</strong> </p><ol><li><p>房价的预测，通过数据来整合出特征量，然后模拟出函数（假设），最后通过给出的特征量来得出房价</p></li><li><p>癌症肿瘤检测，通过数据来根据肿瘤大小，进行一个分类，然后下次预测检测肿瘤大小判断良性或者恶性。</p></li></ol><p>总结： 监督学习就是每一个数据都会有一个结果量（标签量）来和其他数据进行区分，比如房价的特征量最终的结果是房价的多少，肿瘤的大小决定了是否为良性。 需要一个结果标签量</p><p><strong>非监督学习</strong></p><p>人群分类，天文数据分析，市场比重分析</p><p>这一类的数据不需要结果标签量来区分彼此，每一个其他量没有其他的分别。主要用于没有区分的数据，进行相互组合及分类。</p><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>线性回归属于监督学习。</p><h3 id="线性回归一个参数"><a href="#线性回归一个参数" class="headerlink" title="线性回归一个参数"></a>线性回归一个参数</h3><p>具体的步骤就是<br><img src="/2020/01/20/Machine-Learning-Programming-Exercise-1-Linear-Regression/1.png" alt=""></p><p>所以接下来的步骤就是要得出这个假设函数。</p><p>所以接下来引出了<strong>cost function （代价函数）</strong></p><p><img src="/2020/01/20/Machine-Learning-Programming-Exercise-1-Linear-Regression/2.png" alt=""></p><p>现在已经得出了代价函数，接下来就是需要通过使代价函数最小来模拟出假设函数。</p><p><strong>Gradient descent（梯度下降）</strong></p><p>就像下图一样可以直接得出<br><img src="/2020/01/20/Machine-Learning-Programming-Exercise-1-Linear-Regression/3.png" alt=""></p><p>梯度下降的算法如下图所示<br><img src="/2020/01/20/Machine-Learning-Programming-Exercise-1-Linear-Regression/4.png" alt=""></p><p>注意注意 梯度下降算法 必须要同时进行，如果没有同时进行，那么不算数。</p><p>并且在$\alpha$的选择上，一定要适合，不然的话就会造成以下的后果，不过一般这种情况题目会提前给予相应的情况。</p><p><img src="/2020/01/20/Machine-Learning-Programming-Exercise-1-Linear-Regression/5.png" alt=""></p><p>最终将上述两者结合起来</p><p>梯度下降算法如下<br><img src="/2020/01/20/Machine-Learning-Programming-Exercise-1-Linear-Regression/6.png" alt=""></p><p>以上就被称为：“Batch” Gradient Descent。（后面的章节会着重讲到）</p><h3 id="线性回归多个参数"><a href="#线性回归多个参数" class="headerlink" title="线性回归多个参数"></a>线性回归多个参数</h3><p>由于之前 只有一个参数就能够模拟出相应的假设，但是现在有更复杂的徒刑，需要更多的参数 </p><p>就想着样子：<br><img src="/2020/01/20/Machine-Learning-Programming-Exercise-1-Linear-Regression/7.png" alt=""></p><p>所以更改下来 新的梯度下降算法，便成为了这个：<br><img src="/2020/01/20/Machine-Learning-Programming-Exercise-1-Linear-Regression/8.png" alt=""></p><h3 id="参数缩放以及归一化"><a href="#参数缩放以及归一化" class="headerlink" title="参数缩放以及归一化"></a>参数缩放以及归一化</h3><p>由于多个参数之后，有的特征量与其他的特征量相比，过于大了，所以现在需要采用的是 参数缩放将参数除以其的范围，限制在0到1之间。</p><p><strong>归一化</strong><br><img src="/2020/01/20/Machine-Learning-Programming-Exercise-1-Linear-Regression/10.png" alt=""></p><p>减去平均值 然后再除上最大范围</p><p><strong>正确的选择\alpha的值</strong></p><p>选择参数的时候可以灵活一点 就想这样</p><p><img src="/2020/01/20/Machine-Learning-Programming-Exercise-1-Linear-Regression/11.png" alt=""></p><p><strong>Normal Equation</strong></p><p>第二种计算线性回归的方法</p><p><img src="/2020/01/20/Machine-Learning-Programming-Exercise-1-Linear-Regression/12.png" alt=""></p><h2 id="作业要求"><a href="#作业要求" class="headerlink" title="作业要求"></a>作业要求</h2><pre><code>作业就是要求使用matlab 运用线性代数的知识，来模拟线性回归，已经上传到github上面，这里就不再重复叙述了。后面有时间，使用pytorch来重新练习一遍。  </code></pre><p>·</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
      <category term="机器学习" scheme="http://wsx1128.cn/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://wsx1128.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性回归" scheme="http://wsx1128.cn/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>告别2019，初遇2020</title>
    <link href="http://wsx1128.cn/2020/01/01/%E5%91%8A%E5%88%AB2019%EF%BC%8C%E5%88%9D%E9%81%872020/"/>
    <id>http://wsx1128.cn/2020/01/01/告别2019，初遇2020/</id>
    <published>2019-12-31T22:04:42.000Z</published>
    <updated>2020-02-27T00:22:49.714Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>2019年有很多遗憾，有很多没有达成的目标，曾经以为特别重要的东西，随着时间流逝，反而觉得没有那么重要了，曾经消极的看待世界，沉迷于世界的阴霾，绝望，无奈，以及看不到生活的希望，开始质疑生活的初心。一次又一次的被打击自信心，又一次又一次进行了信心的重塑。</p><p>在2019年 参加过很多辩论赛，打过acm，蓝桥杯，去过北京，去过其他的各大高校，同时也遇见过形形色色的很多人，感谢这些出现在我生命里的人，是他们让我们知道人生的多种可能性，也是他们让我不再纠结于自己的人生不能够在多元化迈进。</p><p>曾经觉得自己的一生一定是精彩的一生，曾经觉得自己是世界上最独一无二的人，只不过还没有觉醒发力，曾经觉得自己能做很多其他常人做不到的事情。</p><p>到最后才发现自己可能真的是一个普通人，平凡的家庭，平凡的经历，以及平凡的生活，以前会觉得这种思想是消极的懦弱的想法，但是现在只是觉得 早点认清事实，也许可以为平凡的生活增添一些不平凡的地方，就像是疲惫的平凡生活中增添几丝无限向往的英雄梦想。</p><p>2019年又很多从2018年继承下来的期望，以前的以前觉得世界很大，大到自己无法想象的程度，恐惧，无知有的时候真的会占据一大半你对这个世界的想象。有的时候其实走向未来最大的阻力真的可能就是自己。 也许真的应了那句话，奥力给。</p><p>虽然迈向2020年之前的前几天，还陷在无数考试和实验报告中，自己也曾经抑郁过一个星期不怎么起床。不过，那都是过去了，我相信未来的一年，一定是充满朝阳的一年。</p><p>任何情绪都有存在的必要，任何感觉都有被诉说的可能，任何需求都应该有发泄的可能，我自己不再阻塞，不再自闭，不再责怪自己，拥抱希望，相信时间会给你所有想要的，才是迈向人生终极梦想的道路。</p><p>现在是6点43分，跟喜欢狼人杀的同号打了一晚上的狼人杀游戏，大家有的睡着了，有的还在打游戏，真的很庆幸有一群志趣相同的朋友陪着自己度过这些天的紧张生活。 狼人杀，希望新的一年爆爆爆吧。</p><p>也许2020年会遭受很多痛苦，会有很多时间花在怀疑自己这个过程中，也许会对生活失去原本的热心。真的很害怕自己会最终成为失去光华的人，就像游戏蔚蓝一样，如果真的爬不到山顶，真的完成不了自己的目标，那么该怎么办？就像蔚蓝的女主角一样“那就失败吧，你是我需要放下的一切，在此之前，请接受自己。”</p><p>最后以一句话结束自己的告别，与迎接新的一年</p><p>我的一生注定是与自己抗争的一生，但同时也是逐渐接受自己的一生。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
      <category term="成长" scheme="http://wsx1128.cn/categories/%E6%88%90%E9%95%BF/"/>
    
    
      <category term="生活" scheme="http://wsx1128.cn/tags/%E7%94%9F%E6%B4%BB/"/>
    
      <category term="年终总结" scheme="http://wsx1128.cn/tags/%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>人生第一场话剧---一个人的莎士比亚</title>
    <link href="http://wsx1128.cn/2019/12/08/%E4%BA%BA%E7%94%9F%E7%AC%AC%E4%B8%80%E5%9C%BA%E8%AF%9D%E5%89%A7-%E4%B8%80%E4%B8%AA%E4%BA%BA%E7%9A%84%E8%8E%8E%E5%A3%AB%E6%AF%94%E4%BA%9A/"/>
    <id>http://wsx1128.cn/2019/12/08/人生第一场话剧-一个人的莎士比亚/</id>
    <published>2019-12-08T15:43:07.000Z</published>
    <updated>2019-12-31T22:03:30.190Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>这次可以说人生的第一场话剧，本来就是有一点不想去，毕竟最近自己的状态并没有恢复过来，上次生日的花费预算，也是超了一点，然后秉持着不太想乱花钱的想，于是就有点不想去。</p><p>感谢朋友的坚持，让我感受到了又浅到深的心灵交流。</p><p>这是我接触到的第一场全英文独白的话剧了，全程就是一个年过半百的美国老爷爷上台表演，通过多种表情，姿态，语气，台词展现多个角色的独角戏。而展示的角色大致如下：</p><ul><li>主角的学生时代</li><li>主角的父亲</li><li>主角的莎士比亚老师</li><li>主角的成年时代</li><li>主角莎士比亚老师的学生</li></ul><p>说句实话，自己实在是算个粗人，对比话剧人士来说，我实在是无法接收到这种不是特别强烈的画面感以及动情的背景音乐的交流。以至于我刚开始看这个话剧的时候，觉得有点带有低俗性质的无法共情。正是由于是全英文独白，没有任何额外的配音，与道具，考验观众的共情能力，以及英语能力（虽然我每个单词都能够大致听懂，但是连成一个句子实在是有点强人所难了）。但是更考验的却是演员的临场能力，以及各种情绪语气的转变能力。只能说观众向有点精湛，如果读过莎士比亚的选集再来观看可能又是不太一样的感受吧。</p><p>个人理解，这部话剧是将作者亲身经历的故事，和莎士比亚的剧本话剧，代入到一个莎士比亚悲剧精神的校长之间发生的故事。</p><p>如果由剧情上进行一个划分，基本上可以分成四个部分</p><h4 id="主角与莎士比亚话剧集的初识"><a href="#主角与莎士比亚话剧集的初识" class="headerlink" title="主角与莎士比亚话剧集的初识"></a>主角与莎士比亚话剧集的初识</h4><p>这个地方就是作者被老师校长逼迫读<strong>皆大欢喜</strong>中间的一段，由于总是读错，于是紧张的尿裤子。剧情上显得很一般，主要角色有两个，但是演员厉害的是在两种角色中转变中游刃有余，并且将学生的紧张感表现的唯妙唯俏，要知道，这仅仅只是靠了台词以及肢体语言，并没有其他多余的表现形式。 这个地方也是刚刚开始共情的地方了，从最先开始的看不懂的惊讶，到后面对于一个词语 遗赠 产生了莫大的兴趣 这也是主角尿裤子的主要原因了，这一场戏反而结合大家的想象力，反而显得特别有趣，因为每一种场景被赋予了想象力获得了独特的生命力，剧情的推进也到了下一个阶段。</p><h4 id="主角与父亲的对话"><a href="#主角与父亲的对话" class="headerlink" title="主角与父亲的对话"></a>主角与父亲的对话</h4><p>主角由于在学校里面出了丑，不敢再去学校，被父亲开解，于是敢于去学校了。父亲开解主角的那段话，让我记忆深刻，可能是最近对于细节情感的再现，真心的觉得中国普通家庭的亲子关系充满了沉重，被经济压的只剩下现实，希望自己以后能够冲破束缚改变这一切。我希望与父母或者是子女的关系（如果有的话）能够多一些轻松愉悦，不要被焦虑的担心所束缚。<br>所谓父母子女一场，不过真切关系中最有缘分的相连，不要越爱越沉重。</p><h4 id="主角与发生悲剧老师的故事"><a href="#主角与发生悲剧老师的故事" class="headerlink" title="主角与发生悲剧老师的故事"></a>主角与发生悲剧老师的故事</h4><p>「有时间再去补上」</p><h4 id="主角与老年老师的故事"><a href="#主角与老年老师的故事" class="headerlink" title="主角与老年老师的故事"></a>主角与老年老师的故事</h4><p>换言之，整部话剧最令我感动的就是这个地方，由老年的校长说出<br>全世界是一个大舞台，所有的男男女女不过是一些演员；他们都有下场的时候，也有上场的时候。<br>一个人的一生中扮演着好几个角色，他的表演可以分为七个时期。<br>最初是婴孩，在保姆的怀中啼哭呕吐。<br>然后是背着书报、满面红光的学童，像蜗牛一样慢腾腾地拖着脚步，不情愿地呜咽着上学堂。<br>然后是情人，像炉灶一样叹着气，写了一首悲哀的诗歌咏着他恋人地眉毛。<br>然后是一个军人，满口发着古怪地誓，胡须长得像豹子一样，爱惜着名誉，动不动就要打架，在炮口上寻求着泡沫一样得荣名。<br>然后是法官，胖胖圆圆的肚子塞满了阉鸡，凛然的眼光，整洁的胡须，满嘴都是格言和老生常谈；他这样扮了他的一个角色。<br>第六个时期变成了精瘦的趿着拖鞋的龙钟老叟，鼻子上架着眼镜，腰边悬着钱袋；他那年轻时候节省下来的长袜子套在他皱瘪的小腿上显得宽大异常；他那朗朗的男子的口音又变成了孩子似的尖声，像是吹着风笛和哨子。<br>终结着这段古怪的多事的历史的最后一场，是孩提时代的再现，全然的遗忘，没有牙齿，没有眼睛，没有口味，没有一切。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
      <category term="成长" scheme="http://wsx1128.cn/categories/%E6%88%90%E9%95%BF/"/>
    
    
      <category term="话剧" scheme="http://wsx1128.cn/tags/%E8%AF%9D%E5%89%A7/"/>
    
      <category term="莎士比亚" scheme="http://wsx1128.cn/tags/%E8%8E%8E%E5%A3%AB%E6%AF%94%E4%BA%9A/"/>
    
  </entry>
  
  <entry>
    <title>Data Compression --- the course of algorithm</title>
    <link href="http://wsx1128.cn/2019/10/10/Data-Compression-the-course-of-algorithm/"/>
    <id>http://wsx1128.cn/2019/10/10/Data-Compression-the-course-of-algorithm/</id>
    <published>2019-10-10T08:33:38.000Z</published>
    <updated>2019-10-11T10:31:05.472Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="数据压缩"><a href="#数据压缩" class="headerlink" title="数据压缩"></a>数据压缩</h2><h3 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h3><p>压缩数据可以节省存储数据需要的空间和传输数据需要的时间，虽然摩尔定律说集成芯片上的晶体管每 18-24 个月翻一倍，帕金森定律说数据会自己拓展来填满可用空间，但数据压缩还是最经济的做法。</p><p>数据压缩的基本模型如下，很简单，压缩和解压，压缩率即 C(B) 和 B 的比特数之比。</p><p><img src="/2019/10/10/Data-Compression-the-course-of-algorithm/1.png" alt=""></p><p>数据压缩对象的本质实际上就是将二进制文件，抽象层次为比特流， 下面直接给出了相应读写二进制的类</p><p><img src="/2019/10/10/Data-Compression-the-course-of-algorithm/2.png" alt=""><br>这里 java的默认处理是基于8位字节流<br><img src="/2019/10/10/Data-Compression-the-course-of-algorithm/3.png" alt=""></p><p>这里我举出一个简单的数据压缩的例子</p><p>将日期 12/31/1990 这个字符串进行压缩</p><p><img src="/2019/10/10/Data-Compression-the-course-of-algorithm/4.png" alt=""></p><p>上面就可以轻而易举的看出来 压缩的好处。<br>第一张图就是原来的写法，第二张图就是将其换成int类型进行压缩<br>第三张图<br>则按照特定的压缩进行压缩</p><p><strong>这里需要注意的是，并不存在通用的压缩算法</strong></p><p>当然，这里存在一种可以供人类阅读的比特流形式，这个被称为 转储。<br>下面图，就是一些例子：</p><p><img src="/2019/10/10/Data-Compression-the-course-of-algorithm/5.png" alt=""></p><p>BinaryDump 将比特流按 0 和 1 输出来；HexDump 将比特流组织成 8 位并用两位的 16 进制数表示；PictureDump 则将比特流变为 Picture 对象，其中白色像素表示 0，黑色像素表示 1。</p><h3 id="run-length-Coding"><a href="#run-length-Coding" class="headerlink" title="run-length Coding"></a>run-length Coding</h3><p>游程编码，就是专门用来处理冗杂的数据，他是通过计算重复的个数，来最终决定压缩的形式，就像下图：</p><p><img src="/2019/10/10/Data-Compression-the-course-of-algorithm/6.png" alt=""></p><p>上面这里就是用的4位计数，而下面的代码则是使用了8位计数。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RunLength</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="keyword">int</span> R = <span class="number">256</span>;    <span class="comment">// maximum run-length conut</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="keyword">int</span> lgR = <span class="number">8</span>;    <span class="comment">// number of bits per conut</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">compress</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">char</span> cnt = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">boolean</span> b, old = <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">while</span> (!BinaryStdIn.isEmpty()) &#123;</span><br><span class="line">            b = BinaryStdIn.readBoolean();</span><br><span class="line">            <span class="keyword">if</span> (b != old) &#123;</span><br><span class="line">                BinaryStdOut.write(cnt);</span><br><span class="line">                cnt = <span class="number">0</span>;</span><br><span class="line">                old = !old;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 由于这个地方是八位计数，所以最大的限制也就是255</span></span><br><span class="line">                <span class="keyword">if</span> (cnt == <span class="number">255</span>) &#123;</span><br><span class="line">                    BinaryStdOut.write(cnt);</span><br><span class="line">                    cnt = <span class="number">0</span>;</span><br><span class="line">                    BinaryStdOut.write(cnt);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            cnt++;</span><br><span class="line">        &#125;</span><br><span class="line">        BinaryStdOut.write(cnt);</span><br><span class="line">        BinaryStdOut.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">expand</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">boolean</span> bit = <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">while</span> (!BinaryStdIn.isEmpty()) &#123;</span><br><span class="line">            <span class="keyword">int</span> run = BinaryStdIn.readInt(lgR); <span class="comment">// read 8-bit conut from standard input</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; run; i++)</span><br><span class="line">                BinaryStdOut.write(bit);        <span class="comment">// write 1 bit to standard output</span></span><br><span class="line">            bit = !bit;</span><br><span class="line">        &#125;</span><br><span class="line">        BinaryStdOut.close();                  <span class="comment">// pad 0s for byte alignment</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这种策略专门对付经常出现的，冗杂的比特流是十分有效的，游程编码的一个应用就是压缩位图，位图被广泛的用于保存图片和扫描文档。</p><p>就像这张图，左边经过压缩之后极大程度上小了很多。</p><p><img src="/2019/10/10/Data-Compression-the-course-of-algorithm/7.png" alt=""></p><p>这里的游程编码不适用于含有大量短游程的输入，而不是所有我们希望压缩的比特都能像上面一样重复个数多，且冗杂。所以接下来我们就介绍两种适用于多种类型的文件压缩算法。</p><h3 id="Huffman-Compression"><a href="#Huffman-Compression" class="headerlink" title="Huffman Compression"></a>Huffman Compression</h3><p>哈夫曼压缩</p><p>这里直接介绍了一个摩斯密码，但是像摩斯密码这一类的编码很容易产生多义性，所以密码之间还有一定的间隙隔开。</p><p>多义性的本质原因是有些字符的编码是其它字符编码的前缀，所以才可能会有不同的解读。而有种特殊的变长编码——前缀码（prefix-free code），字符编码肯定不是其它字符编码的前缀，也就不存在多义性的问题。</p><p>这里表示这种前缀码，可以很自然的使用字典树来进行表示</p><p><img src="/2019/10/10/Data-Compression-the-course-of-algorithm/8.png" alt=""></p><h3 id="节点代码"><a href="#节点代码" class="headerlink" title="节点代码"></a>节点代码</h3><p>这里直接用代码表示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span> <span class="keyword">implements</span> <span class="title">Comparable</span>&lt;<span class="title">Node</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">char</span> ch;    <span class="comment">// used only for leaf nodes</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> freq;   <span class="comment">// used only for compress</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Node left, right;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Node</span><span class="params">(<span class="keyword">char</span> ch, <span class="keyword">int</span> freq, Node left, Node right)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.ch = ch;</span><br><span class="line">        <span class="keyword">this</span>.freq = freq;</span><br><span class="line">        <span class="keyword">this</span>.left = left;</span><br><span class="line">        <span class="keyword">this</span>.right = right;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">isLeaf</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> left == <span class="keyword">null</span> &amp;&amp; right == <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(Node that)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.freq - that.freq;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>字符频率在下面生成最优前缀码的时候会使用到。</p><p>当然，在下面使用前，得将这个表示前缀码的字典树一样给压缩进入到比特流，而这里就直接使用前序遍历了。</p><p><img src="/2019/10/10/Data-Compression-the-course-of-algorithm/9.png" alt=""></p><p>当然，需要将叶子节点与其他的节点区分开来的话，到叶子节点的时候会先输出一个true，意思就是1，而其他的节点则是0，附在开头的Trie相对就会显得很小，没有什么关系。</p><p><img src="/2019/10/10/Data-Compression-the-course-of-algorithm/10.png" alt=""></p><p>上面已经将压缩字典树，以及解压缩字典树都讲到了，接下来就是构造这个前缀码字典树了。</p><p>实际上，哈夫曼的做法很好描述：首先你要知道字符出现的频率，然后每次挑两个最小的加起来，加起来的值再和原来的那些一起重复挑两个最小的加起来，从下往上接成 Trie。</p><p><img src="/2019/10/10/Data-Compression-the-course-of-algorithm/11.png" alt=""></p><h3 id="构造代码"><a href="#构造代码" class="headerlink" title="构造代码"></a>构造代码</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Node <span class="title">buildTrie</span><span class="params">(<span class="keyword">int</span>[] freq)</span> </span>&#123;</span><br><span class="line">    MinPQ&lt;Node&gt; pq = <span class="keyword">new</span> MinPQ&lt;Node&gt;();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">char</span> i = <span class="number">0</span>; i &lt; R; i++)</span><br><span class="line">        <span class="keyword">if</span> (freq[i] &gt; <span class="number">0</span>)</span><br><span class="line">            pq.insert(<span class="function">New <span class="title">Node</span><span class="params">(i, freq[i], <span class="keyword">null</span>, <span class="keyword">null</span>)</span>)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// merge two smallest tries</span></span><br><span class="line">    <span class="keyword">while</span> (pq.size() &gt; <span class="number">1</span>) &#123;</span><br><span class="line">        Node x = pq.delMin();</span><br><span class="line">        Node y = pq.delMin();</span><br><span class="line">        Node parent = <span class="keyword">new</span> Node(<span class="string">'\0'</span>, x.freq + y.freq, x, y);</span><br><span class="line">        <span class="comment">// 这里有点类似于广搜的操作，这样的做法就是直接将每一个最小的值弄出来，然后分别给其设置父亲节点</span></span><br><span class="line">        pq.insert(parent);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 最后一定一定就是根节点了。</span></span><br><span class="line">    <span class="keyword">return</span> pa.delMin();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="最优解证明"><a href="#最优解证明" class="headerlink" title="最优解证明"></a>最优解证明</h3><p>这个地方目前对自己不作要求，贴两个网址</p><p><a href="https://www.cnblogs.com/mingyueanyao/p/9516423.html" target="_blank" rel="noopener">文字讲解</a></p><p><a href="https://www.coursera.org/lecture/algorithms/058ha-fu-man-suan-fa-de-zheng-que-xing-zheng-ming-nLQya" target="_blank" rel="noopener">视频讲解</a></p><h3 id="LZW-compression"><a href="#LZW-compression" class="headerlink" title="LZW-compression"></a>LZW-compression</h3><p>LZW 压缩算法是自适应性的（adaptive）模型，在读入文本的时候学习并更新模型，不需要将模型附在比特流中用于解压，但解压的时候只能从文本开头开始。</p><h3 id="压缩例子"><a href="#压缩例子" class="headerlink" title="压缩例子"></a>压缩例子</h3><p>展开和压缩类似，有下面几个步骤：</p><p>创建符号表，但这次编码为键，对应的字符串为值。<br>初始化符号表，加入单个字符的键值对。<br>从压缩文件读入 W 位的编码，输出编码对应的字符串。<br>预读下一个编码，得到下个字符，类似地更新符号表。<br>重复上两步直到读入结束编码。<br>例图即展开上面压缩形成的编码。</p><p><img src="/2019/10/10/Data-Compression-the-course-of-algorithm/12.png" alt=""></p><p>一开始读入 8 位编码 41，从符号表可知对应字符串 A，输出 A 后预读下一个编码 42，对应 B，于是往符号表中加入新键值对 (81, AB)；现在读到编码 42，输出 B 并预读 52 得到 R，所以加入 (82, BR) … 直到读入编码 80，表示文件结束。</p><p>似乎展开和压缩差不多，甚至更简单，因为不需要找最长前缀，符号表直接用数组简单实现。但是，展开有时会碰到一个特殊的情况：</p><p><img src="/2019/10/10/Data-Compression-the-course-of-algorithm/13.png" alt=""></p><p>压缩字符串 ABABABA 编码成 41 42 81 83 80，现在对这编码进行展开。编码 41 输出 A，预读 42 后加入 (81, AB) 更新符号表；编码 42 输出 B，预读 81 知道下个字符是 A，加入 (82, BA)；编码 81 输出 AB，预读 83 卡住，因为符号表中还没有这个键。</p><p>但是，这种时候我们还是可以知道 AB 的下一个字符是什么的。假设 AB 后面的字符分别为 𝑐1，𝑐2，𝑐3，卡住的时候（更新要加入的编码和预读到的编码一样）肯定有 AB𝑐1=𝑐1𝑐2𝑐3,所以下个字符即 A，加入 (83, ABA) 即可。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">expand</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i;                                      <span class="comment">// 当前更新符号表要加入的编码</span></span><br><span class="line">    String[] st = <span class="keyword">new</span> String[L];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; R; i++)</span><br><span class="line">        st[i] = <span class="string">""</span> + (<span class="keyword">char</span>) i;</span><br><span class="line">    st[i++] = <span class="string">" "</span>;                             <span class="comment">// 例图中表示文件结束的 0x80</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> codeword = BinaryStdIn.readInt(W);</span><br><span class="line">    String val = st[codeword];</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        BinaryStdOut.write(val);</span><br><span class="line">        codeword = BinaryStdIn.readInt(W);    <span class="comment">// 预读的编码</span></span><br><span class="line">        <span class="keyword">if</span> (codeword == R) <span class="keyword">break</span>;</span><br><span class="line">        String s = st[codeword];</span><br><span class="line">        <span class="keyword">if</span> (i == codeword)                   <span class="comment">// 要加入的编码和预读的编码相同</span></span><br><span class="line">            s = val + val.charAt(<span class="number">0</span>);</span><br><span class="line">        <span class="keyword">if</span> (i &lt; L)</span><br><span class="line">            st[i++] = val + s.charAt(<span class="number">0</span>);</span><br><span class="line">        val = s;</span><br><span class="line">    &#125;</span><br><span class="line">    BinaryStdOut.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>关于第二种压缩方式 有点不是很好理解，可以配合算法视频课一起食用，这里就可以不用传输模型展开压缩，编码文件了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
      <category term="算法" scheme="http://wsx1128.cn/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="java" scheme="http://wsx1128.cn/tags/java/"/>
    
      <category term="普林斯顿算法课" scheme="http://wsx1128.cn/tags/%E6%99%AE%E6%9E%97%E6%96%AF%E9%A1%BF%E7%AE%97%E6%B3%95%E8%AF%BE/"/>
    
      <category term="数据压缩" scheme="http://wsx1128.cn/tags/%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9/"/>
    
  </entry>
  
  <entry>
    <title>Regular Expressions --- the course of algorithm</title>
    <link href="http://wsx1128.cn/2019/10/09/Regular-Expressions-the-course-of-algorithm/"/>
    <id>http://wsx1128.cn/2019/10/09/Regular-Expressions-the-course-of-algorithm/</id>
    <published>2019-10-09T11:11:42.000Z</published>
    <updated>2019-10-09T12:52:28.485Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="Regular-Expressions"><a href="#Regular-Expressions" class="headerlink" title="Regular Expressions"></a>Regular Expressions</h2><h3 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h3><p>在文本中查找子字符串只是寻找一个单一的字符串，但经常的我们可能不知道这个字符串的完整信息，或是寻找的是吻合某种模式的一些字符串，即所谓 模式匹配（Pattern Matching）。</p><p>正则表达式（Regular Expressions） 就是用来描述模式的，表示符合某种模式的字符串的集合（可能是无限的），它有下面几种基本操作：</p><p><img src="/2019/10/09/Regular-Expressions-the-course-of-algorithm/1.png" alt=""></p><p>就是普通的正则表达式里面的操作。</p><p>连接、或、闭包和括号，都不难理解。其中闭包表示若干个自身连接，可以是零个。</p><p>然后他一般有以下这些缩略方式</p><p><img src="/2019/10/09/Regular-Expressions-the-course-of-algorithm/2.png" alt=""></p><p>关于完整的正则表达式 可以去看看</p><p><a href="https://www.jb51.net/tools/regexsc.htm" target="_blank" rel="noopener">正则表达式速查表</a></p><p><a href="https://blog.csdn.net/carechere/article/details/52315728" target="_blank" rel="noopener">基本上常用的正则表达式</a></p><p>因为之前接触过正则表达式，所以这个地方也就不再多余赘述了。</p><h3 id="REs-And-NFAs"><a href="#REs-And-NFAs" class="headerlink" title="REs And NFAs"></a>REs And NFAs</h3><p>其实，正则表达式和确定型有穷自动机间存在着二元性（duality），即 Kleene 定理所说：对任意 DFA 存在着描述同样字符串集合的正则表达式，对任意正则表达式存在着识别同样字符串集合的 DFA。</p><p>就像之前学习KMP的时候构造的DFA，这里其实也可以构造一个DFA，就像这样<br><img src="/2019/10/09/Regular-Expressions-the-course-of-algorithm/3.png" alt=""></p><p>当然应用在KMP上面的DFA是线性时间的性能保证，但是这个办法并不可行，因为正则表达式对应的DFA的状态数目可能是指数级的。</p><p>于是，就来了解一下 非确定型有限状态自动机，状态之间的转移并不是确定的。</p><ul><li><p>正则表达式用括号括起。</p></li><li><p>正则每个符号对应 NFA 一个状态，再加个接受状态。</p></li><li><p>接受空串𝜖，不扫描下个字符而直接改变状态（下图红线），不确定性所在。</p></li><li><p>扫描字符，匹配转移到一下个状态（下图黑线）。</p></li><li><p>在扫描完全部文本字符后，如果有 任一 转移序列到达接受状态，则匹配成功。</p></li></ul><p><img src="/2019/10/09/Regular-Expressions-the-course-of-algorithm/4.png" alt=""></p><h3 id="NFA-construction"><a href="#NFA-construction" class="headerlink" title="NFA-construction"></a>NFA-construction</h3><p>首先先讲一下它的构造过程</p><p>最先开始的状态，最后一个位置表示完成匹配<br><img src="/2019/10/09/Regular-Expressions-the-course-of-algorithm/5.png" alt=""></p><p>所有字符直接指向下一个<br><img src="/2019/10/09/Regular-Expressions-the-course-of-algorithm/6.png" alt=""></p><p>括号或者空的，就直接往下面指就行了<br><img src="/2019/10/09/Regular-Expressions-the-course-of-algorithm/7.png" alt=""></p><p>闭包，比较巧妙<br><img src="/2019/10/09/Regular-Expressions-the-course-of-algorithm/8.png" alt=""></p><p>不过分两种情况，例子里面的就是第一种情况<br><img src="/2019/10/09/Regular-Expressions-the-course-of-algorithm/9.png" alt=""></p><p>符号 ‘|’<br><img src="/2019/10/09/Regular-Expressions-the-course-of-algorithm/10.png" alt=""></p><p>最后再来两个空转移<br><img src="/2019/10/09/Regular-Expressions-the-course-of-algorithm/11.png" alt=""></p><p>当然，在完成上面的构造的过程中，需要借助数据结构栈，碰到左括号或者‘|’的时候就直接将其的编号压到栈内，碰到右括号就弹出栈顶，如果对于的是其他东西，就直接根据上面的图来顺应结果，具体看代码。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 这里就是直接用有向图来进行表示。</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> Digraph <span class="title">buildNFA</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Digraph g = <span class="keyword">new</span> Digraph(M + <span class="number">1</span>); <span class="comment">// 顶点数加1</span></span><br><span class="line">    Stack&lt;Integer&gt; ops = <span class="keyword">new</span> Stack&lt;Integer&gt;();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; M; i++) &#123;</span><br><span class="line">        <span class="keyword">int</span> lp = i;</span><br><span class="line">        <span class="keyword">if</span> (re[i] == <span class="string">'('</span> || re[i] == <span class="string">'|'</span>) ops.push(i);</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (re[i] == <span class="string">')'</span>) &#123;</span><br><span class="line">            <span class="keyword">int</span> or = ops.pop();</span><br><span class="line">            <span class="keyword">if</span> (re[or] == <span class="string">'|'</span>) &#123;</span><br><span class="line">                lp = ops.pop();</span><br><span class="line">                g.addEdge(lp, or + <span class="number">1</span>);</span><br><span class="line">                g.addEdge(or + <span class="number">1</span>, lp);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> lp = or;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (i &lt; M - <span class="number">1</span> &amp;&amp; re[i + <span class="number">1</span>] == <span class="string">'*'</span>) &#123;</span><br><span class="line">            g.addEdge(lp, i + <span class="number">1</span>);</span><br><span class="line">            g.addEdge(i + <span class="number">1</span>, lp);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (re[i] == <span class="string">'('</span> || re[i] == <span class="string">'*'</span> || re[i] == <span class="string">')'</span>)</span><br><span class="line">            g.addEdge(i, i + <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> g;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="NFA-simulation"><a href="#NFA-simulation" class="headerlink" title="NFA-simulation"></a>NFA-simulation</h3><p>首先我们这样来表示 NFA：用整数 0 到 M（正则长度）来标号状态（像上图），用数组 re 来存储正则表达式，用有向图来存储空转移</p><p>至于怎么模拟 NFA 输入文本运行，感觉类似广搜，维护每一步所有可能走到的状态，下一步再拓展这些状态，要是文本流结束那步的状态里包含接受状态，就表示匹配成功。例图：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NFA</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">char</span>[] re;    <span class="comment">// match transitions</span></span><br><span class="line">    <span class="keyword">private</span> Digraph G;    <span class="comment">// epsilon transition digraph</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> M;        <span class="comment">// number of states</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">NFA</span><span class="params">(String regexp)</span> </span>&#123;</span><br><span class="line">        M = regexp.length();</span><br><span class="line">        re = regexp.toCharArray();    </span><br><span class="line">        G = buildEpsilonTransitionDigraph();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">recognizes</span><span class="params">(String txt)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// states reachable from start by epsilon transitions</span></span><br><span class="line">        Bag&lt;Integer&gt; pc = <span class="keyword">new</span> Bag&lt;Integer&gt;();</span><br><span class="line">        DirectedDFS dfs = <span class="keyword">new</span> DirectedDFS(G, <span class="number">0</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> v = <span class="number">0</span>; v &lt; G.V(); v++)</span><br><span class="line">            <span class="keyword">if</span> (dfs.marked(v)) pc.add(v);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; txt.length(); i++) &#123;</span><br><span class="line">            <span class="comment">// states reachable after scanning past txt.charAt(i)</span></span><br><span class="line">            Bag&lt;Integer&gt; match = <span class="keyword">new</span> Bag&lt;Integer&gt;();</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> v : pc) &#123;</span><br><span class="line">                <span class="keyword">if</span> (v == M) <span class="keyword">continue</span>;</span><br><span class="line">                <span class="comment">// 匹配时直接加上下一个状态</span></span><br><span class="line">                <span class="keyword">if</span> ((re[v] == txt.charAt(i)) || re[v] == <span class="string">'.'</span>)</span><br><span class="line">                    match.add(v + <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">//最坏的结果就是 match 重新复制了一遍上述的全部</span></span><br><span class="line">            dfs = <span class="keyword">new</span> DirectedDFS(G, match);    <span class="comment">// 拓展上一步的所有状态</span></span><br><span class="line">            pc = <span class="keyword">new</span> Bag&lt;Integer&gt;();</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> v = <span class="number">0</span>; v &lt; G.V(); v++)</span><br><span class="line">                <span class="keyword">if</span> (dfs.marked(v)) pc.add(v);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// accept if can end in state M</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> v : pc)</span><br><span class="line">            <span class="keyword">if</span> (v == M) <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>图文 大部分借鉴 <a href="https://www.cnblogs.com/mingyueanyao/p/9495243.html" target="_blank" rel="noopener">博客</a></p><p>若侵权，必删。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
      <category term="算法" scheme="http://wsx1128.cn/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="http://wsx1128.cn/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="java" scheme="http://wsx1128.cn/tags/java/"/>
    
      <category term="普林斯顿算法课" scheme="http://wsx1128.cn/tags/%E6%99%AE%E6%9E%97%E6%96%AF%E9%A1%BF%E7%AE%97%E6%B3%95%E8%AF%BE/"/>
    
  </entry>
  
  <entry>
    <title>BoggleSolver</title>
    <link href="http://wsx1128.cn/2019/09/28/BoggleSolver/"/>
    <id>http://wsx1128.cn/2019/09/28/BoggleSolver/</id>
    <published>2019-09-28T13:44:37.000Z</published>
    <updated>2019-10-09T12:50:55.537Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="题目"><a href="#题目" class="headerlink" title="题目"></a><a href="https://coursera.cs.princeton.edu/algs4/assignments/boggle/specification.php" target="_blank" rel="noopener">题目</a></h3><p><img src="/2019/09/28/BoggleSolver/1.png" alt="1"><br><img src="/2019/09/28/BoggleSolver/2.png" alt="2"><br><img src="/2019/09/28/BoggleSolver/3.png" alt="3"><br><img src="/2019/09/28/BoggleSolver/4.png" alt="4"><br><img src="/2019/09/28/BoggleSolver/5.png" alt="5"></p><h3 id="解析"><a href="#解析" class="headerlink" title="解析"></a>解析</h3><p>这道题的意思就是 给你一个四乘以四的方块，然后再这个方块里面遍历所有能到的位置，八个方向，每个位置只能走一次，方块上面各有一个字母，走过每一个字母之后构成一个单词，然后看这个单词最后在不在字典中，且能得到多少分？</p><p><img src="https://coursera.cs.princeton.edu/algs4/assignments/boggle/pins.png" alt=""></p><p>最先开始的想法，是用一个bfs，然后将每一个遍历每一个单词，然后再讲每一个单词放入到其中进行判断，这样的话，时间复杂度会特别高，没一个单词都要遍历全部位置，并且 一些单词没有的前缀，<strong>比如，没有Y 开头的单词的话，那么我在从Y这里开始走的话，会浪费特别多的精力。</strong></p><p>所以，看了一下解析，是直接用字典树去存储，然后遍历位置，就相当于遍历字典树一样。</p><p>找时间得重新写一下。自己做的很不对的地方就在于，自己用的bfs，注意的是这个状态并不是循序渐进的变化，而是每一个状态都不一样，如果要用bfs的话，那么每一个状态都要存进去大量的东西重置，所以，这就是不准确的地方。</p><h3 id="贴代码"><a href="#贴代码" class="headerlink" title="贴代码"></a>贴代码</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> edu.princeton.cs.algs4.Bag;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashSet;</span><br><span class="line"><span class="keyword">import</span> edu.princeton.cs.algs4.Stack;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BoggleSolver</span></span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">    <span class="comment">//自己建立的 字典树，这样方便后面dfs查询的时候的剪枝。</span></span><br><span class="line">    <span class="keyword">private</span> Node root;</span><br><span class="line">    <span class="keyword">private</span> BoggleBoard board;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> col,row;</span><br><span class="line">    <span class="keyword">private</span> HashSet&lt;String&gt; allwords;</span><br><span class="line">    <span class="keyword">private</span> Bag&lt;Integer&gt;[] adj;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span>[] vis;</span><br><span class="line">    <span class="keyword">private</span> Stack&lt;Integer&gt; dice;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> val = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">private</span> Node[] next = <span class="keyword">new</span> Node[<span class="number">26</span>];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">BoggleSolver</span><span class="params">(String[] dictionary)</span> </span>&#123;</span><br><span class="line">        root = <span class="keyword">new</span> Node();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; dictionary.length; i++) &#123;</span><br><span class="line">            put(dictionary[i]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">put</span><span class="params">(String word)</span> </span>&#123;</span><br><span class="line">        root = put(root, word, <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//这里就是直接构造出一个字典树，通过这个字典树来存储所有字符，并且剪枝dfs。</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> Node <span class="title">put</span><span class="params">(Node x, String word, <span class="keyword">int</span> d)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (x == <span class="keyword">null</span>) x = <span class="keyword">new</span> Node();</span><br><span class="line">        <span class="keyword">if</span> (d == word.length()) &#123;</span><br><span class="line">            x.val = <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">return</span> x;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> c = word.charAt(d) - <span class="string">'A'</span>;</span><br><span class="line">        x.next[c] = put(x.next[c], word, d + <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">return</span> x;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">get</span><span class="params">(String word)</span> </span>&#123;</span><br><span class="line">        Node x = get(root, word, <span class="number">0</span>);</span><br><span class="line">        <span class="keyword">if</span> (x == <span class="keyword">null</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">return</span> x.val;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> Node <span class="title">get</span><span class="params">(Node x, String word, <span class="keyword">int</span> d)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (x == <span class="keyword">null</span>) <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">if</span> (d == word.length()) <span class="keyword">return</span> x;</span><br><span class="line">        <span class="keyword">int</span> c = word.charAt(d) - <span class="string">'A'</span>;</span><br><span class="line">        <span class="keyword">return</span> get(x.next[c], word, d + <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">check</span><span class="params">(<span class="keyword">int</span> i, <span class="keyword">int</span> j)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> i &gt;= <span class="number">0</span> &amp;&amp; i &lt; row &amp;&amp; j &gt;= <span class="number">0</span> &amp;&amp; j &lt; col;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">getAllValidWords</span><span class="params">(BoggleBoard board)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.board = board;</span><br><span class="line">        allwords = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line">        row = board.rows();</span><br><span class="line">        col = board.cols();</span><br><span class="line">        <span class="comment">//这个地方的写法需要注意一下。</span></span><br><span class="line">        adj = (Bag&lt;Integer&gt;[]) <span class="keyword">new</span> Bag[row * col];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; row; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; col; j++) &#123;</span><br><span class="line">                <span class="keyword">int</span> v = i * col + j;</span><br><span class="line">                adj[v] = <span class="keyword">new</span> Bag&lt;Integer&gt;();</span><br><span class="line">                <span class="comment">//这里就相当于图论里面的建立边，直接给后面dfs提供条件</span></span><br><span class="line">                <span class="keyword">if</span> (check(i - <span class="number">1</span>, j)) adj[v].add((i - <span class="number">1</span>) * col + j);</span><br><span class="line">                <span class="keyword">if</span> (check(i + <span class="number">1</span>, j)) adj[v].add((i + <span class="number">1</span>) * col + j);</span><br><span class="line">                <span class="keyword">if</span> (check(i, j - <span class="number">1</span>)) adj[v].add(i * col + j - <span class="number">1</span>);</span><br><span class="line">                <span class="keyword">if</span> (check(i, j + <span class="number">1</span>)) adj[v].add(i * col + j + <span class="number">1</span>);</span><br><span class="line">                <span class="keyword">if</span> (check(i + <span class="number">1</span>, j - <span class="number">1</span>)) adj[v].add((i + <span class="number">1</span>) * col + j - <span class="number">1</span>);</span><br><span class="line">                <span class="keyword">if</span> (check(i + <span class="number">1</span>, j + <span class="number">1</span>)) adj[v].add((i + <span class="number">1</span>) * col + j + <span class="number">1</span>);</span><br><span class="line">                <span class="keyword">if</span> (check(i - <span class="number">1</span>, j - <span class="number">1</span>)) adj[v].add((i - <span class="number">1</span>) * col + j - <span class="number">1</span>);</span><br><span class="line">                <span class="keyword">if</span> (check(i - <span class="number">1</span>, j + <span class="number">1</span>)) adj[v].add((i - <span class="number">1</span>) * col + j + <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//接下来就到了 dfs搜图的时候</span></span><br><span class="line">        <span class="comment">//最先开始我个人的想法是在这个地方用bfs来进行，当时发现到后面存在很多的问题，比如时间复杂度是特别高的。</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; row * col; i++) &#123;</span><br><span class="line">            vis = <span class="keyword">new</span> <span class="keyword">boolean</span>[row * col];</span><br><span class="line">            dice = <span class="keyword">new</span> Stack&lt;Integer&gt;();</span><br><span class="line">            vis[i] = <span class="keyword">true</span>;</span><br><span class="line">            dice.push(i);</span><br><span class="line">            <span class="comment">//这个地方需要留意的是 root在这里并没有其他的含义</span></span><br><span class="line">            <span class="keyword">char</span> c = getLetter(i);</span><br><span class="line">            <span class="keyword">if</span> (c == <span class="string">'Q'</span>) dfs(i, root.next[<span class="string">'Q'</span> - <span class="string">'A'</span>].next[<span class="string">'U'</span> - <span class="string">'A'</span>], <span class="string">"QU"</span>, dice);</span><br><span class="line">            <span class="keyword">else</span> dfs(i, root.next[c - <span class="string">'A'</span>], c + <span class="string">""</span>, dice);</span><br><span class="line">            <span class="comment">//由于这个地方前面就直接重新定义了，所以就不需要采用清空操作了。</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> allwords;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">char</span> <span class="title">getLetter</span><span class="params">(<span class="keyword">int</span> v)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> board.getLetter(v / col , v % col);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> v, Node x, String prefix, Stack&lt;Integer&gt;dices)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (prefix.length() &gt; <span class="number">2</span> &amp;&amp; x != <span class="keyword">null</span> &amp;&amp; x.val == <span class="number">1</span>) &#123;</span><br><span class="line">            allwords.add(prefix);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> w : adj[v]) &#123;</span><br><span class="line">            <span class="keyword">char</span> c = getLetter(w);</span><br><span class="line">            <span class="keyword">if</span> (!vis[w] &amp;&amp; x != <span class="keyword">null</span> &amp;&amp; x.next[c - <span class="string">'A'</span>] != <span class="keyword">null</span>) &#123;</span><br><span class="line">                dice.push(w);</span><br><span class="line">                vis[w] = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (c == <span class="string">'Q'</span>) &#123;</span><br><span class="line">                    dfs(w, x.next[<span class="string">'Q'</span> - <span class="string">'A'</span>].next[<span class="string">'U'</span> - <span class="string">'A'</span>], prefix + <span class="string">"QU"</span>, dice);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span> dfs(w, x.next[c - <span class="string">'A'</span>], prefix + c, dice);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">int</span> d = dice.pop();</span><br><span class="line">                vis[d] = <span class="keyword">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">scoreOf</span><span class="params">(String word)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (get(word) == <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">int</span> len = word.length();</span><br><span class="line">            <span class="keyword">if</span> (len &lt;= <span class="number">2</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(len == <span class="number">3</span> || len == <span class="number">4</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (len == <span class="number">5</span>) <span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (len == <span class="number">6</span>) <span class="keyword">return</span> <span class="number">3</span>;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (len == <span class="number">7</span>) <span class="keyword">return</span> <span class="number">5</span>;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">return</span> <span class="number">11</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当然，这次的作业后面还有后续，后续的地方就是 直接构成一个框架进行写，直接将这个游戏具象化，等自己熟悉了之后，再将代码贴出来。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;/assets/css/APlayer.min.css&quot;&gt;&lt;script src=&quot;/assets/js/APlayer.min.js&quot; cla
      
    
    </summary>
    
      <category term="算法" scheme="http://wsx1128.cn/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="http://wsx1128.cn/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="java" scheme="http://wsx1128.cn/tags/java/"/>
    
      <category term="普林斯顿算法大作业" scheme="http://wsx1128.cn/tags/%E6%99%AE%E6%9E%97%E6%96%AF%E9%A1%BF%E7%AE%97%E6%B3%95%E5%A4%A7%E4%BD%9C%E4%B8%9A/"/>
    
  </entry>
  
</feed>
